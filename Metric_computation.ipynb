{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP8siEGLGJA7pbZmj79L/2m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"vkIhGHpZrFwh"}},{"cell_type":"code","source":["import torch\n","!pip install torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html"],"metadata":{"id":"YhaqcWyUq80L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679298069406,"user_tz":0,"elapsed":14493,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"81b6e8f9-0c33-482b-8c20-2ee6ec8761e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.1%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (9.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.17%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.27.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (5.9.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.0.12)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.1.1)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=f7be802a34319d9deaaf21a09785702149d73f3cbfce98870b6b77be5bb4aae0\n","  Stored in directory: /root/.cache/pip/wheels/31/b2/8c/9b4bb72a4384eabd1ffeab2b7ead692c9165e35711f8a9dc72\n","Successfully built torch-geometric\n","Installing collected packages: torch-scatter, torch-sparse, torch-geometric\n","Successfully installed torch-geometric-2.2.0 torch-scatter-2.1.1+pt113cu116 torch-sparse-0.6.17+pt113cu116\n"]}]},{"cell_type":"code","source":["import multiprocessing as mp\n","import os\n","import pickle\n","import warnings\n","import torch\n","from functools import partial\n","from itertools import combinations\n","import numpy as np\n","from scipy.spatial.distance import cosine\n","from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n","from sklearn.metrics.pairwise import linear_kernel\n","from sklearn.neighbors import BallTree\n","from sklearn.preprocessing import StandardScaler, normalize\n","from tqdm import tqdm\n","\n","from scipy.linalg import orthogonal_procrustes\n","from torch_geometric.datasets import Planetoid"],"metadata":{"id":"9N8Dd7rpOy1l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use google drive for saving and loading information\n","from google.colab import drive\n","import pickle\n","import os\n","\n","drive.mount('/content/drive')\n","file_path = '/content/drive/MyDrive/L45_project/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HqQMyXHQOJvY","executionInfo":{"status":"ok","timestamp":1679298095708,"user_tz":0,"elapsed":15370,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"15dee696-83d1-430d-b41c-42af361d6130"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Cora Data Set Stats"],"metadata":{"id":"phShuTT_rlrx"}},{"cell_type":"code","source":["cora_dataset = Planetoid(\"/tmp/cora\", name=\"cora\", split=\"full\")\n","cora_data = cora_dataset[0]"],"metadata":{"id":"IHWijlUEry4j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679298100496,"user_tz":0,"elapsed":2352,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"d10e13a6-e22d-498d-a704-b9b47ff8c404"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n","Processing...\n","Done!\n"]}]},{"cell_type":"code","source":["# Create a random list of indices with 10 items from each class for 2nd cossim\n","cora_labels = cora_dataset[0].y.detach().numpy()\n","cora_num_classes = 7\n","num_samples = 10\n","cora_indices = []\n","for i in range(cora_num_classes):\n","  class_i = np.random.choice(np.where(cora_labels == i)[0], size=num_samples, replace=False)\n","  print(class_i)\n","  cora_indices += class_i.tolist()\n"],"metadata":{"id":"69r9kl54rz2i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679262814394,"user_tz":0,"elapsed":392,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"fe2dca7f-4789-4a35-de19-5cc904b46e08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2181 2180 2653   52 2470 1801   10 2333  274 1435]\n","[ 126  103 1180 1781 1337 1564  133  176  958 1991]\n","[1263 2492 1383 1738   58   90 2530 1492 1148 1759]\n","[1014 2568  812  309  821  827 2140 1065  512 2126]\n","[ 151 1774    1   43 2153 2694 2386   78 2679 2121]\n","[1960 2188 2184  925 2506 1338  113 1029  252  438]\n","[  31 2638 1900 2135 2161  123   26  850  537  331]\n"]}]},{"cell_type":"markdown","source":["# Metric Calculations"],"metadata":{"id":"kpFfkagZrKpT"}},{"cell_type":"code","source":["# Adapted from https://github.com/SGDE2020/embedding_stability/blob/master/lib/tools/comparison.py\n","\n","class Comparison:\n","\n","    def __init__(self, emb_dir, embeddings, num_nodes, file_prefix):\n","        \"\"\"\n","        emb_dir: str, path where embeddings are saved\n","        embeddings: list, list of strings that specify the embedding files\n","        \"\"\"\n","        self.dir = emb_dir\n","        self.embeddings = embeddings\n","        self.pairs = self._combinations(embeddings)\n","        self.num_vertices = num_nodes\n","\n","        # use file names without numbering to mark result files\n","        self.file_prefix = file_prefix\n","\n","    def _combinations(self, emb_list):\n","        \"\"\" Computes all different comparison pairs of a list of embeddings \"\"\"\n","        return [pair for pair in combinations(emb_list, 2)]\n","\n","    def _analyse_jaccard(self, queries, nodes, k, pair):\n","        \"\"\"This function is called in the multiprocessing of jaccard score\"\"\"\n","        indices_0 = np.asarray(queries[pair[0]])\n","        indices_1 = np.asarray(queries[pair[1]])\n","        nodes = np.asarray(nodes)\n","        jaccard_score = {}\n","        for i in range(len(nodes)):\n","            jaccard_score[nodes[i]] = \\\n","                len(np.intersect1d(indices_0[i, 1:(k + 1)], indices_1[i, 1:(k + 1)], assume_unique=True)) / \\\n","                len(np.union1d(indices_0[i, 1:(k + 1)], indices_1[i, 1:(k + 1)]))\n","        return list(jaccard_score.values())\n","\n","    def _analyse_second_cossim(self, queries, normed_embs, nodes, k, pair):\n","        \"\"\"\n","        This function is called in the multiprocessing of the second order cosine similarity.\n","        \"\"\"\n","\n","        # Convert the indices of nearest neighbors back into numpy\n","        indices_0 = np.asarray(queries[pair[0]])\n","        indices_1 = np.asarray(queries[pair[1]])\n","\n","        # Convert the embeddings and nodes back into numpy\n","        norm_emb_0 = np.asarray(normed_embs[pair[0]])\n","        norm_emb_1 = np.asarray(normed_embs[pair[1]])\n","        nodes = np.asarray(nodes)\n","\n","        # Compute the second order cosine similarity\n","        pair_results = []\n","        for i in range(len(nodes)):\n","            # Build the set of nearest neighbors w.r.t. both embeddings\n","            # Use indices from 1 to k+1, because the first entry will always be the node itself\n","            neighbors_union = np.union1d(indices_0[i, 1:(k + 1)], indices_1[i, 1:(k + 1)])\n","\n","            # Vectors of cosine similarity values of nearest neighbors. There was an error in the original source code, did not use norm_emb_1 for m1\n","            m0 = cos_sim(norm_emb_0[neighbors_union], norm_emb_0[nodes[i]].reshape(1, -1))\n","            m1 = cos_sim(norm_emb_1[neighbors_union], norm_emb_1[nodes[i]].reshape(1, -1))\n","\n","            # Flatten output matrix\n","            assert m0.shape[1] == 1 and m1.shape[1] == 1, \"m0 and m1 should only have a single variable in the second dimension\"\n","            m0 = m0.flatten()\n","            m1 = m1.flatten()\n","\n","            # Cosine similarity between similarity vectors\n","            pair_results.append(float(1-cosine(m0,m1)))\n","        return pair_results\n","\n","    def k_nearest_neighbors(self, nodes=None, append=False, samples=100, k=10, jaccard=False, load=False,\n","                            kload_size=100, save=True, save_path=None, num_processes=4):\n","        \"\"\"\n","        Computes the k nearest neighbors to some specified nodes with respect to cosine similarity. As an intermediate\n","        step, it computes the 100 nearest neighbors and saves them to file. Based on these neighbors, the k-nn overlaps\n","        are computed.\n","        Args:\n","            nodes: list, that specifies the nodes\n","            append: bool, if nodes are specified, whether additional nodes will be sampled and added to nodes\n","            samples: int, number of nodes that will be sampled\n","            k: int, number of neighbors that will be used in the comparison\n","            jaccard: bool, whether jaccard score or overlap will be used as similarity measure\n","            load: bool, whether a file of computed neighbors will be used\n","            kload_size: Size of k in knn file to load\n","            save: bool, whether the results should be saved in a text file\n","            save_path: str, path where the file will be saved\n","            num_processes: number of random processes in parallelization\n","        Returns:\n","            dict, \"nodes\": array of nodes, \"overlaps\": array of overlaps, columns are values per node; or array of\n","            jaccard scores\n","        \"\"\"\n","        # Handle the nodes input: Whether to sample nodes, use given nodes, or both.\n","        nodes = self._get_nodes(nodes, samples, append)\n","\n","        # Load k nearest neighbors to save time? Else we will have to compute them first.\n","        if load:\n","            file_name = self.file_prefix + \"_\" + str(kload_size) + \"nns.pickle\"\n","            with open(os.path.join(save_path, file_name), \"rb\") as pickle_file:\n","                queries = pickle.load(pickle_file)\n","            assert list(queries.keys()) == self.embeddings, (\"Keys of loaded queries do not match\"\n","                                                             \" with available embeddings\")\n","\n","        # Normalizing the embeddings to be able to use distance as a proxy for cosine similarity\n","        # BallTree from sklearn is used to compute the neighbors efficiently\n","        else:\n","            queries = self.nearest_neighbors_queries(nodes, k, save_path)\n","        # Store the naive overlaps for all pairs\n","\n","        print(f\"\\n\\n {queries.keys()} \\n\")\n","\n","        # Use multiprocessing to speed up overlap computation.\n","        # Too much data is passed to the processes which makes it inefficient.\n","        # Possibly, it is faster to store the data as a file as an intermediate step.\n","        # only run if less than 100 neighbors are queried, as too large neighborhoods may cause memory issues when\n","        # distributing tasks\n","        if num_processes > 1 and k <= 100:\n","            with mp.Pool(num_processes) as p:\n","                # arguments passed in multiprocessing must be pickable\n","                p_queries = queries\n","                p_nodes = nodes.tolist()\n","                if jaccard:\n","                    multiprocess_func = partial(self._analyse_jaccard, p_queries, p_nodes, k)\n","                else:\n","                    multiprocess_func = partial(self._analyse_knn, p_queries, p_nodes, k)\n","                li_overlap = []\n","                for result in tqdm(p.imap(multiprocess_func, self.pairs), total=len(self.pairs)):\n","                  li_overlap.append(result)\n","        else:\n","            if jaccard:\n","                li_overlap = [self._analyse_jaccard(queries, nodes.tolist(), k, pair) for pair in self.pairs]\n","            else:\n","                li_overlap = [self._analyse_knn(queries, nodes.tolist(), k, pair) for pair in self.pairs]\n","\n","        # Convert the result into numpy\n","        overlap = np.asarray(li_overlap)\n","\n","        # Save the results\n","        if jaccard:\n","            nodes_suffix = \"jaccard_nodes\"\n","            scores_suffix = f\"{k}nn_jaccard\"\n","        else:\n","            nodes_suffix = \"overlap_nodes\"\n","            scores_suffix = f\"{k}nn_overlap\"\n","        if save is True:\n","            np.save(os.path.join(save_path, f\"{self.file_prefix}_{nodes_suffix}\"), nodes)\n","            np.save(os.path.join(save_path, f\"{self.file_prefix}_{scores_suffix}\"), overlap)\n","        return {\"nodes\": nodes, \"overlaps\": overlap}\n","\n","    def jaccard_similarity(self, nodes=None, append=False, samples=100, k=10, load=False, kload_size=100, save=True,\n","                           save_path=None, num_processes=4):\n","        \"\"\"\n","        Alias for k_nearest_neighbors with jaccard=True.\n","        See k_nearest_neighbors for detailed documentation.\n","        \"\"\"\n","        return self.k_nearest_neighbors(nodes=nodes, append=append, samples=samples, k=k, jaccard=True, load=load,\n","                                        kload_size=kload_size, save=save, save_path=save_path,\n","                                        num_processes=num_processes)\n","\n","\n","    def second_order_cosine_similarity(self, nodes=None, append=False, num_samples=1000, k=10, load=True, save=False,\n","                                       save_path=None, num_processes=4):\n","        \"\"\" Computes second order cosine similarity.\n","        Args:\n","            nodes: list, that specifies the nodes\n","            append: bool, if nodes are specified, whether additional nodes will be sampled and added to nodes\n","            num_samples: int, number of nodes that will be sampled\n","            k: int, number of neighbors that will be used in the comparison\n","            load: bool, whether to load nearest neighbors from file\n","            save: bool, whether the results should be saved\n","            save_path: str, path where the file will be saved\n","            num_processes: number of random processes in parallelization\n","        Returns:\n","            nodes: numpy array of used nodes\n","            results: numpy array of similarity values of size (number of embedding pairs, number of nodes)\n","        \"\"\"\n","\n","        # Handle the nodes input: Whether to sample nodes, use given nodes, or both.\n","        nodes = self._get_nodes(nodes, num_samples, append)\n","\n","        # Load required data: nearest neighbors, embeddings\n","        normed_embs = {}\n","\n","        if load:\n","            # Load nearest neighbors from file\n","            file_name = self.file_prefix + \"_\" + str(k)+ \"nns.pickle\"\n","            with open(os.path.join(save_path, file_name), \"rb\") as pickle_file:\n","                queries = pickle.load(pickle_file)\n","            assert list(queries.keys()) == self.embeddings, (\"Keys of loaded queries do not match with \"\n","                                                             \"available embeddings\")\n","            for emb in tqdm(self.embeddings, desc=\"Loading nearest neighbor files\"):\n","                normed_embs[emb] = normalize(self.read_embedding(os.path.join(self.dir, emb)), norm='l2', copy=False)\n","        else:\n","            queries, normed_embs = self.nearest_neighbors_queries(nodes, k, save_path, return_embeddings=True)\n","\n","        # Start computation of second order cosine similarity\n","        # arguments passed in multiprocessing must be pickable\n","        p_normed_embs = dict([(key, norm_emb.tolist()) for key, norm_emb in normed_embs.items()])\n","        p_nodes = nodes.tolist()\n","        # Avoid multiprocessing on Colab, not sure if it works locally though\n","        if num_processes > 1 and k <= 100:\n","            with mp.Pool(num_processes) as p:\n","                li_results = p.map(partial(self._analyse_second_cossim, queries, p_normed_embs, p_nodes, k), self.pairs)\n","              # li_results = []\n","              # partial_func = partial(self._analyse_second_cossim, queries, p_normed_embs, p_nodes, k) \n","              # for result in tqdm(p.imap(partial_func, self.pairs), total=len(self.pairs)):\n","              #   li_results.append(result)\n","        else:\n","          li_results = []\n","          for pair in tqdm(self.pairs, desc=\"Comparing embeddings\"):\n","            li_results.append(self._analyse_second_cossim(queries, p_normed_embs, p_nodes, k, pair))\n","\n","        results = np.asarray(li_results)\n","\n","        if save is True:\n","            np.save(os.path.join(save_path, f\"{self.file_prefix}_{k}nn_2nd_order_cossim\"), results)\n","\n","        return nodes, results\n","\n","    def nearest_neighbors_queries(self, nodes, k, save_path, return_embeddings=False):\n","        \"\"\"Uses a ball tree to compute the nearest neighbors in the embedding space\"\"\"\n","        queries = {}\n","        normed_embs = {}\n","        # Normalizing the embeddings to be able to use distance as a proxy for cosine similarity\n","        # BallTree from sklearn is used to compute the neighbors efficiently\n","        if return_embeddings:\n","            for emb in tqdm(self.embeddings, desc=\"Querying nearest neighbors\"):\n","                normed_embs[emb] = normalize(self.read_embedding(os.path.join(self.dir, emb)), norm='l2', copy=False)\n","                ball_tree = BallTree(normed_embs[emb], leaf_size=40)\n","                queries[emb] = ball_tree.query(normed_embs[emb][nodes, :], k=k + 1, return_distance=False).tolist()\n","        else:\n","            for emb in tqdm(self.embeddings, desc=\"Querying nearest neighbors\"):\n","                normalized_embedding = normalize(self.read_embedding(os.path.join(self.dir, emb)), norm='l2',\n","                                                 copy=False)\n","                ball_tree = BallTree(normalized_embedding, leaf_size=40)\n","                # Query the k+1 nearest neighbors, because a node will always be the closest neighbor to itself\n","                queries[emb] = ball_tree.query(normalized_embedding[nodes, :], k=k + 1, return_distance=False).tolist()\n","\n","        # Save the computed neighbors to be able to skip the computation\n","        self.save_pickle(queries, save_path, self.file_prefix + \"_\" + str(k) + \"nns\")\n","\n","        if return_embeddings:\n","            return queries, normed_embs\n","        else:\n","            return queries\n","\n","    def sample_nodes(self, k):\n","        \"\"\"\n","        Sample unique nodes of an embedding\n","        Args:\n","            k: int, number of nodes to sample\n","        Returns:\n","            numpy array of node ids of length k if k is smaller than the number of nodes available.\n","            Otherwise, all available nodes are returned.\n","        \"\"\"\n","        vertices = np.arange(self.num_vertices)\n","        np.random.shuffle(vertices)\n","        return vertices[:min(k, self.num_vertices)]\n","\n","\n","    def cossim_analysis(self, save_path):\n","        \"\"\" Computes aligned cosine similarity values. Internally performs orthogonal transformation (Procrustes\n","        problem) between two embeddings and saves transformation matrices as well as vector of resulting errors\n","        \"\"\"\n","\n","        # Set up file naming\n","        results_suffix = \"aligned_cossim\"\n","\n","        # Read the embeddings\n","        normed_embs = {}\n","        for emb in tqdm(self.embeddings, desc=\"Reading embeddings\"):\n","            normed_embs[emb] = normalize(\n","                    self.read_embedding(os.path.join(self.dir, emb))[np.arange(self.num_vertices)], norm='l2',\n","                    copy=False)\n","\n","        # Do the analysis\n","        emb_ind = -1\n","        results = []\n","        for pair in tqdm(self.pairs, desc=\"Comparing embeddings\"):\n","\n","            # only update first embedding if it does not change\n","            if emb_ind != pair[0]:\n","                emb_ind = pair[0]\n","\n","            W1 = normed_embs[emb_ind]\n","\n","            # transform W2 into W1 using procrustes matrix\n","            Q, _ = orthogonal_procrustes(normed_embs[pair[1]], normed_embs[pair[0]], check_finite=False)\n","            W2 = normed_embs[pair[1]].dot(Q)\n","\n","            # Do 1-cosine to get the actual cosine similarity instead of cosine difference\n","            pair_results = np.array([1-cosine(W1[i], W2[i]) for i in range(self.num_vertices)])\n","            results.append(pair_results)\n","\n","        results = np.asarray(results)\n","        np.save(os.path.join(save_path, f\"{self.file_prefix}_{results_suffix}\"), results)\n","        return np.arange(self.num_vertices), results\n","\n","    def unaligned_cosine_analysis(self, save_path):\n","        \"\"\" Computes regular cosine similarity values, without alignment, between embedding pairs\n","        \"\"\"\n","\n","        # Set up file naming\n","        results_suffix = \"cossim\"\n","\n","        # Read the embeddings\n","        normed_embs = {}\n","        for emb in tqdm(self.embeddings, desc=\"Reading embeddings\"):\n","            normed_embs[emb] = normalize(\n","                    self.read_embedding(os.path.join(self.dir, emb))[np.arange(self.num_vertices)], norm='l2',\n","                    copy=False)\n","\n","        # Do the analysis\n","        emb_ind = -1\n","        results = []\n","        for pair in tqdm(self.pairs, desc=\"Comparing embeddings\"):\n","\n","            # only update first embedding if it does not change\n","            if emb_ind != pair[0]:\n","                emb_ind = pair[0]\n","\n","            # Get our two embedding matrics\n","            W1 = normed_embs[emb_ind]\n","            W2 = normed_embs[pair[1]]\n","\n","            # Do 1-cosine to get the actual cosine similarity instead of cosine distance\n","            pair_results = np.array([1-cosine(W1[i], W2[i]) for i in range(self.num_vertices)])\n","            results.append(pair_results)\n","\n","        results = np.asarray(results)\n","        np.save(os.path.join(save_path, f\"{self.file_prefix}_{results_suffix}\"), results)\n","        return np.arange(self.num_vertices), results\n","\n","    def pairwise_distance(self, nodes, save_path, norm=False):\n","        \"\"\"Calculate the pairwise distance between all our embeddings. Save these to the given file path\"\"\"\n","\n","        results_suffix = \"euclidean_distance\"\n","\n","        # Read the embeddings\n","        normed_embs = {}\n","        for emb in tqdm(self.embeddings, desc=\"Reading embeddings\"):\n","            # Option for calculating distance between normalized embeddings\n","            if norm:\n","              normed_embs[emb] = normalize(\n","                      self.read_embedding(os.path.join(self.dir, emb))[np.arange(self.num_vertices)], norm='l2',\n","                      copy=False)\n","            else:\n","              normed_embs[emb] = self.read_embedding(os.path.join(self.dir, emb))[np.arange(self.num_vertices)]\n","\n","        # Do the analysis\n","        emb_ind = -1\n","        results = []\n","        for pair in tqdm(self.pairs, desc=\"Comparing embeddings\"):\n","\n","            # only update first embedding if it does not change\n","            if emb_ind != pair[0]:\n","                emb_ind = pair[0]\n","\n","            # Get the distance between the two matrices of embeddings and add to results array\n","            results.append(np.linalg.norm(normed_embs[emb_ind] - normed_embs[pair[1]], axis=1))\n","\n","        results = np.asarray(results)\n","        np.save(os.path.join(save_path, f\"{self.file_prefix}_{results_suffix}\"), results)\n","\n","\n","    def _get_nodes(self, nodes, num_samples, append):\n","        \"\"\"\n","        Handles getting nodes for the experiments.\n","        Args:\n","            nodes: list, node ids\n","            num_samples: int, how many nodes should be sampled\n","            append: bool, whether to append sampled nodes to specified nodes\n","        Returns:\n","            numpy array of (sampled) node ids\n","        \"\"\"\n","        if nodes is None:\n","            nodes = self.sample_nodes(num_samples)\n","        elif append is True:\n","            # allows specified nodes to be taken twice\n","            nodes.extend(self.sample_nodes(num_samples))\n","        return np.asarray(nodes)\n","\n","    def save_pickle(self, obj, save_path, file_name):\n","        if save_path is None:\n","            save_path = os.getcwd()\n","        if file_name is None:\n","            # generate name of report from embedding input: use name of first embedding file without number information\n","            file_name = self.file_prefix\n","        with open(os.path.join(save_path, f\"{file_name}.pickle\"), \"wb\") as f:\n","            pickle.dump(obj, f)\n","\n","    def get_combinations(self):\n","        return self.pairs\n","\n","    def get_vertex_count(self):\n","        return self.num_vertices\n","\n","    # The original code reorders the nodes by ID, but our nodes have the same ordering so there is no need to reorder\n","    def read_embedding(self, path):\n","      node_embedding = torch.load(path, map_location=torch.device('cpu')).detach().numpy()\n","      return node_embedding"],"metadata":{"id":"vL3dj_plpMyw","executionInfo":{"status":"ok","timestamp":1679312065676,"user_tz":0,"elapsed":316,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kd9PN4KBZkYj"},"outputs":[],"source":["# Code adapted from https://github.com/SGDE2020/embedding_stability/blob/master/similarity_tests/similarity_tests.py\n","TESTS= [\"cossim\", \"jaccard\", \"2ndcos\", \"unalign_cossim\", \"dist\"]\n","\n","# Expects the the file names in the embedding directory to be of the form model-name_seed_emb.pt\n","def run_tests(embedding_dir, file_prefix, tests=TESTS, num_nodes=-1, knn_size=20, load_knn=False, kload_size=None,\n","                results_dir=None, num_processes=4, cossim_sec_indices=None):\n","    \"\"\"\n","    Run specified similarity tests.\n","    Params:\n","        embedding_dir: str, path to directory where embeddings are stored\n","        file_prefix: the prefix for files for saving results\n","        tests: list of str, tests that will be conducted\n","        num_nodes: int, number of nodes of a graph that are used in the tests (just the number of nodes in the graph)\n","        knn_size: int, neighborhood size for knn test\n","        load_knn: bool, whether a stored knn matrix should be loaded\n","        nodeinfo_dir = str, path to directory where nodeinfo is stored (tables)\n","        results_dir = str, path to directory where results will be saved to\n","    \"\"\"\n","    # Store results in the embedding directory in a results file if no other location is specified\n","    if results_dir is None:\n","      results_dir = embedding_dir + \"results/\"\n","    # Create results directory if it does not exist\n","    if not os.path.exists(results_dir):\n","        os.mkdir(results_dir) \n","\n","    # Computes a list of all _emb.pt files in a given directory. Assumes the directory being used only contains results from one architecture and data set\n","    fnames = sorted([f for f in os.listdir(embedding_dir) if f.endswith(\"_emb.pt\")])\n","    print(fnames)\n","\n","    if len(fnames) <= 1:\n","        print(f\"Did not find any embeddings for in directory {embedding_dir}\")\n"," \n","    else:\n","        # To be compatible with the original source code, we need a list of nodes indices we will be using in our metric computations\n","        # We use all the nodes\n","        nodes = [i for i in range(num_nodes)]\n","\n","        # Allow us to use specific nodes for the second order cossine similarity as it can be very computational expensive\n","        cossim_nodes = nodes if cossim_sec_indices is None else cossim_sec_indices\n","\n","        if load_knn and kload_size is None:\n","            kload_size = knn_size\n","\n","        # Start tests\n","        comp = Comparison(emb_dir=embedding_dir, embeddings=fnames, num_nodes=num_nodes, file_prefix=file_prefix)\n","        if \"cossim\" in tests:\n","            print(\"Executing aligned cosine similarity\")\n","            comp.cossim_analysis(save_path=results_dir)\n","        if \"jaccard\" in tests:\n","            print(\"Executing jaccard score\")\n","            comp.jaccard_similarity(\n","                nodes=nodes, append=False, k=knn_size,\n","                load=load_knn, kload_size=kload_size, save=True, save_path=results_dir, num_processes=num_processes\n","            )\n","        if \"2ndcos\" in tests:\n","            print(\"Executing second order cosine similarity\")\n","            comp.second_order_cosine_similarity(\n","                nodes=cossim_nodes, append=False, k=knn_size,\n","                save=True, save_path=results_dir, num_processes=num_processes, load=load_knn\n","            )\n","        if \"unalign_cossim\" in tests:\n","            print(\"Executing cosine similarity\")\n","            comp.unaligned_cosine_analysis(save_path=results_dir)\n","        if \"dist\" in tests:\n","            print(\"Executing euclidean distance\")\n","            comp.pairwise_distance(nodes=nodes, save_path=results_dir)\n","\n"]},{"cell_type":"markdown","source":["# Run metric calculations"],"metadata":{"id":"FTsS-KbhxRVY"}},{"cell_type":"code","source":["NUM_NODES_CORA = cora_data.x.shape[0]\n","# NUM_NODES_CORA = 18333\n","# CHANGE these to match you directory naming structure\n","model_directory = \"GraphSAGE/\"\n","model_file_prefix = \"GraphSAGE_analysis\"\n","# model_directory = \"GraphSAGE/\"\n","# model_file_prefix = \"GraphSAGE_analysis\""],"metadata":{"id":"p4qF6bbXxBlU","executionInfo":{"status":"ok","timestamp":1679327431225,"user_tz":0,"elapsed":212,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Run the tests for tests=[\"cossim\", \"jaccard\"], multiple processes work for these\n","run_tests(embedding_dir=file_path+model_directory, file_prefix=model_file_prefix, tests=[\"cossim\", \"jaccard\", \"unalign_cossim\", \"dist\"], num_nodes=NUM_NODES_CORA, num_processes=4)"],"metadata":{"id":"JcbF5Mo5SRIU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679327813710,"user_tz":0,"elapsed":370936,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"61382cc2-9b83-4765-fa22-2001a3f871a0"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["['GraphSAGE_1024258131_emb.pt', 'GraphSAGE_116846604_emb.pt', 'GraphSAGE_1221215631_emb.pt', 'GraphSAGE_1517964140_emb.pt', 'GraphSAGE_170173784_emb.pt', 'GraphSAGE_1818027900_emb.pt', 'GraphSAGE_1860537279_emb.pt', 'GraphSAGE_1863727779_emb.pt', 'GraphSAGE_2036056847_emb.pt', 'GraphSAGE_2105922959_emb.pt', 'GraphSAGE_2135956485_emb.pt', 'GraphSAGE_2342954646_emb.pt', 'GraphSAGE_2455059856_emb.pt', 'GraphSAGE_2739899259_emb.pt', 'GraphSAGE_2794978777_emb.pt', 'GraphSAGE_2952735006_emb.pt', 'GraphSAGE_3300171104_emb.pt', 'GraphSAGE_3303475786_emb.pt', 'GraphSAGE_361232447_emb.pt', 'GraphSAGE_3647665043_emb.pt', 'GraphSAGE_3692371949_emb.pt', 'GraphSAGE_3710910636_emb.pt', 'GraphSAGE_400225693_emb.pt', 'GraphSAGE_4083009686_emb.pt', 'GraphSAGE_4193977854_emb.pt', 'GraphSAGE_516507873_emb.pt', 'GraphSAGE_572297925_emb.pt', 'GraphSAGE_806299656_emb.pt', 'GraphSAGE_880019963_emb.pt', 'GraphSAGE_89475662_emb.pt']\n","Executing aligned cosine similarity\n"]},{"output_type":"stream","name":"stderr","text":["Reading embeddings: 100%|██████████| 30/30 [00:10<00:00,  3.00it/s]\n","Comparing embeddings: 100%|██████████| 435/435 [01:53<00:00,  3.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Executing jaccard score\n"]},{"output_type":"stream","name":"stderr","text":["Querying nearest neighbors: 100%|██████████| 30/30 [00:32<00:00,  1.09s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","\n"," dict_keys(['GraphSAGE_1024258131_emb.pt', 'GraphSAGE_116846604_emb.pt', 'GraphSAGE_1221215631_emb.pt', 'GraphSAGE_1517964140_emb.pt', 'GraphSAGE_170173784_emb.pt', 'GraphSAGE_1818027900_emb.pt', 'GraphSAGE_1860537279_emb.pt', 'GraphSAGE_1863727779_emb.pt', 'GraphSAGE_2036056847_emb.pt', 'GraphSAGE_2105922959_emb.pt', 'GraphSAGE_2135956485_emb.pt', 'GraphSAGE_2342954646_emb.pt', 'GraphSAGE_2455059856_emb.pt', 'GraphSAGE_2739899259_emb.pt', 'GraphSAGE_2794978777_emb.pt', 'GraphSAGE_2952735006_emb.pt', 'GraphSAGE_3300171104_emb.pt', 'GraphSAGE_3303475786_emb.pt', 'GraphSAGE_361232447_emb.pt', 'GraphSAGE_3647665043_emb.pt', 'GraphSAGE_3692371949_emb.pt', 'GraphSAGE_3710910636_emb.pt', 'GraphSAGE_400225693_emb.pt', 'GraphSAGE_4083009686_emb.pt', 'GraphSAGE_4193977854_emb.pt', 'GraphSAGE_516507873_emb.pt', 'GraphSAGE_572297925_emb.pt', 'GraphSAGE_806299656_emb.pt', 'GraphSAGE_880019963_emb.pt', 'GraphSAGE_89475662_emb.pt']) \n","\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 435/435 [02:23<00:00,  3.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Executing cosine similarity\n"]},{"output_type":"stream","name":"stderr","text":["Reading embeddings: 100%|██████████| 30/30 [00:00<00:00, 122.15it/s]\n","Comparing embeddings: 100%|██████████| 435/435 [01:08<00:00,  6.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Executing euclidean distance\n"]},{"output_type":"stream","name":"stderr","text":["Reading embeddings: 100%|██████████| 30/30 [00:00<00:00, 203.19it/s]\n","Comparing embeddings: 100%|██████████| 435/435 [00:00<00:00, 1575.60it/s]\n"]}]},{"cell_type":"code","source":["# Run the tests for tests=[\"2ndcos\"], multiple processes does not work for this so only run on 1, but we can load the nearest neighbours from the previous metric calculations\n","run_tests(embedding_dir=file_path+model_directory, file_prefix=model_file_prefix, tests=[\"2ndcos\"], num_nodes=NUM_NODES_CORA, num_processes=1, load_knn=True)"],"metadata":{"id":"isXLxy0ywLtq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679329272579,"user_tz":0,"elapsed":1030385,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"d7ec811b-b96c-43a7-878e-3e5f77555776"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["['GraphSAGE_1024258131_emb.pt', 'GraphSAGE_116846604_emb.pt', 'GraphSAGE_1221215631_emb.pt', 'GraphSAGE_1517964140_emb.pt', 'GraphSAGE_170173784_emb.pt', 'GraphSAGE_1818027900_emb.pt', 'GraphSAGE_1860537279_emb.pt', 'GraphSAGE_1863727779_emb.pt', 'GraphSAGE_2036056847_emb.pt', 'GraphSAGE_2105922959_emb.pt', 'GraphSAGE_2135956485_emb.pt', 'GraphSAGE_2342954646_emb.pt', 'GraphSAGE_2455059856_emb.pt', 'GraphSAGE_2739899259_emb.pt', 'GraphSAGE_2794978777_emb.pt', 'GraphSAGE_2952735006_emb.pt', 'GraphSAGE_3300171104_emb.pt', 'GraphSAGE_3303475786_emb.pt', 'GraphSAGE_361232447_emb.pt', 'GraphSAGE_3647665043_emb.pt', 'GraphSAGE_3692371949_emb.pt', 'GraphSAGE_3710910636_emb.pt', 'GraphSAGE_400225693_emb.pt', 'GraphSAGE_4083009686_emb.pt', 'GraphSAGE_4193977854_emb.pt', 'GraphSAGE_516507873_emb.pt', 'GraphSAGE_572297925_emb.pt', 'GraphSAGE_806299656_emb.pt', 'GraphSAGE_880019963_emb.pt', 'GraphSAGE_89475662_emb.pt']\n","Executing second order cosine similarity\n"]},{"output_type":"stream","name":"stderr","text":["Loading nearest neighbor files: 100%|██████████| 30/30 [00:00<00:00, 197.21it/s]\n","Comparing embeddings: 100%|██████████| 435/435 [17:08<00:00,  2.36s/it]\n"]}]},{"cell_type":"markdown","source":["# Compute metric averages"],"metadata":{"id":"g1plphzXZvaC"}},{"cell_type":"code","source":["def read_and_average_metric(file_suffix, class_labels, num_classes):\n","  results_arr = np.load(file_path+model_directory+\"/results/\"+model_file_prefix+\"_\"+file_suffix+\".npy\", mmap_mode='r')\n","  results_flat = results_arr.flatten() \n","  # Compute the 5 number summary for the data across all classes, better than mean and std for potentially skewed distribution\n","  q1, q2, q3 = np.percentile(results_flat, [25,50,75])\n","  min, max = results_flat.min(), results_flat.max()\n","  # Print results\n","  print(\"TOTAL STATS\")\n","  print(\"q1=\" + str(q1) + \", q2=\" + str(q2) + \", q3=\" + str(q3) + \", min=\" + str(min) + \", max=\" + str(max))\n","\n","  for i in range(num_classes):\n","    indices = np.where(class_labels==i)[0]\n","    # Filter the results by class\n","    class_vals = results_arr[:, indices]\n","    # Compute stats for the class\n","    q1_c, q2_c, q3_c = np.percentile(class_vals, [25,50,75])\n","    min_c, max_c = class_vals.min(), class_vals.max()\n","    # Print results\n","    print(\"CLASS STATS \" + str(i))\n","    print(\"q1=\" + str(q1_c), \", q2=\" + str(q2_c) + \", q3=\" + str(q3_c) + \", min=\" + str(min_c) + \", max=\" + str(max_c))\n"],"metadata":{"id":"VR5yJTLlaULL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read and compute aligned cosine metric\n","read_and_average_metric(\"aligned_cossim\", cora_labels, cora_num_classes)"],"metadata":{"id":"-dYdz2YuVVjV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read and compute jaccard metric\n","read_and_average_metric(\"20nn_jaccard\", cora_labels, cora_num_classes)"],"metadata":{"id":"_xQJ4uQPZ0Va"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read and compute 2nd order cosine similarity metric\n","read_and_average_metric(\"20nn_2nd_order_cossim\", cora_labels, cora_num_classes)"],"metadata":{"id":"aPjICoJ9Z1rX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read and compute 2nd order cosine similarity metric\n","read_and_average_metric(\"cossim\", cora_labels, cora_num_classes)"],"metadata":{"id":"MqHNfK-ySM4S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["read_and_average_metric(\"euclidean_distance\", cora_labels, cora_num_classes)"],"metadata":{"id":"j1PKJKLVS19A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To do:\n","- Store the nodes being used if we're only using a subset\n","- Subsets for second_order_cosine_similarity for larger dataset"],"metadata":{"id":"pOoPSZmgMrkZ"}}]}