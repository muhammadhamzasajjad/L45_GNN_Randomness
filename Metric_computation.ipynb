{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkIhGHpZrFwh"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YhaqcWyUq80L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b3713c-cc8c-4a5a-e44c-d5a0f3a5d0e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.1%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.17%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (5.9.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.1.1)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=f39de97d2ad678c60a8e2935d8b0dad4bb93caa725745fa550c50798219d1662\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/b2/8c/9b4bb72a4384eabd1ffeab2b7ead692c9165e35711f8a9dc72\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-scatter, torch-sparse, torch-geometric\n",
            "Successfully installed torch-geometric-2.2.0 torch-scatter-2.1.1+pt113cu116 torch-sparse-0.6.17+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.13.1+cu116)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.2.2)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.26.15)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.22.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (63.4.3)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=b8c08503501ea849360261bbcc115895c4bb75623cf4c3c0d091b7e28feb46b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/bb/0d/2d02ec45f29c48d6192476bfb59c5a0e64b605e7212374dd15\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.5 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "!pip install torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9N8Dd7rpOy1l"
      },
      "outputs": [],
      "source": [
        "import multiprocessing as mp\n",
        "import os\n",
        "import pickle\n",
        "import warnings\n",
        "import torch\n",
        "from functools import partial\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.neighbors import BallTree\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "from tqdm import tqdm\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "from torch_geometric.datasets import Planetoid, Coauthor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HqQMyXHQOJvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce1e11b-744e-42c5-dec3-ea63efc2be2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# use google drive for saving and loading information\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/MyDrive/L45_project/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phShuTT_rlrx"
      },
      "source": [
        "# Cora Data Set Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IHWijlUEry4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a319d9b-24ae-478d-a2c5-9c9e2bb47234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "cora_dataset = Planetoid(\"/tmp/cora\", name=\"cora\", split=\"full\")\n",
        "cora_data = cora_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "69r9kl54rz2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f73da826-599f-44e2-e989-b8310aa5f924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2576 1912 2200  423 2116 1296 2219 2266 1203  751]\n",
            "[ 589 1767  655 2478 1009  112  910  876 2144 2532]\n",
            "[ 860 1599 1718  823  564  867  353 1444 1358 1037]\n",
            "[2610 2204  674 1186 1692 2241 1865 2668  386 1175]\n",
            "[ 638 1484 1001 2223 2357 2670 1234 2473  498   63]\n",
            "[2553 1953 1825 2103 1578 1394  941 1979 2275  793]\n",
            "[2110  939 2157  778  213  123  331 1170 2608  908]\n"
          ]
        }
      ],
      "source": [
        "# Create a random list of indices with 10 items from each class for 2nd cossim\n",
        "cora_labels = cora_dataset[0].y.detach().numpy()\n",
        "cora_num_classes = 7\n",
        "num_samples = 10\n",
        "cora_indices = []\n",
        "for i in range(cora_num_classes):\n",
        "  class_i = np.random.choice(np.where(cora_labels == i)[0], size=num_samples, replace=False)\n",
        "  print(class_i)\n",
        "  cora_indices += class_i.tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVkq2gvzJhos"
      },
      "source": [
        "# CoauthorCS Data Set Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "oL2UxAGHEyz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "613de198-76cb-436f-b232-70849af775fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ms_academic_cs.npz\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[18333, 6805], edge_index=[2, 163788], y=[18333])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "cs_dataset = Coauthor(\"/tmp/coauthor\", name=\"CS\")\n",
        "cs_data = cs_dataset[0]\n",
        "cs_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "DoGkr9uxE7ck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38709ef8-0bf8-452a-dbc6-ee29da1a8546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1738 11908   164  5685  4882  2188 15054 14202  6519 11263]\n",
            "[17340  2739  7216  6967  2434  9553   361  3532  1727  6344]\n",
            "[ 6746  5672  2103  6218 13375  2596  8004 12211 17863 14256]\n",
            "[ 4861  2871  9022  2756 17701 11697 17024 14378  8596  6382]\n",
            "[14787 17693 16931  3684  2107  4098  4252 18079 12822 14164]\n",
            "[12010  9075 14639  2645 15214 10014  5268  6928  1169  8939]\n",
            "[13148 14500 14525  8582   988 16381  1460 15069   530  3596]\n",
            "[16870 12410  6654 15057 10901  7034  5804 12884 12318 14082]\n",
            "[14855 14133  8575 13112  8658 17572 10423 10580 13123  9389]\n",
            "[ 5980  4926  3965   326   243 14402  1465 13669  8111  6104]\n",
            "[17456  5971  1211 12764  2552 14878  2059 15918 10887  2982]\n",
            "[ 4040 13890  2597  1052   270  6304  7733  3144 16988  9746]\n",
            "[10056  9629  8290 12833  9149 14268  2792  9888  1479 16629]\n",
            "[16784 16007 17898  7822 12449   572 14636  4305 11708   964]\n",
            "[10477 16354 10186  6711  6782   715  8620 10329 13076 13599]\n"
          ]
        }
      ],
      "source": [
        "# Create a random list of indices with 10 items from each class for 2nd cossim\n",
        "cs_labels = cs_dataset[0].y.detach().numpy()\n",
        "cs_num_classes = 15\n",
        "num_samples = 10\n",
        "cs_indices = []\n",
        "for i in range(cs_num_classes):\n",
        "  class_i = np.random.choice(np.where(cs_labels == i)[0], size=num_samples, replace=False)\n",
        "  print(class_i)\n",
        "  cs_indices += class_i.tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmJvZoEaJn6F"
      },
      "source": [
        "# Arxiv Data Set Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQkYd6GGIAgD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabd9b16-f464-4889-f248-8852e0dd15d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:01<00:00, 43.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1737.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 4072.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343], y=[169343])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "d_name = \"ogbn-arxiv\"\n",
        "\n",
        "arxiv_dataset = PygNodePropPredDataset(name = d_name)\n",
        "\n",
        "split_idx = arxiv_dataset.get_idx_split()\n",
        "train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "arxiv_data = arxiv_dataset[0]\n",
        "arxiv_data.y = arxiv_data.y.squeeze()\n",
        "arxiv_data.node_year = arxiv_data.node_year.squeeze()\n",
        "arxiv_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2ekASiZJGcj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1fe1bd-ea6f-4405-e28c-a87fd7600469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 39838  83452 121005  72065 120836  33614 104457  92137 166835  89449]\n",
            "[163199  50487  66258  20164  55813 141377 164081 152616 131507   7907]\n",
            "[ 82068  73931 110838 157702 106276  55810 111240  52022 115509  93760]\n",
            "[129633  27137  85147  93966  26410  88265  54493 124833 129571   5939]\n",
            "[120595 165436  50616 111979 141603  80235  23864 155495 167339  43562]\n",
            "[147926  96304 168765 116695  29632  52958  42149 103225 150136  18575]\n",
            "[ 18358  26269 119274 164042  31333  96623 114025 163142 166729   6698]\n",
            "[ 46360 157799  96427 165400  11855  48412 114634 169026 131173   2824]\n",
            "[  4080  97610 103879  59235 153709  64641  69881 137656  41905 149905]\n",
            "[113501  54510  26145 116171 120055  13141  42119 131513 123927 135053]\n",
            "[ 54921  83081  29244 102887  48373  11656  42716   8950 156374  67367]\n",
            "[134831  35357  50249  98685  17244 126726  53801  36857 104449  51917]\n",
            "[162411  28709  28409  65874 126627  47486  53177 145824  92632  14455]\n",
            "[112676  23049 157197  10517  65275  56907  23397 147847 137612  94372]\n",
            "[165327  93357 158368 116037   3215  54723  77954 133135  59181  34527]\n"
          ]
        }
      ],
      "source": [
        "# Create a random list of indices with 10 items from each class for 2nd cossim\n",
        "arxiv_labels = arxiv_dataset[0].y.detach().numpy()\n",
        "arxiv_num_classes = 15\n",
        "num_samples = 10\n",
        "arxiv_indices = []\n",
        "for i in range(arxiv_num_classes):\n",
        "  class_i = np.random.choice(np.where(arxiv_labels == i)[0], size=num_samples, replace=False)\n",
        "  print(class_i)\n",
        "  cs_indices += class_i.tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpFfkagZrKpT"
      },
      "source": [
        "# Metric Calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vL3dj_plpMyw"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/SGDE2020/embedding_stability/blob/master/lib/tools/comparison.py\n",
        "\n",
        "class Comparison:\n",
        "\n",
        "    def __init__(self, emb_dir, embeddings, num_nodes, file_prefix):\n",
        "        \"\"\"\n",
        "        emb_dir: str, path where embeddings are saved\n",
        "        embeddings: list, list of strings that specify the embedding files\n",
        "        \"\"\"\n",
        "        self.dir = emb_dir\n",
        "        self.embeddings = embeddings\n",
        "        self.pairs = self._combinations(embeddings)\n",
        "        self.num_vertices = num_nodes\n",
        "\n",
        "        # use file names without numbering to mark result files\n",
        "        self.file_prefix = file_prefix\n",
        "\n",
        "    def _combinations(self, emb_list):\n",
        "        \"\"\" Computes all different comparison pairs of a list of embeddings \"\"\"\n",
        "        return [pair for pair in combinations(emb_list, 2)]\n",
        "\n",
        "    def _analyse_jaccard(self, queries, nodes, k, pair):\n",
        "        \"\"\"This function is called in the multiprocessing of jaccard score\"\"\"\n",
        "        indices_0 = np.asarray(queries[pair[0]])\n",
        "        indices_1 = np.asarray(queries[pair[1]])\n",
        "        nodes = np.asarray(nodes)\n",
        "        jaccard_score = {}\n",
        "        for i in range(len(nodes)):\n",
        "            jaccard_score[nodes[i]] = \\\n",
        "                len(np.intersect1d(indices_0[i, 1:(k + 1)], indices_1[i, 1:(k + 1)], assume_unique=True)) / \\\n",
        "                len(np.union1d(indices_0[i, 1:(k + 1)], indices_1[i, 1:(k + 1)]))\n",
        "        return list(jaccard_score.values())\n",
        "\n",
        "    def _analyse_second_cossim(self, queries, normed_embs, nodes, k, pair):\n",
        "        \"\"\"\n",
        "        This function is called in the multiprocessing of the second order cosine similarity.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert the indices of nearest neighbors back into numpy\n",
        "        indices_0 = np.asarray(queries[pair[0]])\n",
        "        indices_1 = np.asarray(queries[pair[1]])\n",
        "\n",
        "        # Convert the embeddings and nodes back into numpy\n",
        "        norm_emb_0 = np.asarray(normed_embs[pair[0]])\n",
        "        norm_emb_1 = np.asarray(normed_embs[pair[1]])\n",
        "        nodes = np.asarray(nodes)\n",
        "\n",
        "        # Compute the second order cosine similarity\n",
        "        pair_results = []\n",
        "        for i in range(len(nodes)):\n",
        "            # Build the set of nearest neighbors w.r.t. both embeddings\n",
        "            # Use indices from 1 to k+1, because the first entry will always be the node itself\n",
        "            neighbors_union = np.union1d(indices_0[i, 1:(k + 1)], indices_1[i, 1:(k + 1)])\n",
        "\n",
        "            # Vectors of cosine similarity values of nearest neighbors. There was an error in the original source code, did not use norm_emb_1 for m1\n",
        "            m0 = cos_sim(norm_emb_0[neighbors_union], norm_emb_0[nodes[i]].reshape(1, -1))\n",
        "            m1 = cos_sim(norm_emb_1[neighbors_union], norm_emb_1[nodes[i]].reshape(1, -1))\n",
        "\n",
        "            # Flatten output matrix\n",
        "            assert m0.shape[1] == 1 and m1.shape[1] == 1, \"m0 and m1 should only have a single variable in the second dimension\"\n",
        "            m0 = m0.flatten()\n",
        "            m1 = m1.flatten()\n",
        "\n",
        "            # Cosine similarity between similarity vectors\n",
        "            pair_results.append(float(1-cosine(m0,m1)))\n",
        "        return pair_results\n",
        "\n",
        "    def k_nearest_neighbors(self, nodes=None, append=False, samples=100, k=10, jaccard=False, load=False,\n",
        "                            kload_size=100, save=True, save_path=None, num_processes=4):\n",
        "        \"\"\"\n",
        "        Computes the k nearest neighbors to some specified nodes with respect to cosine similarity. As an intermediate\n",
        "        step, it computes the 100 nearest neighbors and saves them to file. Based on these neighbors, the k-nn overlaps\n",
        "        are computed.\n",
        "        Args:\n",
        "            nodes: list, that specifies the nodes\n",
        "            append: bool, if nodes are specified, whether additional nodes will be sampled and added to nodes\n",
        "            samples: int, number of nodes that will be sampled\n",
        "            k: int, number of neighbors that will be used in the comparison\n",
        "            jaccard: bool, whether jaccard score or overlap will be used as similarity measure\n",
        "            load: bool, whether a file of computed neighbors will be used\n",
        "            kload_size: Size of k in knn file to load\n",
        "            save: bool, whether the results should be saved in a text file\n",
        "            save_path: str, path where the file will be saved\n",
        "            num_processes: number of random processes in parallelization\n",
        "        Returns:\n",
        "            dict, \"nodes\": array of nodes, \"overlaps\": array of overlaps, columns are values per node; or array of\n",
        "            jaccard scores\n",
        "        \"\"\"\n",
        "        # Handle the nodes input: Whether to sample nodes, use given nodes, or both.\n",
        "        nodes = self._get_nodes(nodes, samples, append)\n",
        "\n",
        "        # Load k nearest neighbors to save time? Else we will have to compute them first.\n",
        "        if load:\n",
        "            file_name = self.file_prefix + \"_\" + str(kload_size) + \"nns.pickle\"\n",
        "            with open(os.path.join(save_path, file_name), \"rb\") as pickle_file:\n",
        "                queries = pickle.load(pickle_file)\n",
        "            assert list(queries.keys()) == self.embeddings, (\"Keys of loaded queries do not match\"\n",
        "                                                             \" with available embeddings\")\n",
        "\n",
        "        # Normalizing the embeddings to be able to use distance as a proxy for cosine similarity\n",
        "        # BallTree from sklearn is used to compute the neighbors efficiently\n",
        "        else:\n",
        "            queries = self.nearest_neighbors_queries(nodes, k, save_path)\n",
        "        # Store the naive overlaps for all pairs\n",
        "\n",
        "        print(f\"\\n\\n {queries.keys()} \\n\")\n",
        "\n",
        "        # Use multiprocessing to speed up overlap computation.\n",
        "        # Too much data is passed to the processes which makes it inefficient.\n",
        "        # Possibly, it is faster to store the data as a file as an intermediate step.\n",
        "        # only run if less than 100 neighbors are queried, as too large neighborhoods may cause memory issues when\n",
        "        # distributing tasks\n",
        "        if num_processes > 1 and k <= 100:\n",
        "            with mp.Pool(num_processes) as p:\n",
        "                # arguments passed in multiprocessing must be pickable\n",
        "                p_queries = queries\n",
        "                p_nodes = nodes.tolist()\n",
        "                if jaccard:\n",
        "                    multiprocess_func = partial(self._analyse_jaccard, p_queries, p_nodes, k)\n",
        "                else:\n",
        "                    multiprocess_func = partial(self._analyse_knn, p_queries, p_nodes, k)\n",
        "                li_overlap = []\n",
        "                for result in tqdm(p.imap(multiprocess_func, self.pairs), total=len(self.pairs)):\n",
        "                  li_overlap.append(result)\n",
        "        else:\n",
        "            if jaccard:\n",
        "                li_overlap = [self._analyse_jaccard(queries, nodes.tolist(), k, pair) for pair in self.pairs]\n",
        "            else:\n",
        "                li_overlap = [self._analyse_knn(queries, nodes.tolist(), k, pair) for pair in self.pairs]\n",
        "\n",
        "        # Convert the result into numpy\n",
        "        overlap = np.asarray(li_overlap)\n",
        "\n",
        "        # Save the results\n",
        "        if jaccard:\n",
        "            nodes_suffix = \"jaccard_nodes\"\n",
        "            scores_suffix = f\"{k}nn_jaccard\"\n",
        "        else:\n",
        "            nodes_suffix = \"overlap_nodes\"\n",
        "            scores_suffix = f\"{k}nn_overlap\"\n",
        "        if save is True:\n",
        "            np.save(os.path.join(save_path, f\"{self.file_prefix}_{nodes_suffix}\"), nodes)\n",
        "            np.save(os.path.join(save_path, f\"{self.file_prefix}_{scores_suffix}\"), overlap)\n",
        "        return {\"nodes\": nodes, \"overlaps\": overlap}\n",
        "\n",
        "    def jaccard_similarity(self, nodes=None, append=False, samples=100, k=10, load=False, kload_size=100, save=True,\n",
        "                           save_path=None, num_processes=4):\n",
        "        \"\"\"\n",
        "        Alias for k_nearest_neighbors with jaccard=True.\n",
        "        See k_nearest_neighbors for detailed documentation.\n",
        "        \"\"\"\n",
        "        return self.k_nearest_neighbors(nodes=nodes, append=append, samples=samples, k=k, jaccard=True, load=load,\n",
        "                                        kload_size=kload_size, save=save, save_path=save_path,\n",
        "                                        num_processes=num_processes)\n",
        "\n",
        "\n",
        "    def second_order_cosine_similarity(self, nodes=None, append=False, num_samples=1000, k=10, load=True, save=False,\n",
        "                                       save_path=None, num_processes=4):\n",
        "        \"\"\" Computes second order cosine similarity.\n",
        "        Args:\n",
        "            nodes: list, that specifies the nodes\n",
        "            append: bool, if nodes are specified, whether additional nodes will be sampled and added to nodes\n",
        "            num_samples: int, number of nodes that will be sampled\n",
        "            k: int, number of neighbors that will be used in the comparison\n",
        "            load: bool, whether to load nearest neighbors from file\n",
        "            save: bool, whether the results should be saved\n",
        "            save_path: str, path where the file will be saved\n",
        "            num_processes: number of random processes in parallelization\n",
        "        Returns:\n",
        "            nodes: numpy array of used nodes\n",
        "            results: numpy array of similarity values of size (number of embedding pairs, number of nodes)\n",
        "        \"\"\"\n",
        "\n",
        "        # Handle the nodes input: Whether to sample nodes, use given nodes, or both.\n",
        "        nodes = self._get_nodes(nodes, num_samples, append)\n",
        "\n",
        "        # Load required data: nearest neighbors, embeddings\n",
        "        normed_embs = {}\n",
        "\n",
        "        if load:\n",
        "            # Load nearest neighbors from file\n",
        "            file_name = self.file_prefix + \"_\" + str(k)+ \"nns.pickle\"\n",
        "            with open(os.path.join(save_path, file_name), \"rb\") as pickle_file:\n",
        "                queries = pickle.load(pickle_file)\n",
        "            assert list(queries.keys()) == self.embeddings, (\"Keys of loaded queries do not match with \"\n",
        "                                                             \"available embeddings\")\n",
        "            for emb in tqdm(self.embeddings, desc=\"Loading nearest neighbor files\"):\n",
        "                normed_embs[emb] = normalize(self.read_embedding(os.path.join(self.dir, emb)), norm='l2', copy=False)\n",
        "        else:\n",
        "            queries, normed_embs = self.nearest_neighbors_queries(nodes, k, save_path, return_embeddings=True)\n",
        "\n",
        "        # Start computation of second order cosine similarity\n",
        "        # arguments passed in multiprocessing must be pickable\n",
        "        p_normed_embs = dict([(key, norm_emb.tolist()) for key, norm_emb in normed_embs.items()])\n",
        "        p_nodes = nodes.tolist()\n",
        "        # Avoid multiprocessing on Colab, not sure if it works locally though\n",
        "        if num_processes > 1 and k <= 100:\n",
        "            with mp.Pool(num_processes) as p:\n",
        "                li_results = p.map(partial(self._analyse_second_cossim, queries, p_normed_embs, p_nodes, k), self.pairs)\n",
        "              # li_results = []\n",
        "              # partial_func = partial(self._analyse_second_cossim, queries, p_normed_embs, p_nodes, k) \n",
        "              # for result in tqdm(p.imap(partial_func, self.pairs), total=len(self.pairs)):\n",
        "              #   li_results.append(result)\n",
        "        else:\n",
        "          li_results = []\n",
        "          for pair in tqdm(self.pairs, desc=\"Comparing embeddings\"):\n",
        "            li_results.append(self._analyse_second_cossim(queries, p_normed_embs, p_nodes, k, pair))\n",
        "\n",
        "        results = np.asarray(li_results)\n",
        "\n",
        "        if save is True:\n",
        "            np.save(os.path.join(save_path, f\"{self.file_prefix}_{k}nn_2nd_order_cossim\"), results)\n",
        "\n",
        "        return nodes, results\n",
        "\n",
        "    def nearest_neighbors_queries(self, nodes, k, save_path, return_embeddings=False):\n",
        "        \"\"\"Uses a ball tree to compute the nearest neighbors in the embedding space\"\"\"\n",
        "        queries = {}\n",
        "        normed_embs = {}\n",
        "        # Normalizing the embeddings to be able to use distance as a proxy for cosine similarity\n",
        "        # BallTree from sklearn is used to compute the neighbors efficiently\n",
        "        if return_embeddings:\n",
        "            for emb in tqdm(self.embeddings, desc=\"Querying nearest neighbors\"):\n",
        "                normed_embs[emb] = normalize(self.read_embedding(os.path.join(self.dir, emb)), norm='l2', copy=False)\n",
        "                ball_tree = BallTree(normed_embs[emb], leaf_size=40)\n",
        "                queries[emb] = ball_tree.query(normed_embs[emb][nodes, :], k=k + 1, return_distance=False).tolist()\n",
        "        else:\n",
        "            for emb in tqdm(self.embeddings, desc=\"Querying nearest neighbors\"):\n",
        "                normalized_embedding = normalize(self.read_embedding(os.path.join(self.dir, emb)), norm='l2',\n",
        "                                                 copy=False)\n",
        "                ball_tree = BallTree(normalized_embedding, leaf_size=40)\n",
        "                # Query the k+1 nearest neighbors, because a node will always be the closest neighbor to itself\n",
        "                queries[emb] = ball_tree.query(normalized_embedding[nodes, :], k=k + 1, return_distance=False).tolist()\n",
        "\n",
        "        # Save the computed neighbors to be able to skip the computation\n",
        "        self.save_pickle(queries, save_path, self.file_prefix + \"_\" + str(k) + \"nns\")\n",
        "\n",
        "        if return_embeddings:\n",
        "            return queries, normed_embs\n",
        "        else:\n",
        "            return queries\n",
        "\n",
        "    def sample_nodes(self, k):\n",
        "        \"\"\"\n",
        "        Sample unique nodes of an embedding\n",
        "        Args:\n",
        "            k: int, number of nodes to sample\n",
        "        Returns:\n",
        "            numpy array of node ids of length k if k is smaller than the number of nodes available.\n",
        "            Otherwise, all available nodes are returned.\n",
        "        \"\"\"\n",
        "        vertices = np.arange(self.num_vertices)\n",
        "        np.random.shuffle(vertices)\n",
        "        return vertices[:min(k, self.num_vertices)]\n",
        "\n",
        "\n",
        "    def cossim_analysis(self, save_path):\n",
        "        \"\"\" Computes aligned cosine similarity values. Internally performs orthogonal transformation (Procrustes\n",
        "        problem) between two embeddings and saves transformation matrices as well as vector of resulting errors\n",
        "        \"\"\"\n",
        "\n",
        "        # Set up file naming\n",
        "        results_suffix = \"aligned_cossim\"\n",
        "\n",
        "        # Read the embeddings\n",
        "        normed_embs = {}\n",
        "        for emb in tqdm(self.embeddings, desc=\"Reading embeddings\"):\n",
        "            normed_embs[emb] = normalize(\n",
        "                    self.read_embedding(os.path.join(self.dir, emb))[np.arange(self.num_vertices)], norm='l2',\n",
        "                    copy=False)\n",
        "\n",
        "        # Do the analysis\n",
        "        emb_ind = -1\n",
        "        results = []\n",
        "        for pair in tqdm(self.pairs, desc=\"Comparing embeddings\"):\n",
        "\n",
        "            # only update first embedding if it does not change\n",
        "            if emb_ind != pair[0]:\n",
        "                emb_ind = pair[0]\n",
        "\n",
        "            W1 = normed_embs[emb_ind]\n",
        "\n",
        "            # transform W2 into W1 using procrustes matrix\n",
        "            Q, _ = orthogonal_procrustes(normed_embs[pair[1]], normed_embs[pair[0]], check_finite=False)\n",
        "            W2 = normed_embs[pair[1]].dot(Q)\n",
        "\n",
        "            # Do 1-cosine to get the actual cosine similarity instead of cosine difference\n",
        "            pair_results = np.array([1-cosine(W1[i], W2[i]) for i in range(self.num_vertices)])\n",
        "            results.append(pair_results)\n",
        "\n",
        "        results = np.asarray(results)\n",
        "        np.save(os.path.join(save_path, f\"{self.file_prefix}_{results_suffix}\"), results)\n",
        "        return np.arange(self.num_vertices), results\n",
        "\n",
        "    def unaligned_cosine_analysis(self, save_path):\n",
        "        \"\"\" Computes regular cosine similarity values, without alignment, between embedding pairs\n",
        "        \"\"\"\n",
        "\n",
        "        # Set up file naming\n",
        "        results_suffix = \"cossim\"\n",
        "\n",
        "        # Read the embeddings\n",
        "        normed_embs = {}\n",
        "        for emb in tqdm(self.embeddings, desc=\"Reading embeddings\"):\n",
        "            normed_embs[emb] = normalize(\n",
        "                    self.read_embedding(os.path.join(self.dir, emb))[np.arange(self.num_vertices)], norm='l2',\n",
        "                    copy=False)\n",
        "\n",
        "        # Do the analysis\n",
        "        emb_ind = -1\n",
        "        results = []\n",
        "        for pair in tqdm(self.pairs, desc=\"Comparing embeddings\"):\n",
        "\n",
        "            # only update first embedding if it does not change\n",
        "            if emb_ind != pair[0]:\n",
        "                emb_ind = pair[0]\n",
        "\n",
        "            # Get our two embedding matrics\n",
        "            W1 = normed_embs[emb_ind]\n",
        "            W2 = normed_embs[pair[1]]\n",
        "\n",
        "            # Do 1-cosine to get the actual cosine similarity instead of cosine distance\n",
        "            pair_results = np.array([1-cosine(W1[i], W2[i]) for i in range(self.num_vertices)])\n",
        "            results.append(pair_results)\n",
        "\n",
        "        results = np.asarray(results)\n",
        "        np.save(os.path.join(save_path, f\"{self.file_prefix}_{results_suffix}\"), results)\n",
        "        return np.arange(self.num_vertices), results\n",
        "\n",
        "    def pairwise_distance(self, nodes, save_path, norm=False):\n",
        "        \"\"\"Calculate the pairwise distance between all our embeddings. Save these to the given file path\"\"\"\n",
        "\n",
        "        results_suffix = \"euclidean_distance\"\n",
        "\n",
        "        # Read the embeddings\n",
        "        normed_embs = {}\n",
        "        for emb in tqdm(self.embeddings, desc=\"Reading embeddings\"):\n",
        "            # Option for calculating distance between normalized embeddings\n",
        "            if norm:\n",
        "              normed_embs[emb] = normalize(\n",
        "                      self.read_embedding(os.path.join(self.dir, emb))[np.arange(self.num_vertices)], norm='l2',\n",
        "                      copy=False)\n",
        "            else:\n",
        "              normed_embs[emb] = self.read_embedding(os.path.join(self.dir, emb))[np.arange(self.num_vertices)]\n",
        "\n",
        "        # Do the analysis\n",
        "        emb_ind = -1\n",
        "        results = []\n",
        "        for pair in tqdm(self.pairs, desc=\"Comparing embeddings\"):\n",
        "\n",
        "            # only update first embedding if it does not change\n",
        "            if emb_ind != pair[0]:\n",
        "                emb_ind = pair[0]\n",
        "\n",
        "            # Get the distance between the two matrices of embeddings and add to results array\n",
        "            results.append(np.linalg.norm(normed_embs[emb_ind] - normed_embs[pair[1]], axis=1))\n",
        "\n",
        "        results = np.asarray(results)\n",
        "        np.save(os.path.join(save_path, f\"{self.file_prefix}_{results_suffix}\"), results)\n",
        "\n",
        "\n",
        "    def _get_nodes(self, nodes, num_samples, append):\n",
        "        \"\"\"\n",
        "        Handles getting nodes for the experiments.\n",
        "        Args:\n",
        "            nodes: list, node ids\n",
        "            num_samples: int, how many nodes should be sampled\n",
        "            append: bool, whether to append sampled nodes to specified nodes\n",
        "        Returns:\n",
        "            numpy array of (sampled) node ids\n",
        "        \"\"\"\n",
        "        if nodes is None:\n",
        "            nodes = self.sample_nodes(num_samples)\n",
        "        elif append is True:\n",
        "            # allows specified nodes to be taken twice\n",
        "            nodes.extend(self.sample_nodes(num_samples))\n",
        "        return np.asarray(nodes)\n",
        "\n",
        "    def save_pickle(self, obj, save_path, file_name):\n",
        "        if save_path is None:\n",
        "            save_path = os.getcwd()\n",
        "        if file_name is None:\n",
        "            # generate name of report from embedding input: use name of first embedding file without number information\n",
        "            file_name = self.file_prefix\n",
        "        with open(os.path.join(save_path, f\"{file_name}.pickle\"), \"wb\") as f:\n",
        "            pickle.dump(obj, f)\n",
        "\n",
        "    def get_combinations(self):\n",
        "        return self.pairs\n",
        "\n",
        "    def get_vertex_count(self):\n",
        "        return self.num_vertices\n",
        "\n",
        "    # The original code reorders the nodes by ID, but our nodes have the same ordering so there is no need to reorder\n",
        "    def read_embedding(self, path):\n",
        "      node_embedding = torch.load(path, map_location=torch.device('cpu')).detach().numpy()\n",
        "      return node_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd9PN4KBZkYj"
      },
      "outputs": [],
      "source": [
        "# Code adapted from https://github.com/SGDE2020/embedding_stability/blob/master/similarity_tests/similarity_tests.py\n",
        "TESTS= [\"cossim\", \"jaccard\", \"2ndcos\", \"unalign_cossim\", \"dist\"]\n",
        "\n",
        "# Expects the the file names in the embedding directory to be of the form model-name_seed_emb.pt\n",
        "def run_tests(embedding_dir, file_prefix, tests=TESTS, num_nodes=-1, knn_size=20, load_knn=False, kload_size=None,\n",
        "                results_dir=None, num_processes=4, cossim_sec_indices=None):\n",
        "    \"\"\"\n",
        "    Run specified similarity tests.\n",
        "    Params:\n",
        "        embedding_dir: str, path to directory where embeddings are stored\n",
        "        file_prefix: the prefix for files for saving results\n",
        "        tests: list of str, tests that will be conducted\n",
        "        num_nodes: int, number of nodes of a graph that are used in the tests (just the number of nodes in the graph)\n",
        "        knn_size: int, neighborhood size for knn test\n",
        "        load_knn: bool, whether a stored knn matrix should be loaded\n",
        "        nodeinfo_dir = str, path to directory where nodeinfo is stored (tables)\n",
        "        results_dir = str, path to directory where results will be saved to\n",
        "    \"\"\"\n",
        "    # Store results in the embedding directory in a results file if no other location is specified\n",
        "    if results_dir is None:\n",
        "      results_dir = embedding_dir + \"results/\"\n",
        "    # Create results directory if it does not exist\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.mkdir(results_dir) \n",
        "\n",
        "    # Computes a list of all _emb.pt files in a given directory. Assumes the directory being used only contains results from one architecture and data set\n",
        "    fnames = sorted([f for f in os.listdir(embedding_dir) if f.endswith(\"_emb.pt\")])\n",
        "    print(fnames)\n",
        "\n",
        "    if len(fnames) <= 1:\n",
        "        print(f\"Did not find any embeddings for in directory {embedding_dir}\")\n",
        " \n",
        "    else:\n",
        "        # To be compatible with the original source code, we need a list of nodes indices we will be using in our metric computations\n",
        "        # We use all the nodes\n",
        "        nodes = [i for i in range(num_nodes)]\n",
        "\n",
        "        # Allow us to use specific nodes for the second order cossine similarity as it can be very computational expensive\n",
        "        cossim_nodes = nodes if cossim_sec_indices is None else cossim_sec_indices\n",
        "\n",
        "        if load_knn and kload_size is None:\n",
        "            kload_size = knn_size\n",
        "\n",
        "        # Start tests\n",
        "        comp = Comparison(emb_dir=embedding_dir, embeddings=fnames, num_nodes=num_nodes, file_prefix=file_prefix)\n",
        "        if \"cossim\" in tests:\n",
        "            print(\"Executing cosine similarity\")\n",
        "            comp.cossim_analysis(save_path=results_dir)\n",
        "        if \"jaccard\" in tests:\n",
        "            print(\"Executing jaccard score\")\n",
        "            comp.jaccard_similarity(\n",
        "                nodes=nodes, append=False, k=knn_size,\n",
        "                load=load_knn, kload_size=kload_size, save=True, save_path=results_dir, num_processes=num_processes\n",
        "            )\n",
        "        if \"2ndcos\" in tests:\n",
        "            print(\"Executing second order cosine similarity\")\n",
        "            comp.second_order_cosine_similarity(\n",
        "                nodes=cossim_nodes, append=False, k=knn_size,\n",
        "                save=True, save_path=results_dir, num_processes=num_processes, load=load_knn\n",
        "            )\n",
        "        if \"unalign_cossim\" in tests:\n",
        "            print(\"Executing cosine similarity\")\n",
        "            comp.unaligned_cosine_analysis(save_path=results_dir)\n",
        "        if \"dist\" in tests:\n",
        "            print(\"Executing euclidean distance\")\n",
        "            comp.pairwise_distance(nodes=nodes, save_path=results_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTsS-KbhxRVY"
      },
      "source": [
        "# Run metric calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "p4qF6bbXxBlU",
        "outputId": "ad5dee2e-6bab-49ff-94fb-4b227dded874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['GIN_Cora_Layers_10_1024258131_emb.pt', 'GIN_Cora_Layers_10_116846604_emb.pt', 'GIN_Cora_Layers_10_1221215631_emb.pt', 'GIN_Cora_Layers_10_1517964140_emb.pt', 'GIN_Cora_Layers_10_170173784_emb.pt', 'GIN_Cora_Layers_10_1818027900_emb.pt', 'GIN_Cora_Layers_10_1860537279_emb.pt', 'GIN_Cora_Layers_10_1863727779_emb.pt', 'GIN_Cora_Layers_10_2036056847_emb.pt', 'GIN_Cora_Layers_10_2105922959_emb.pt', 'GIN_Cora_Layers_10_2135956485_emb.pt', 'GIN_Cora_Layers_10_2342954646_emb.pt', 'GIN_Cora_Layers_10_2455059856_emb.pt', 'GIN_Cora_Layers_10_2739899259_emb.pt', 'GIN_Cora_Layers_10_2794978777_emb.pt', 'GIN_Cora_Layers_10_2952735006_emb.pt', 'GIN_Cora_Layers_10_3300171104_emb.pt', 'GIN_Cora_Layers_10_3303475786_emb.pt', 'GIN_Cora_Layers_10_361232447_emb.pt', 'GIN_Cora_Layers_10_3647665043_emb.pt', 'GIN_Cora_Layers_10_3692371949_emb.pt', 'GIN_Cora_Layers_10_3710910636_emb.pt', 'GIN_Cora_Layers_10_400225693_emb.pt', 'GIN_Cora_Layers_10_4083009686_emb.pt', 'GIN_Cora_Layers_10_4193977854_emb.pt', 'GIN_Cora_Layers_10_516507873_emb.pt', 'GIN_Cora_Layers_10_572297925_emb.pt', 'GIN_Cora_Layers_10_806299656_emb.pt', 'GIN_Cora_Layers_10_880019963_emb.pt', 'GIN_Cora_Layers_10_89475662_emb.pt']\n",
            "Executing cosine similarity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading embeddings:  63%|██████▎   | 19/30 [00:07<00:04,  2.45it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3498f0f09b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# model_file_prefix = \"GraphSAGE_analysis\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Run the tests for tests=[\"cossim\", \"jaccard\"], multiple processes work for these\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mrun_tests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_file_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cossim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jaccard\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unalign_cossim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dist\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_NODES_CORA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;31m# Run the tests for tests=[\"2ndcos\"], multiple processes does not work for this so only run on 1, but we can load the nearest neighbours from the previous metric calculations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mrun_tests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_file_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"2ndcos\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_NODES_CORA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_knn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-48012eec85c9>\u001b[0m in \u001b[0;36mrun_tests\u001b[0;34m(embedding_dir, file_prefix, tests, num_nodes, knn_size, load_knn, kload_size, results_dir, num_processes, cossim_sec_indices)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"cossim\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Executing cosine similarity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mcomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcossim_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"jaccard\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Executing jaccard score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-fa889ee3a08c>\u001b[0m in \u001b[0;36mcossim_analysis\u001b[0;34m(self, save_path)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0memb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Reading embeddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             normed_embs[emb] = normalize(\n\u001b[0;32m--> 269\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_vertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                     copy=False)\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-fa889ee3a08c>\u001b[0m in \u001b[0;36mread_embedding\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;31m# The original code reorders the nodes by ID, but our nodes have the same ordering so there is no need to reorder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m       \u001b[0mnode_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnode_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;31m# If we want to actually tail call to torch.jit.load, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mbyte\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mread_bytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "NUM_NODES_CORA = cora_data.x.shape[0]\n",
        "# CHANGE these to match you directory naming structure\n",
        "model_directories = [\"GIN_Cora_Layers_10\", \"GIN_Cora_Layers_5\", \"GAT_Cora_Layers_10\", \"GAT_Cora_Layers_5\", \"GraphSAGE_Cora_Layers_10\", \"GraphSAGE_Cora_Layers_5\"]\n",
        "for model_dir in model_directories:\n",
        "  model_directory = model_dir\n",
        "  model_file_prefix = model_directory+\"_analysis\"\n",
        "  # model_directory = \"GraphSAGE/\"\n",
        "  # model_file_prefix = \"GraphSAGE_analysis\"\n",
        "  # Run the tests for tests=[\"cossim\", \"jaccard\"], multiple processes work for these\n",
        "  run_tests(embedding_dir=file_path+model_directory, file_prefix=model_file_prefix, tests=[\"cossim\", \"jaccard\", \"unalign_cossim\", \"dist\"], num_nodes=NUM_NODES_CORA, num_processes=16)\n",
        "  # Run the tests for tests=[\"2ndcos\"], multiple processes does not work for this so only run on 1, but we can load the nearest neighbours from the previous metric calculations\n",
        "  run_tests(embedding_dir=file_path+model_directory, file_prefix=model_file_prefix, tests=[\"2ndcos\"], num_nodes=NUM_NODES_CORA, num_processes=1, load_knn=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JcbF5Mo5SRIU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "isXLxy0ywLtq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1plphzXZvaC"
      },
      "source": [
        "# Compute metric averages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR5yJTLlaULL"
      },
      "outputs": [],
      "source": [
        "def read_and_average_metric(file_suffix, class_labels, num_classes):\n",
        "  results_arr = np.load(file_path+model_directory+\"/results/\"+model_file_prefix+\"_\"+file_suffix+\".npy\", mmap_mode='r')\n",
        "  results_flat = results_arr.flatten() \n",
        "  # Compute the 5 number summary for the data across all classes, better than mean and std for potentially skewed distribution\n",
        "  q1, q2, q3 = np.percentile(results_flat, [25,50,75])\n",
        "  min, max = results_flat.min(), results_flat.max()\n",
        "  # Print results\n",
        "  print(\"TOTAL STATS\")\n",
        "  print(\"q1=\" + str(q1) + \", q2=\" + str(q2) + \", q3=\" + str(q3) + \", min=\" + str(min) + \", max=\" + str(max))\n",
        "\n",
        "  for i in range(num_classes):\n",
        "    indices = np.where(class_labels==i)[0]\n",
        "    # Filter the results by class\n",
        "    class_vals = results_arr[:, indices]\n",
        "    # Compute stats for the class\n",
        "    q1_c, q2_c, q3_c = np.percentile(class_vals, [25,50,75])\n",
        "    min_c, max_c = class_vals.min(), class_vals.max()\n",
        "    # Print results\n",
        "    print(\"CLASS STATS \" + str(i))\n",
        "    print(\"q1=\" + str(q1_c), \", q2=\" + str(q2_c) + \", q3=\" + str(q3_c) + \", min=\" + str(min_c) + \", max=\" + str(max_c))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dYdz2YuVVjV"
      },
      "outputs": [],
      "source": [
        "# Read and compute aligned cosine metric\n",
        "read_and_average_metric(\"aligned_cossim\", cora_labels, cora_num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xQJ4uQPZ0Va"
      },
      "outputs": [],
      "source": [
        "# Read and compute jaccard metric\n",
        "read_and_average_metric(\"20nn_jaccard\", cora_labels, cora_num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPjICoJ9Z1rX"
      },
      "outputs": [],
      "source": [
        "# Read and compute 2nd order cosine similarity metric\n",
        "read_and_average_metric(\"20nn_2nd_order_cossim\", cora_labels, cora_num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqHNfK-ySM4S"
      },
      "outputs": [],
      "source": [
        "# Read and compute 2nd order cosine similarity metric\n",
        "read_and_average_metric(\"cossim\", cora_labels, cora_num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1PKJKLVS19A"
      },
      "outputs": [],
      "source": [
        "read_and_average_metric(\"euclidean_distance\", cora_labels, cora_num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOoPSZmgMrkZ"
      },
      "source": [
        "To do:\n",
        "- Store the nodes being used if we're only using a subset\n",
        "- Subsets for second_order_cosine_similarity for larger dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prediction instability"
      ],
      "metadata": {
        "id": "6GB4p-UFGKOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import seaborn as sns\n",
        "import matplotlib.figure\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Optional, Union\n",
        "import logging\n",
        "from torch_geometric.typing import Adj\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "def max_pi(acc1: float, acc2: float) -> float:\n",
        "    return min(1.0, 2.0 - acc1 - acc2)\n",
        "\n",
        "\n",
        "def min_pi(acc1: float, acc2: float) -> float:\n",
        "    return abs(acc1 - acc2)\n",
        "\n",
        "def savefig(\n",
        "    fig: Union[matplotlib.figure.Figure, sns.FacetGrid],\n",
        "    path: Union[str, Path],\n",
        "    fname: Optional[str] = None,\n",
        "):\n",
        "    if os.path.isdir(path):\n",
        "        if fname is None:\n",
        "            raise ValueError(\n",
        "                \"Path to save at is a directory, but no filename is specified\"\n",
        "            )\n",
        "        fig.savefig(Path(path, fname))\n",
        "    else:\n",
        "        fig.savefig(path)\n",
        "\n",
        "def save_pairwise_instability_distribution(\n",
        "    diffs: np.ndarray, savepath: Optional[Union[str, Path]] = None\n",
        "):\n",
        "    if savepath is not None:\n",
        "        try:\n",
        "            g = sns.displot(x=diffs)\n",
        "            g.set_xlabels(\"Pairwise instable predictions\")\n",
        "            savefig(g, savepath, fname=\"pairwise_instable.jpg\")\n",
        "            plt.close()\n",
        "        except Exception as e:  # should only be numpy.core._exceptions.MemoryError\n",
        "            log.error(\"Histogram failed (%s)\", e)\n",
        "\n",
        "def normalized_pairwise_instability(\n",
        "    preds: np.ndarray, accs: np.ndarray, figurepath = None\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Calculate the min-max normalized pairwise instability.\n",
        "    Args:\n",
        "        preds (np.ndarray): Predictions of shape (n_models, n_nodes)\n",
        "        accs (np.ndarray): Model accuracies of shape (n_models)\n",
        "        figurepath (Optional[Path]): Path to save a plot of the distribution to. Defaults to None.\n",
        "    \"\"\"\n",
        "    assert (\n",
        "        preds.shape[0] == accs.shape[0]\n",
        "    ), f\"preds and accs must have same size of first dimension: {preds.shape[0]=}, {accs.shape[0]=}\"\n",
        "\n",
        "    diffs = []\n",
        "    for i, j in itertools.combinations(np.arange(preds.shape[0]), 2):\n",
        "        x, y = preds[i], preds[j]\n",
        "        diff = np.not_equal(x, y).sum() / len(x)\n",
        "\n",
        "        acc1, acc2 = accs[i], accs[j]\n",
        "        lower = min_pi(acc1, acc2)\n",
        "        upper = max_pi(acc1, acc2)\n",
        "        diff = (diff - lower) / (upper - lower)\n",
        "\n",
        "        diffs.append(diff)\n",
        "    diffs = np.asarray(diffs)\n",
        "\n",
        "    if figurepath is not None and figurepath.is_dir():\n",
        "        figurepath = Path(figurepath, \"rel_pi.jpg\")\n",
        "\n",
        "    save_pairwise_instability_distribution(\n",
        "        diffs, savepath=figurepath\n",
        "    )\n",
        "    return diffs\n",
        "\n",
        "def get_test_accuracies(model_directory):\n",
        "    objects = []\n",
        "    with (open(file_path+model_directory+\"/\"+model_directory+\".pkl\", \"rb\")) as openfile:\n",
        "        while True:\n",
        "            try:\n",
        "                objects.append(pickle.load(openfile))\n",
        "            except EOFError:\n",
        "                break\n",
        "    logs = objects[-30:]\n",
        "    test_accuracies = []\n",
        "    for log in logs:\n",
        "      test_accuracies.append((str(log[0]), log[1].item()))\n",
        "    return sorted(test_accuracies)"
      ],
      "metadata": {
        "id": "Tjw9gC44GJar"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GIN\n",
        "\n",
        "class GINWrapper(GIN):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n",
        "    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n",
        "    super().__init__(in_channels, hidden_channels, num_layers)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output, x"
      ],
      "metadata": {
        "id": "brlDd1LLVUE8"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_NODES_CORA = cs_data.x.shape[0]\n",
        "# CHANGE these to match you directory naming structure\n",
        "model_directory = \"GIN_Coauthor\"\n",
        "\n",
        "embedding_dir = file_path+model_directory\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model =  GINWrapper(in_channels = cs_data.x.shape[-1], hidden_channels = 128, num_layers=1, out_channels=cs_num_classes).to(device)\n",
        "cs_data = cs_data.to(device)"
      ],
      "metadata": {
        "id": "VCtuWP_SMLJs"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "def run_prediction(embedding_dir, model_raw, dataset, results_dir=None):\n",
        "    results_dir = None\n",
        "    # Store results in the embedding directory in a results file if no other location is specified\n",
        "    if results_dir is None:\n",
        "      results_dir = embedding_dir + \"results/\"\n",
        "    # Create results directory if it does not exist\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.mkdir(results_dir) \n",
        "\n",
        "    # find all model files\n",
        "    fnames = sorted([f for f in os.listdir(embedding_dir) if f.endswith(\"_model.pt\")])\n",
        "    print(fnames)\n",
        "    predictions = []\n",
        "    model = model_raw\n",
        "    for emb in fnames:\n",
        "        full_path = os.path.join(file_path+model_directory+\"/\"+emb)\n",
        "        model.load_state_dict(torch.load(full_path))\n",
        "        #print(\"asdf\")\n",
        "\n",
        "        # Now, get the prediction logits for the dataset\n",
        "        loader = DataLoader(dataset, batch_size=1)\n",
        "        logits = []\n",
        "\n",
        "        for data in loader:\n",
        "            data = data   #.to('cuda')  # use cuda if needed\n",
        "            model = model #.to('cuda')  # use cuda if needed\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                out, x = model(data.x.to(device), data.edge_index.to(device))\n",
        "                logits.append(out.cpu().numpy())\n",
        "\n",
        "        predictions.append(logits)\n",
        "        #print(len(logits))\n",
        "    predictions = np.stack(predictions).squeeze()\n",
        "    return predictions\n",
        "predictions = run_prediction(embedding_dir, model, cs_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9RQ9s_kVdLo",
        "outputId": "cf0fc544-f7dc-48c5-e41e-01817f88cedf"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['GIN_Coauthor_1024258131_model.pt', 'GIN_Coauthor_116846604_model.pt', 'GIN_Coauthor_1221215631_model.pt', 'GIN_Coauthor_1517964140_model.pt', 'GIN_Coauthor_170173784_model.pt', 'GIN_Coauthor_1818027900_model.pt', 'GIN_Coauthor_1860537279_model.pt', 'GIN_Coauthor_1863727779_model.pt', 'GIN_Coauthor_2036056847_model.pt', 'GIN_Coauthor_2105922959_model.pt', 'GIN_Coauthor_2135956485_model.pt', 'GIN_Coauthor_2342954646_model.pt', 'GIN_Coauthor_2455059856_model.pt', 'GIN_Coauthor_2739899259_model.pt', 'GIN_Coauthor_2794978777_model.pt', 'GIN_Coauthor_2952735006_model.pt', 'GIN_Coauthor_3300171104_model.pt', 'GIN_Coauthor_3303475786_model.pt', 'GIN_Coauthor_361232447_model.pt', 'GIN_Coauthor_3647665043_model.pt', 'GIN_Coauthor_3692371949_model.pt', 'GIN_Coauthor_3710910636_model.pt', 'GIN_Coauthor_400225693_model.pt', 'GIN_Coauthor_4083009686_model.pt', 'GIN_Coauthor_4193977854_model.pt', 'GIN_Coauthor_516507873_model.pt', 'GIN_Coauthor_572297925_model.pt', 'GIN_Coauthor_806299656_model.pt', 'GIN_Coauthor_880019963_model.pt', 'GIN_Coauthor_89475662_model.pt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Infl8II0XAg3",
        "outputId": "c2d4d413-8e5f-4a65-f717-3b8043e63ed2"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 18333, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = get_test_accuracies(model_directory)\n",
        "acc = [x[1] for x in accuracies]\n",
        "acc = np.stack(acc)\n",
        "print(accuracies)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXMxZIW0ahxA",
        "outputId": "5ac0b2c7-eff3-47bb-fe05-f9c662a165ac"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('1024258131', 88.31698608398438), ('116846604', 88.83441925048828), ('1221215631', 89.5969467163086), ('1517964140', 88.04466247558594), ('170173784', 89.4880142211914), ('1818027900', 89.40631866455078), ('1860537279', 88.34422302246094), ('1863727779', 90.0599136352539), ('2036056847', 89.51525115966797), ('2105922959', 87.52723693847656), ('2135956485', 88.12635803222656), ('2342954646', 88.07189178466797), ('2455059856', 89.89651489257812), ('2739899259', 88.07189178466797), ('2794978777', 88.80718994140625), ('2952735006', 87.09149932861328), ('3300171104', 89.40631866455078), ('3303475786', 88.28975677490234), ('361232447', 88.42591857910156), ('3647665043', 87.88126373291016), ('3692371949', 90.08714294433594), ('3710910636', 89.84204864501953), ('400225693', 88.77995300292969), ('4083009686', 88.99781799316406), ('4193977854', 87.66339111328125), ('516507873', 89.76034545898438), ('572297925', 88.80718994140625), ('806299656', 87.63616180419922), ('880019963', 89.05228424072266), ('89475662', 88.48039245605469)]\n",
            "[88.31698608 88.83441925 89.59694672 88.04466248 89.48801422 89.40631866\n",
            " 88.34422302 90.05991364 89.51525116 87.52723694 88.12635803 88.07189178\n",
            " 89.89651489 88.07189178 88.80718994 87.09149933 89.40631866 88.28975677\n",
            " 88.42591858 87.88126373 90.08714294 89.84204865 88.779953   88.99781799\n",
            " 87.66339111 89.76034546 88.80718994 87.6361618  89.05228424 88.48039246]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_pairwise_instability(predictions, acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtlP2EIcJzu7",
        "outputId": "2d431d92-cfc9-4921-bda9-149b445c7d56"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.08244244, -0.07742953, -0.08433454, -0.07814037, -0.07867462,\n",
              "       -0.08571124, -0.07442783, -0.07796247, -0.08137163, -0.08480235,\n",
              "       -0.08449047, -0.07548367, -0.08449047, -0.08262305, -0.07887648,\n",
              "       -0.07867462, -0.08573802, -0.08516392, -0.08339888, -0.07425226,\n",
              "       -0.07583649, -0.08280383, -0.08136093, -0.08215128, -0.07636654,\n",
              "       -0.08262305, -0.08199536, -0.08100132, -0.08479954, -0.08034968,\n",
              "       -0.08089223, -0.08106411, -0.08160107, -0.08259748, -0.0773328 ,\n",
              "       -0.08088532, -0.07794677, -0.08135728, -0.08104723, -0.07839399,\n",
              "       -0.08104723, -0.08523293, -0.07546632, -0.08160107, -0.08228743,\n",
              "       -0.08306254, -0.07996207, -0.07715634, -0.07874858, -0.08507789,\n",
              "       -0.08430096, -0.07872183, -0.07928131, -0.08523293, -0.07856682,\n",
              "       -0.08393953, -0.08337263, -0.07589266, -0.08403827, -0.08357721,\n",
              "       -0.07758324, -0.08161378, -0.08419198, -0.07297255, -0.07635371,\n",
              "       -0.07604633, -0.08268284, -0.07604633, -0.08019601, -0.07051345,\n",
              "       -0.08357721, -0.07727586, -0.07804429, -0.07497051, -0.08143601,\n",
              "       -0.08304006, -0.0800423 , -0.08127183, -0.07374094, -0.08357674,\n",
              "       -0.08019601, -0.07358727, -0.08157921, -0.07835172, -0.07660161,\n",
              "       -0.07713444, -0.08415233, -0.07289895, -0.07642418, -0.08319048,\n",
              "       -0.08561304, -0.08597936, -0.07395199, -0.08597936, -0.08107236,\n",
              "       -0.08068753, -0.07713444, -0.08451682, -0.08360646, -0.08522407,\n",
              "       -0.07272385, -0.07430386, -0.08125266, -0.0798136 , -0.08397257,\n",
              "       -0.0748325 , -0.08107236, -0.08381616, -0.07945494, -0.08324305,\n",
              "       -0.08429562, -0.07829427, -0.08100222, -0.08457731, -0.0736779 ,\n",
              "       -0.07706323, -0.07675547, -0.08207014, -0.07675547, -0.08091026,\n",
              "       -0.07121578, -0.08429562, -0.07798651, -0.07875589, -0.07567833,\n",
              "       -0.08082463, -0.08242699, -0.08075635, -0.0819874 , -0.07444724,\n",
              "       -0.08296311, -0.08091026, -0.07429338, -0.08229516, -0.07906369,\n",
              "       -0.07882867, -0.08054356, -0.08411583, -0.07420803, -0.07759649,\n",
              "       -0.07728844, -0.08161064, -0.07728844, -0.08144707, -0.07174363,\n",
              "       -0.08483557, -0.07852062, -0.07929071, -0.07621031, -0.08036612,\n",
              "       -0.08196721, -0.08129303, -0.08252521, -0.07497808, -0.08250291,\n",
              "       -0.08144707, -0.07482408, -0.08283325, -0.0795988 , -0.07458074,\n",
              "       -0.07811632, -0.08119034, -0.08461999, -0.0843082 , -0.07563687,\n",
              "       -0.0843082 , -0.08277815, -0.07869597, -0.07882867, -0.08555536,\n",
              "       -0.08531969, -0.08321696, -0.07440513, -0.07598977, -0.08295898,\n",
              "       -0.08151569, -0.08196975, -0.07651997, -0.08277815, -0.08181388,\n",
              "       -0.08115598, -0.08495521, -0.08115513, -0.06999402, -0.07335761,\n",
              "       -0.07305182, -0.08329562, -0.07305182, -0.07717993, -0.06754771,\n",
              "       -0.08054356, -0.07427496, -0.0750394 , -0.0719816 , -0.08403441,\n",
              "       -0.08298983, -0.07702702, -0.07825016, -0.07075842, -0.08253114,\n",
              "       -0.07717993, -0.07060555, -0.07855594, -0.07534523, -0.07350138,\n",
              "       -0.07688566, -0.07657799, -0.08222334, -0.07657799, -0.0807315 ,\n",
              "       -0.07104001, -0.08411583, -0.07780866, -0.0785778 , -0.07550118,\n",
              "       -0.08097719, -0.08258028, -0.08057765, -0.08180831, -0.07427048,\n",
              "       -0.08311654, -0.0807315 , -0.07411667, -0.08211598, -0.07888551,\n",
              "       -0.08264364, -0.0830081 , -0.07104172, -0.0830081 , -0.07812599,\n",
              "       -0.08415999, -0.07420803, -0.08155298, -0.08064724, -0.08428729,\n",
              "       -0.06981981, -0.0713918 , -0.07830537, -0.0768736 , -0.0857562 ,\n",
              "       -0.07191777, -0.07812599, -0.0859403 , -0.07651677, -0.08028536,\n",
              "       -0.0857693 , -0.07441148, -0.0857693 , -0.08153756, -0.08014303,\n",
              "       -0.07759649, -0.08498478, -0.08407369, -0.08467533, -0.07318236,\n",
              "       -0.07476364, -0.081718  , -0.08027779, -0.083425  , -0.0752927 ,\n",
              "       -0.08153756, -0.08326874, -0.07991885, -0.08370999, -0.07410514,\n",
              "       -0.08613572, -0.08122741, -0.08050593, -0.07728844, -0.08467279,\n",
              "       -0.08376219, -0.08504106, -0.07287667, -0.0744571 , -0.08140776,\n",
              "       -0.07996831, -0.08378995, -0.07498589, -0.08122741, -0.08363359,\n",
              "       -0.07960956, -0.08339868, -0.07410514, -0.07824083, -0.0685909 ,\n",
              "       -0.08161064, -0.07533052, -0.07609637, -0.07303295, -0.08311734,\n",
              "       -0.08406142, -0.07808764, -0.07931303, -0.07180752, -0.08360187,\n",
              "       -0.07824083, -0.07165437, -0.07961937, -0.07640276, -0.08122741,\n",
              "       -0.08050593, -0.07728844, -0.08467279, -0.08376219, -0.08504106,\n",
              "       -0.07287667, -0.0744571 , -0.08140776, -0.07996831, -0.08378995,\n",
              "       -0.07498589, -0.08122741, -0.08363359, -0.07960956, -0.08339868,\n",
              "       -0.07564477, -0.08144707, -0.082468  , -0.08324335, -0.08014192,\n",
              "       -0.07700352, -0.07859534, -0.08525932, -0.08414625, -0.07890129,\n",
              "       -0.07912793, -0.08541442, -0.07874624, -0.08378491, -0.08355354,\n",
              "       -0.07174363, -0.07905706, -0.0781552 , -0.08177963, -0.06737424,\n",
              "       -0.06893949, -0.07582338, -0.07439776, -0.08324223, -0.06946319,\n",
              "       -0.07564477, -0.08342554, -0.07404246, -0.07779519, -0.07852062,\n",
              "       -0.07929071, -0.07621031, -0.08036612, -0.08196721, -0.08129303,\n",
              "       -0.08252521, -0.07497808, -0.08250291, -0.08144707, -0.07482408,\n",
              "       -0.08283325, -0.0795988 , -0.08500819, -0.08358087, -0.07409943,\n",
              "       -0.07568324, -0.08264873, -0.08120621, -0.08233288, -0.07621315,\n",
              "       -0.082468  , -0.08217691, -0.0808467 , -0.08464391, -0.08267197,\n",
              "       -0.07486364, -0.07644955, -0.08342432, -0.08197988, -0.08142592,\n",
              "       -0.07698017, -0.08324335, -0.0812702 , -0.08161988, -0.08542215,\n",
              "       -0.07180677, -0.07338425, -0.08032193, -0.07888517, -0.08507086,\n",
              "       -0.07391205, -0.08014192, -0.08491415, -0.07852709, -0.08230914,\n",
              "       -0.08281134, -0.07685065, -0.07807342, -0.07058397, -0.08235309,\n",
              "       -0.07700352, -0.07043114, -0.07837911, -0.07516937, -0.07844205,\n",
              "       -0.07966818, -0.07215808, -0.08395966, -0.07859534, -0.07200483,\n",
              "       -0.07997472, -0.07675582, -0.08399149, -0.07908092, -0.0789745 ,\n",
              "       -0.08525932, -0.07892582, -0.08363025, -0.08373461, -0.07764723,\n",
              "       -0.08020176, -0.08414625, -0.07749251, -0.08486738, -0.0822894 ,\n",
              "       -0.07268474, -0.07890129, -0.08638463, -0.07728991, -0.08106387,\n",
              "       -0.07912793, -0.07253136, -0.08050858, -0.07728703, -0.07874624,\n",
              "       -0.08378491, -0.08355354, -0.07713529, -0.08090824, -0.08192921])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Accuracy stats"
      ],
      "metadata": {
        "id": "klJNlty8FkxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import statistics\n",
        "\n",
        "def get_test_accuracy_stats(model_directory):\n",
        "    objects = []\n",
        "    with (open(file_path+model_directory+\"/\"+model_directory+\".pkl\", \"rb\")) as openfile:\n",
        "        while True:\n",
        "            try:\n",
        "                objects.append(pickle.load(openfile))\n",
        "            except EOFError:\n",
        "                break\n",
        "    logs = objects[-30:]\n",
        "    test_accuracies = []\n",
        "    for log in logs:\n",
        "      test_accuracies.append(log[1].item())\n",
        "    return test_accuracies\n",
        "def accuracy_stats(model_directory):\n",
        "    test_accuracies = get_test_accuracy_stats(model_directory)\n",
        "    print(test_accuracies)\n",
        "\n",
        "    print(\"std\", statistics.stdev(test_accuracies))\n",
        "    print(\"mean\", statistics.mean(test_accuracies))\n",
        "    print(\"max\", max(test_accuracies))\n",
        "    print(\"min\", min(test_accuracies))"
      ],
      "metadata": {
        "id": "GK-1soEBE0wr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_NODES_CORA = cora_data.x.shape[0]\n",
        "# CHANGE these to match you directory naming structure\n",
        "model_directory = \"GIN_Coauthor_Layers_5\"\n",
        "accuracy_stats(model_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmiIlXGsm1xZ",
        "outputId": "53682314-5589-46ff-9680-34297a8f6133"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[70.3431396484375, 49.48257064819336, 53.94880294799805, 54.520694732666016, 59.12309265136719, 37.03703689575195, 42.374725341796875, 37.636165618896484, 43.27341842651367, 43.845314025878906, 50.38125991821289, 57.053375244140625, 40.71350860595703, 50.245094299316406, 54.08496856689453, 56.01852035522461, 56.50871276855469, 66.69389343261719, 37.69063186645508, 48.80174255371094, 58.22439956665039, 46.568626403808594, 33.38779830932617, 55.93682098388672, 65.08714294433594, 59.09585952758789, 60.375816345214844, 49.1285400390625, 64.59695434570312, 66.6394271850586]\n",
            "std 9.826307514633445\n",
            "mean 52.29393513997396\n",
            "max 70.3431396484375\n",
            "min 33.38779830932617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "97nOMZOjFLH-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}