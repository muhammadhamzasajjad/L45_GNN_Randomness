{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["phShuTT_rlrx"],"authorship_tag":"ABX9TyNVz/mTMWkdSn/4ha4wpLUV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"vkIhGHpZrFwh"}},{"cell_type":"code","source":["import torch\n","!pip install torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":818},"id":"YhaqcWyUq80L","executionInfo":{"status":"ok","timestamp":1678718977823,"user_tz":0,"elapsed":27835,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"e0a2e21e-c699-4ba5-9b48-7ce739960d3e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.0%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (9.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.16%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.25.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.1)\n","Collecting psutil>=5.8.0\n","  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.14)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=dd9277aea8022a3aa4d91316c6f98dad95a89a656f600468bffcc1f5b58d7c5b\n","  Stored in directory: /root/.cache/pip/wheels/31/b2/8c/9b4bb72a4384eabd1ffeab2b7ead692c9165e35711f8a9dc72\n","Successfully built torch-geometric\n","Installing collected packages: torch-scatter, psutil, torch-sparse, torch-geometric\n","  Attempting uninstall: psutil\n","    Found existing installation: psutil 5.4.8\n","    Uninstalling psutil-5.4.8:\n","      Successfully uninstalled psutil-5.4.8\n","Successfully installed psutil-5.9.4 torch-geometric-2.2.0 torch-scatter-2.1.0+pt113cu116 torch-sparse-0.6.16+pt113cu116\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["psutil"]}}},"metadata":{}}]},{"cell_type":"code","source":["import multiprocessing as mp\n","import os\n","import pickle\n","import warnings\n","import torch\n","from functools import partial\n","from itertools import combinations\n","import numpy as np\n","from scipy.spatial.distance import cosine\n","from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n","from sklearn.metrics.pairwise import linear_kernel\n","from sklearn.neighbors import BallTree\n","from sklearn.preprocessing import StandardScaler, normalize\n","from tqdm import tqdm\n","\n","from scipy.linalg import orthogonal_procrustes\n","# from torch_geometric.datasets import Planetoid"],"metadata":{"id":"9N8Dd7rpOy1l","executionInfo":{"status":"ok","timestamp":1678724539413,"user_tz":0,"elapsed":4428,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# use google drive for saving and loading information\n","from google.colab import drive\n","import pickle\n","import os\n","\n","drive.mount('/content/drive')\n","file_path = '/content/drive/MyDrive/L45_project/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HqQMyXHQOJvY","executionInfo":{"status":"ok","timestamp":1678724564542,"user_tz":0,"elapsed":20947,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"574ffc29-a067-4975-e4b7-d1e770154d08"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Cora Data Set Stats (Don't run for now)"],"metadata":{"id":"phShuTT_rlrx"}},{"cell_type":"code","source":["cora_dataset = Planetoid(\"/tmp/cora\", name=\"cora\", split=\"full\")\n","cora_data = cora_dataset[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IHWijlUEry4j","executionInfo":{"status":"ok","timestamp":1678437919094,"user_tz":0,"elapsed":16038,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"8ec41893-fb31-4211-a5e4-d7b18a94098e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n","Processing...\n","Done!\n"]}]},{"cell_type":"code","source":["# Create a random list of indices with 10 items from each class for 2nd cossim\n","labels = cora_dataset[0].y.detach().numpy()\n","num_classes = 7\n","num_samples = 10\n","cora_indices = []\n","for i in range(num_classes):\n","  class_i = np.random.choice(np.where(labels == i)[0], size=num_samples, replace=False)\n","  print(class_i)\n","  cora_indices += class_i.tolist()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69r9kl54rz2i","executionInfo":{"status":"ok","timestamp":1678440403831,"user_tz":0,"elapsed":227,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"3ec493fc-3684-4642-cd6a-93dac6510337"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 693 2133 2578 2197 2155 2330 2055 2202 2347   98]\n","[ 719 2170 1784 2377 2094  138   85  342 2584 1346]\n","[  83  389 1486  976 2359  425 2442 1150  439  758]\n","[2668 1860  520  215 1820 1016 1817 1289 1946 1316]\n","[ 284 2416   33 1406 2292 2130  947  317 2645 2680]\n","[1313 2166 2628 1881 1879  935 1842 2374  822 2269]\n","[ 611  106 1975 1593 1476 2468  539  145 2154 1495]\n"]}]},{"cell_type":"markdown","source":["# Metric Calculations"],"metadata":{"id":"kpFfkagZrKpT"}},{"cell_type":"code","source":["# Adapted from https://github.com/SGDE2020/embedding_stability/blob/master/lib/tools/comparison.py\n","\n","# This potentially solves the issue of the multithreading never returning https://github.com/ipython/ipython/issues/12396\n","# mp.set_start_method('fork')\n","\n","class Comparison:\n","\n","    def __init__(self, emb_dir, embeddings, num_nodes, file_prefix):\n","        \"\"\"\n","        emb_dir: str, path where embeddings are saved\n","        embeddings: list, list of strings that specify the embedding files\n","        \"\"\"\n","        self.dir = emb_dir\n","        self.embeddings = embeddings\n","        self.pairs = self._combinations(embeddings)\n","        self.num_vertices = num_nodes\n","\n","        # use file names without numbering to mark result files\n","        self.file_prefix = file_prefix\n","\n","    def _combinations(self, emb_list):\n","        \"\"\" Computes all different comparison pairs of a list of embeddings \"\"\"\n","        return [pair for pair in combinations(emb_list, 2)]\n","\n","    # KEEP\n","    def _analyse_jaccard(self, queries, nodes, k, pair):\n","        \"\"\"This function is called in the multiprocessing of jaccard score\"\"\"\n","        indices_0 = np.asarray(queries[pair[0]])\n","        indices_1 = np.asarray(queries[pair[1]])\n","        nodes = np.asarray(nodes)\n","        jaccard_score = {}\n","        for i in range(len(nodes)):\n","            jaccard_score[nodes[i]] = \\\n","                len(np.intersect1d(indices_0[i, 1:(k + 1)], indices_1[i, 1:(k + 1)], assume_unique=True)) / \\\n","                len(np.union1d(indices_0[i, 1:(k + 1)], indices_1[i, 1:(k + 1)]))\n","        return list(jaccard_score.values())\n","\n","    # KEEP\n","    def _analyse_second_cossim(self, queries, normed_embs, nodes, k, pair):\n","        \"\"\"\n","        This function is called in the multiprocessing of the second order cosine similarity.\n","        \"\"\"\n","\n","        # Convert the indices of nearest neighbors back into numpy\n","        indices_0 = np.asarray(queries[pair[0]])\n","        indices_1 = np.asarray(queries[pair[1]])\n","\n","        # Convert the embeddings and nodes back into numpy\n","        norm_emb_0 = np.asarray(normed_embs[pair[0]])\n","        norm_emb_1 = np.asarray(normed_embs[pair[1]])\n","        nodes = np.asarray(nodes)\n","\n","        # Compute the second order cosine similarity\n","        pair_results = []\n","        for i in range(len(nodes)):\n","            # Build the set of nearest neighbors w.r.t. both embeddings\n","            # Use indices from 1 to k+1, because the first entry will always be the node itself\n","            neighbors_union = np.union1d(indices_0[i, 1:(k + 1)], indices_1[i, 1:(k + 1)])\n","\n","            # Vectors of cosine similarity values of nearest neighbors. There was an error in the original source code, did not use norm_emb_1 for m1\n","            m0 = cos_sim(norm_emb_0[neighbors_union], norm_emb_0[nodes[i]].reshape(1, -1))\n","            m1 = cos_sim(norm_emb_1[neighbors_union], norm_emb_1[nodes[i]].reshape(1, -1))\n","\n","            assert m0.shape[1] == 1 and m1.shape[1] == 1, \"m0 and m1 should only have a single variable in the second dimension\"\n","            m0 = m0.flatten()\n","            m1 = m1.flatten()\n","\n","            # Cosine similarity between similarity vectors\n","            pair_results.append(float(np.dot(m0, m1) / (np.linalg.norm(m0) * np.linalg.norm(m1))))\n","        return pair_results\n","\n","    # KEEP\n","    def k_nearest_neighbors(self, nodes=None, append=False, samples=100, k=10, jaccard=False, load=False,\n","                            kload_size=100, save=True, save_path=None, num_processes=4):\n","        \"\"\"\n","        Computes the k nearest neighbors to some specified nodes with respect to cosine similarity. As an intermediate\n","        step, it computes the 100 nearest neighbors and saves them to file. Based on these neighbors, the k-nn overlaps\n","        are computed.\n","        Args:\n","            nodes: list, that specifies the nodes\n","            append: bool, if nodes are specified, whether additional nodes will be sampled and added to nodes\n","            samples: int, number of nodes that will be sampled\n","            k: int, number of neighbors that will be used in the comparison\n","            jaccard: bool, whether jaccard score or overlap will be used as similarity measure\n","            load: bool, whether a file of computed neighbors will be used\n","            kload_size: Size of k in knn file to load\n","            save: bool, whether the results should be saved in a text file\n","            save_path: str, path where the file will be saved\n","            num_processes: number of random processes in parallelization\n","        Returns:\n","            dict, \"nodes\": array of nodes, \"overlaps\": array of overlaps, columns are values per node; or array of\n","            jaccard scores\n","        \"\"\"\n","        # Handle the nodes input: Whether to sample nodes, use given nodes, or both.\n","        nodes = self._get_nodes(nodes, samples, append)\n","\n","        # Load k nearest neighbors to save time? Else we will have to compute them first.\n","        if load:\n","            file_name = self.file_prefix + \"_\" + str(kload_size) + \"nns.pickle\"\n","            with open(os.path.join(save_path, file_name), \"rb\") as pickle_file:\n","                queries = pickle.load(pickle_file)\n","            assert list(queries.keys()) == self.embeddings, (\"Keys of loaded queries do not match\"\n","                                                             \" with available embeddings\")\n","\n","        # Normalizing the embeddings to be able to use distance as a proxy for cosine similarity\n","        # BallTree from sklearn is used to compute the neighbors efficiently\n","        else:\n","            queries = self.nearest_neighbors_queries(nodes, k, save_path)\n","        # Store the naive overlaps for all pairs\n","\n","        print(f\"\\n\\n {queries.keys()} \\n\")\n","\n","        # Use multiprocessing to speed up overlap computation.\n","        # Too much data is passed to the processes which makes it inefficient.\n","        # Possibly, it is faster to store the data as a file as an intermediate step.\n","        # only run if less than 100 neighbors are queried, as too large neighborhoods may cause memory issues when\n","        # distributing tasks\n","        if num_processes > 1 and k <= 100:\n","            with mp.Pool(num_processes) as p:\n","                # arguments passed in multiprocessing must be pickable\n","                p_queries = queries\n","                p_nodes = nodes.tolist()\n","                if jaccard:\n","                    multiprocess_func = partial(self._analyse_jaccard, p_queries, p_nodes, k)\n","                else:\n","                    multiprocess_func = partial(self._analyse_knn, p_queries, p_nodes, k)\n","                # li_overlap = p.map(multiprocess_func, self.pairs)\n","                li_overlap = []\n","                for result in tqdm(p.imap(multiprocess_func, self.pairs), total=len(self.pairs)):\n","                  li_overlap.append(result)\n","        else:\n","            if jaccard:\n","                li_overlap = [self._analyse_jaccard(queries, nodes.tolist(), k, pair) for pair in self.pairs]\n","            else:\n","                li_overlap = [self._analyse_knn(queries, nodes.tolist(), k, pair) for pair in self.pairs]\n","\n","        # Convert the result into numpy\n","        overlap = np.asarray(li_overlap)\n","\n","        # Save the results\n","        if jaccard:\n","            nodes_suffix = \"jaccard_nodes\"\n","            scores_suffix = f\"{k}nn_jaccard\"\n","        else:\n","            nodes_suffix = \"overlap_nodes\"\n","            scores_suffix = f\"{k}nn_overlap\"\n","        if save is True:\n","            np.save(os.path.join(save_path, f\"{self.file_prefix}_{nodes_suffix}\"), nodes)\n","            np.save(os.path.join(save_path, f\"{self.file_prefix}_{scores_suffix}\"), overlap)\n","        return {\"nodes\": nodes, \"overlaps\": overlap}\n","\n","    def jaccard_similarity(self, nodes=None, append=False, samples=100, k=10, load=False, kload_size=100, save=True,\n","                           save_path=None, num_processes=4):\n","        \"\"\"\n","        Alias for k_nearest_neighbors with jaccard=True.\n","        See k_nearest_neighbors for detailed documentation.\n","        \"\"\"\n","        return self.k_nearest_neighbors(nodes=nodes, append=append, samples=samples, k=k, jaccard=True, load=load,\n","                                        kload_size=kload_size, save=save, save_path=save_path,\n","                                        num_processes=num_processes)\n","\n","\n","    def second_order_cosine_similarity(self, nodes=None, append=False, num_samples=1000, k=10, load=True, save=False,\n","                                       save_path=None, num_processes=4):\n","        \"\"\" Computes second order cosine similarity.\n","        Args:\n","            nodes: list, that specifies the nodes\n","            append: bool, if nodes are specified, whether additional nodes will be sampled and added to nodes\n","            num_samples: int, number of nodes that will be sampled\n","            k: int, number of neighbors that will be used in the comparison\n","            load: bool, whether to load nearest neighbors from file\n","            save: bool, whether the results should be saved\n","            save_path: str, path where the file will be saved\n","            num_processes: number of random processes in parallelization\n","        Returns:\n","            nodes: numpy array of used nodes\n","            results: numpy array of similarity values of size (number of embedding pairs, number of nodes)\n","        \"\"\"\n","\n","        # Handle the nodes input: Whether to sample nodes, use given nodes, or both.\n","        nodes = self._get_nodes(nodes, num_samples, append)\n","\n","        # Load required data: nearest neighbors, embeddings\n","        normed_embs = {}\n","\n","        if load:\n","            # Load nearest neighbors from file\n","            file_name = self.file_prefix + \"_\" + str(k)+ \"nns.pickle\"\n","            with open(os.path.join(save_path, file_name), \"rb\") as pickle_file:\n","                queries = pickle.load(pickle_file)\n","            assert list(queries.keys()) == self.embeddings, (\"Keys of loaded queries do not match with \"\n","                                                             \"available embeddings\")\n","            for emb in tqdm(self.embeddings, desc=\"Loading nearest neighbor files\"):\n","                normed_embs[emb] = normalize(self.read_embedding(os.path.join(self.dir, emb)), norm='l2', copy=False)\n","        else:\n","            queries, normed_embs = self.nearest_neighbors_queries(nodes, k, save_path, return_embeddings=True)\n","\n","        # Start computation of second order cosine similarity\n","        # arguments passed in multiprocessing must be pickable\n","        p_normed_embs = dict([(key, norm_emb.tolist()) for key, norm_emb in normed_embs.items()])\n","        p_nodes = nodes.tolist()\n","        if num_processes > 1 and k <= 100:\n","            with mp.Pool(num_processes) as p:\n","                li_results = p.map(partial(self._analyse_second_cossim, queries, p_normed_embs, p_nodes, k), self.pairs)\n","              # li_results = []\n","              # partial_func = partial(self._analyse_second_cossim, queries, p_normed_embs, p_nodes, k) \n","              # for result in tqdm(p.imap(partial_func, self.pairs), total=len(self.pairs)):\n","              #   li_results.append(result)\n","        else:\n","          li_results = []\n","          for pair in tqdm(self.pairs, desc=\"Comparing embeddings\"):\n","            li_results.append(self._analyse_second_cossim(queries, p_normed_embs, p_nodes, k, pair))\n","\n","        results = np.asarray(li_results)\n","\n","        if save is True:\n","            np.save(os.path.join(save_path, f\"{self.file_prefix}_{k}nn_2nd_order_cossim\"), results)\n","\n","        return nodes, results\n","\n","    # KEEP \n","    def nearest_neighbors_queries(self, nodes, k, save_path, return_embeddings=False):\n","        \"\"\"Uses a ball tree to compute the nearest neighbors in the embedding space\"\"\"\n","        queries = {}\n","        normed_embs = {}\n","        # Normalizing the embeddings to be able to use distance as a proxy for cosine similarity\n","        # BallTree from sklearn is used to compute the neighbors efficiently\n","        if return_embeddings:\n","            for emb in tqdm(self.embeddings, desc=\"Querying nearest neighbors\"):\n","                normed_embs[emb] = normalize(self.read_embedding(os.path.join(self.dir, emb)), norm='l2', copy=False)\n","                ball_tree = BallTree(normed_embs[emb], leaf_size=40)\n","                queries[emb] = ball_tree.query(normed_embs[emb][nodes, :], k=k + 1, return_distance=False).tolist()\n","        else:\n","            for emb in tqdm(self.embeddings, desc=\"Querying nearest neighbors\"):\n","                normalized_embedding = normalize(self.read_embedding(os.path.join(self.dir, emb)), norm='l2',\n","                                                 copy=False)\n","                ball_tree = BallTree(normalized_embedding, leaf_size=40)\n","                # Query the k+1 nearest neighbors, because a node will always be the closest neighbor to itself\n","                queries[emb] = ball_tree.query(normalized_embedding[nodes, :], k=k + 1, return_distance=False).tolist()\n","\n","        # Save the computed neighbors to be able to skip the computation\n","        self.save_pickle(queries, save_path, self.file_prefix + \"_\" + str(k) + \"nns\")\n","\n","        if return_embeddings:\n","            return queries, normed_embs\n","        else:\n","            return queries\n","\n","    # Don't need this\n","    def sample_nodes(self, k):\n","        \"\"\"\n","        Sample unique nodes of an embedding\n","        Args:\n","            k: int, number of nodes to sample\n","        Returns:\n","            numpy array of node ids of length k if k is smaller than the number of nodes available.\n","            Otherwise, all available nodes are returned.\n","        \"\"\"\n","        vertices = np.arange(self.num_vertices)\n","        np.random.shuffle(vertices)\n","        return vertices[:min(k, self.num_vertices)]\n","\n","\n","    def cossim_analysis(self, nodes=None, save_path=None, center=False):\n","        \"\"\" Computes aligned cosine similarity values. Internally performs orthogonal transformation (Procrustes\n","        problem) between two embeddings and saves transformation matrices as well as vector of resulting errors\n","        \"\"\"\n","\n","        # Warn about deprecated arguments\n","        if nodes is not None:\n","            warnings.warn(\"nodes argument is deprecated. All nodes are used instead.\", DeprecationWarning)\n","\n","        # Set up file naming\n","        if center is True:\n","            # matrix_name = \"linQMatrix\"\n","            results_suffix = \"linproc_cossim\"\n","        else:\n","            # matrix_name = \"QMatrix\"\n","            results_suffix = \"aligned_cossim\"\n","\n","        # Read the embeddings\n","        normed_embs = {}\n","        for emb in tqdm(self.embeddings, desc=\"Reading embeddings\"):\n","            normed_embs[emb] = normalize(\n","                    self.read_embedding(os.path.join(self.dir, emb))[np.arange(self.num_vertices)], norm='l2',\n","                    copy=False)\n","\n","        # Do the analysis\n","        emb_ind = -1\n","        results = []\n","        for pair in tqdm(self.pairs, desc=\"Comparing embeddings\"):\n","\n","            # only update first embedding if it does not change\n","            if emb_ind != pair[0]:\n","                emb_ind = pair[0]\n","\n","            W1 = normed_embs[emb_ind]\n","\n","            # transform W2 into W1 -> load matrix from previous step. CHECK\n","            # emb_number_0 = pair[0].split(\"_\")[-1][:-4]\n","            # emb_number_1 = pair[1].split(\"_\")[-1][:-4]\n","            emb_number_0 = pair[0].split(\"_\")[1]\n","            emb_number_1 = pair[1].split(\"_\")[1]\n","            Q, _ = orthogonal_procrustes(normed_embs[pair[1]], normed_embs[pair[0]], check_finite=False)\n","            W2 = normed_embs[pair[1]].dot(Q)\n","\n","            pair_results = np.array([cosine(W1[i], W2[i]) for i in range(self.num_vertices)])\n","            results.append(pair_results)\n","\n","        results = np.asarray(results)\n","        np.save(os.path.join(save_path, f\"{self.file_prefix}_{results_suffix}\"), results)\n","        return np.arange(self.num_vertices), results\n","\n","    def _get_nodes(self, nodes, num_samples, append):\n","        \"\"\"\n","        Handles getting nodes for the experiments.\n","        Args:\n","            nodes: list, node ids\n","            num_samples: int, how many nodes should be sampled\n","            append: bool, whether to append sampled nodes to specified nodes\n","        Returns:\n","            numpy array of (sampled) node ids\n","        \"\"\"\n","        if nodes is None:\n","            nodes = self.sample_nodes(num_samples)\n","        elif append is True:\n","            # allows specified nodes to be taken twice\n","            nodes.extend(self.sample_nodes(num_samples))\n","        return np.asarray(nodes)\n","\n","    def save_pickle(self, obj, save_path, file_name):\n","        if save_path is None:\n","            save_path = os.getcwd()\n","        if file_name is None:\n","            # generate name of report from embedding input: use name of first embedding file without number information\n","            file_name = self.file_prefix\n","        with open(os.path.join(save_path, f\"{file_name}.pickle\"), \"wb\") as f:\n","            pickle.dump(obj, f)\n","\n","    def get_combinations(self):\n","        return self.pairs\n","\n","    def get_vertex_count(self):\n","        return self.num_vertices\n","\n","    # the original code reorders the nodes by ID, but our nodes have the same ordering so there is no need to reorder\n","    def read_embedding(self, path):\n","      node_embedding = torch.load(path).detach().numpy()\n","      return node_embedding"],"metadata":{"id":"vL3dj_plpMyw","executionInfo":{"status":"ok","timestamp":1678724595310,"user_tz":0,"elapsed":228,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"kd9PN4KBZkYj","executionInfo":{"status":"ok","timestamp":1678724599206,"user_tz":0,"elapsed":214,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["# Code adapted from https://github.com/SGDE2020/embedding_stability/blob/master/similarity_tests/similarity_tests.py\n","TESTS= [\"cossim\", \"knn\", \"jaccard\", \"ranks\", \"rns\", \"angdiv\", \"2ndcos\", \"orthtf\", \"cos\", \"lintf\", \"lincos\"]\n","\n","# Expects the the file names in the embedding directory to be of the form model-name_seed_emb.pt\n","def run_tests(embedding_dir, file_prefix, tests=TESTS, num_nodes=-1, knn_size=20, load_knn=False, kload_size=None,\n","                results_dir=None, num_processes=4, cossim_sec_indices=None):\n","    \"\"\"\n","    Run specified similarity tests.\n","    Params:\n","        embedding_dir: str, path to directory where embeddings are stored\n","        file_prefix: the prefix for files for saving results\n","        tests: list of str, tests that will be conducted\n","        num_nodes: int, number of nodes of a graph that are used in the tests (just the number of nodes in the graph)\n","        knn_size: int, neighborhood size for knn test\n","        load_knn: bool, whether a stored knn matrix should be loaded\n","        nodeinfo_dir = str, path to directory where nodeinfo is stored (tables)\n","        results_dir = str, path to directory where results will be saved to\n","    \"\"\"\n","    # Store results in the embedding directory in a results file if no other location is specified\n","    if results_dir is None:\n","      results_dir = embedding_dir + \"results/\"\n","    # Create results directory if it does not exist\n","    if not os.path.exists(results_dir):\n","        os.mkdir(results_dir) \n","\n","    # Computes a list of all _emb.pt files in a given directory. Assumes the directory being used only contains results from one architecture and data set\n","    fnames = sorted([f for f in os.listdir(embedding_dir) if f.endswith(\"_emb.pt\")])\n","    print(fnames)\n","\n","    if len(fnames) <= 1:\n","        print(f\"Did not find any embeddings for in directory {embedding_dir}\")\n"," \n","    else:\n","        # To be compatible with the original source code, we need a list of nodes indices we will be using in our metric computations\n","        # We use all the nodes\n","        nodes = [i for i in range(num_nodes)]\n","\n","        # Allow us to use specific nodes for the second order cossine similarity as it is very computational expensive\n","        cossim_nodes = nodes if cossim_sec_indices is None else cossim_sec_indices\n","\n","        if load_knn and kload_size is None:\n","            kload_size = knn_size\n","\n","        # Start tests\n","        comp = Comparison(emb_dir=embedding_dir, embeddings=fnames, num_nodes=num_nodes, file_prefix=file_prefix)\n","        if \"cossim\" in tests:\n","            print(\"Aligned cosine similarity\")\n","            comp.cossim_analysis(nodes=nodes, save_path=results_dir)\n","        if \"jaccard\" in tests:\n","            print(\"Executing jaccard score\")\n","            comp.jaccard_similarity(\n","                nodes=nodes, append=False, k=knn_size,\n","                load=load_knn, kload_size=kload_size, save=True, save_path=results_dir, num_processes=num_processes\n","            )\n","        if \"2ndcos\" in tests:\n","            print(\"Executing second order cosine similarity\")\n","            comp.second_order_cosine_similarity(\n","                nodes=cossim_nodes, append=False, k=knn_size,\n","                save=True, save_path=results_dir, num_processes=num_processes, load=load_knn\n","            )\n","\n"]},{"cell_type":"markdown","source":["# Run metric calculations"],"metadata":{"id":"FTsS-KbhxRVY"}},{"cell_type":"code","source":["NUM_NODES_CORA = 2708\n","# CHANGE these to match you directory naming structure\n","model_directory = \"GATV2/\"\n","model_file_prefix = \"GATV2_analysis\""],"metadata":{"id":"p4qF6bbXxBlU","executionInfo":{"status":"ok","timestamp":1678724603573,"user_tz":0,"elapsed":326,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Run the tests for tests=[\"cossim\", \"jaccard\"], multiple processes work for these\n","run_tests(embedding_dir=file_path+model_directory, file_prefix=model_file_prefix, tests=[\"cossim\", \"jaccard\"], num_nodes=NUM_NODES_CORA, num_processes=4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JcbF5Mo5SRIU","outputId":"ca5da890-60c0-4c2f-be0c-954398be657f","executionInfo":{"status":"ok","timestamp":1678726052545,"user_tz":0,"elapsed":1435063,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-b6b6ae2016ca>:271: DeprecationWarning: nodes argument is deprecated. All nodes are used instead.\n","  warnings.warn(\"nodes argument is deprecated. All nodes are used instead.\", DeprecationWarning)\n"]},{"output_type":"stream","name":"stdout","text":["['GATV2_1024258131_emb.pt', 'GATV2_116846604_emb.pt', 'GATV2_1221215631_emb.pt', 'GATV2_1517964140_emb.pt', 'GATV2_170173784_emb.pt', 'GATV2_1818027900_emb.pt', 'GATV2_1860537279_emb.pt', 'GATV2_1863727779_emb.pt', 'GATV2_2036056847_emb.pt', 'GATV2_2105922959_emb.pt', 'GATV2_2135956485_emb.pt', 'GATV2_2342954646_emb.pt', 'GATV2_2455059856_emb.pt', 'GATV2_2739899259_emb.pt', 'GATV2_2794978777_emb.pt', 'GATV2_2952735006_emb.pt', 'GATV2_3300171104_emb.pt', 'GATV2_3303475786_emb.pt', 'GATV2_361232447_emb.pt', 'GATV2_3647665043_emb.pt', 'GATV2_3692371949_emb.pt', 'GATV2_3710910636_emb.pt', 'GATV2_400225693_emb.pt', 'GATV2_4083009686_emb.pt', 'GATV2_4193977854_emb.pt', 'GATV2_516507873_emb.pt', 'GATV2_572297925_emb.pt', 'GATV2_806299656_emb.pt', 'GATV2_880019963_emb.pt', 'GATV2_89475662_emb.pt']\n","Aligned cosine similarity\n"]},{"output_type":"stream","name":"stderr","text":["Reading embeddings: 100%|██████████| 30/30 [00:22<00:00,  1.33it/s]\n","Comparing embeddings: 100%|██████████| 435/435 [13:46<00:00,  1.90s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Executing jaccard score\n"]},{"output_type":"stream","name":"stderr","text":["Querying nearest neighbors: 100%|██████████| 30/30 [07:17<00:00, 14.60s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","\n"," dict_keys(['GATV2_1024258131_emb.pt', 'GATV2_116846604_emb.pt', 'GATV2_1221215631_emb.pt', 'GATV2_1517964140_emb.pt', 'GATV2_170173784_emb.pt', 'GATV2_1818027900_emb.pt', 'GATV2_1860537279_emb.pt', 'GATV2_1863727779_emb.pt', 'GATV2_2036056847_emb.pt', 'GATV2_2105922959_emb.pt', 'GATV2_2135956485_emb.pt', 'GATV2_2342954646_emb.pt', 'GATV2_2455059856_emb.pt', 'GATV2_2739899259_emb.pt', 'GATV2_2794978777_emb.pt', 'GATV2_2952735006_emb.pt', 'GATV2_3300171104_emb.pt', 'GATV2_3303475786_emb.pt', 'GATV2_361232447_emb.pt', 'GATV2_3647665043_emb.pt', 'GATV2_3692371949_emb.pt', 'GATV2_3710910636_emb.pt', 'GATV2_400225693_emb.pt', 'GATV2_4083009686_emb.pt', 'GATV2_4193977854_emb.pt', 'GATV2_516507873_emb.pt', 'GATV2_572297925_emb.pt', 'GATV2_806299656_emb.pt', 'GATV2_880019963_emb.pt', 'GATV2_89475662_emb.pt']) \n","\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 435/435 [02:26<00:00,  2.97it/s]\n"]}]},{"cell_type":"code","source":["# Run the tests for tests=[\"2ndcos\"], multiple processes does not work for this so only run on 1, but we can load the nearest neighbours from the previous metric calculations\n","run_tests(embedding_dir=file_path+model_directory, file_prefix=model_file_prefix, tests=[\"2ndcos\"], num_nodes=NUM_NODES_CORA, num_processes=1, load_knn=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"isXLxy0ywLtq","executionInfo":{"status":"ok","timestamp":1678728607082,"user_tz":0,"elapsed":2500488,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"b70bd6d7-fb34-4afe-cf85-74c30dc485f1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["['GATV2_1024258131_emb.pt', 'GATV2_116846604_emb.pt', 'GATV2_1221215631_emb.pt', 'GATV2_1517964140_emb.pt', 'GATV2_170173784_emb.pt', 'GATV2_1818027900_emb.pt', 'GATV2_1860537279_emb.pt', 'GATV2_1863727779_emb.pt', 'GATV2_2036056847_emb.pt', 'GATV2_2105922959_emb.pt', 'GATV2_2135956485_emb.pt', 'GATV2_2342954646_emb.pt', 'GATV2_2455059856_emb.pt', 'GATV2_2739899259_emb.pt', 'GATV2_2794978777_emb.pt', 'GATV2_2952735006_emb.pt', 'GATV2_3300171104_emb.pt', 'GATV2_3303475786_emb.pt', 'GATV2_361232447_emb.pt', 'GATV2_3647665043_emb.pt', 'GATV2_3692371949_emb.pt', 'GATV2_3710910636_emb.pt', 'GATV2_400225693_emb.pt', 'GATV2_4083009686_emb.pt', 'GATV2_4193977854_emb.pt', 'GATV2_516507873_emb.pt', 'GATV2_572297925_emb.pt', 'GATV2_806299656_emb.pt', 'GATV2_880019963_emb.pt', 'GATV2_89475662_emb.pt']\n","Executing second order cosine similarity\n"]},{"output_type":"stream","name":"stderr","text":["Loading nearest neighbor files: 100%|██████████| 30/30 [00:01<00:00, 20.21it/s]\n","Comparing embeddings: 100%|██████████| 435/435 [41:27<00:00,  5.72s/it]\n"]}]},{"cell_type":"markdown","source":["To do:\n","- Store the nodes being used if we're only using a subset\n","- Subsets for second_order_cosine_similarity for larger dataset"],"metadata":{"id":"pOoPSZmgMrkZ"}}]}