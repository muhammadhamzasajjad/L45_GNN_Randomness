{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU87TNON39IV"
      },
      "source": [
        "# **Preliminaries:** Install and import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpygj8TTZ-ur"
      },
      "outputs": [],
      "source": [
        "#@title [RUN] install\n",
        "!pip install networkx\n",
        "!pip install mycolorpy\n",
        "!pip install colorama\n",
        "!pip install ogb\n",
        "\n",
        "import torch\n",
        "import os\n",
        "!pip install torch-geometric torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZLrrWpkk6xv-"
      },
      "outputs": [],
      "source": [
        "#@title [RUN] Import modules\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import math\n",
        "import itertools\n",
        "import scipy as sp\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_geometric\n",
        "from torch_geometric.datasets import Planetoid, Coauthor\n",
        "from torch_scatter import scatter_mean, scatter_max, scatter_sum\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "from torch.nn import Embedding\n",
        "from torch_geometric.typing import Adj\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "\n",
        "\n",
        "import pdb\n",
        "from datetime import datetime\n",
        "\n",
        "#for nice visualisations\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mycolorpy import colorlist as mcp\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from typing import Mapping, Tuple, Sequence, List\n",
        "import colorama\n",
        "\n",
        "import scipy.linalg\n",
        "from scipy.linalg import block_diag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VLrKgQEuwgtb"
      },
      "outputs": [],
      "source": [
        "####### PLOTS #######\n",
        "\n",
        "def update_stats(training_stats, epoch_stats):\n",
        "    \"\"\" Store metrics along the training\n",
        "    Args:\n",
        "      epoch_stats: dict containg metrics about one epoch\n",
        "      training_stats: dict containing lists of metrics along training\n",
        "    Returns:\n",
        "      updated training_stats\n",
        "    \"\"\"\n",
        "    if training_stats is None:\n",
        "        training_stats = {}\n",
        "        for key in epoch_stats.keys():\n",
        "            training_stats[key] = []\n",
        "    for key,val in epoch_stats.items():\n",
        "        training_stats[key].append(val)\n",
        "    return training_stats\n",
        "\n",
        "def plot_stats(training_stats, figsize=(5, 5), name=\"\"):\n",
        "    \"\"\" Create one plot for each metric stored in training_stats\n",
        "    \"\"\"\n",
        "    stats_names = [key[6:] for key in training_stats.keys() if key.startswith('train_')]\n",
        "    f, ax = plt.subplots(len(stats_names), 1, figsize=figsize)\n",
        "    if len(stats_names)==1:\n",
        "        ax = np.array([ax])\n",
        "    for key, axx in zip(stats_names, ax.reshape(-1,)):\n",
        "        axx.plot(\n",
        "            training_stats['epoch'],\n",
        "            training_stats[f'train_{key}'],\n",
        "            label=f\"Training {key}\")\n",
        "        axx.plot(\n",
        "            training_stats['epoch'],\n",
        "            training_stats[f'val_{key}'],\n",
        "            label=f\"Validation {key}\")\n",
        "        axx.set_xlabel(\"Training epoch\")\n",
        "        axx.set_ylabel(key)\n",
        "        axx.legend()\n",
        "    plt.title(name)\n",
        "\n",
        "\n",
        "def get_color_coded_str(i, color):\n",
        "    return \"\\033[3{}m{}\\033[0m\".format(int(color), int(i))\n",
        "\n",
        "def print_color_numpy(map, list_graphs):\n",
        "    \"\"\" print matrix map in color according to list_graphs\n",
        "    \"\"\"\n",
        "    list_blocks = []\n",
        "    for i,graph in enumerate(list_graphs):\n",
        "        block_i = (i+1)*np.ones((graph.num_nodes,graph.num_nodes))\n",
        "        list_blocks += [block_i]\n",
        "    block_color = block_diag(*list_blocks)\n",
        "    \n",
        "    map_modified = np.vectorize(get_color_coded_str)(map, block_color)\n",
        "    print(\"\\n\".join([\" \".join([\"{}\"]*map.shape[0])]*map.shape[1]).format(*[x for y in map_modified.tolist() for x in y]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82mrcZX0A3QR"
      },
      "source": [
        "# Cora dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBTnJEZWA-Iq"
      },
      "outputs": [],
      "source": [
        "cora_dataset = Planetoid(\"/tmp/cora\", name=\"cora\", split=\"full\")\n",
        "cora_data = cora_dataset[0]\n",
        "cora_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuZwDeBwJPZg"
      },
      "outputs": [],
      "source": [
        "print(\"Training class sizes\")\n",
        "print(torch.bincount(cora_dataset[0].y[cora_dataset[0].train_mask]))\n",
        "print(\"Validation class sizes\")\n",
        "print(torch.bincount(cora_dataset[0].y[cora_dataset[0].val_mask]))\n",
        "print(\"Test class sizes\")\n",
        "print(torch.bincount(cora_dataset[0].y[cora_dataset[0].test_mask]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9AoJuMmKQAO"
      },
      "source": [
        "# OBGN-ARVIX dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jswepg0KKQAP"
      },
      "outputs": [],
      "source": [
        "d_name = \"ogbn-arxiv\"\n",
        "\n",
        "arxiv_dataset = PygNodePropPredDataset(name = d_name)\n",
        "\n",
        "split_idx = arxiv_dataset.get_idx_split()\n",
        "train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "arxiv_data = arxiv_dataset[0]\n",
        "arxiv_data.y = arxiv_data.y.squeeze()\n",
        "arxiv_data.node_year = arxiv_data.node_year.squeeze()\n",
        "arxiv_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training class sizes\")\n",
        "print(torch.bincount(arxiv_data.y[train_idx]))\n",
        "print(\"Validation class sizes\")\n",
        "print(torch.bincount(arxiv_data.y[valid_idx]))\n",
        "print(\"Test class sizes\")\n",
        "print(torch.bincount(arxiv_data.y[test_idx]))"
      ],
      "metadata": {
        "id": "xj0Rb5CQKyLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Coauthor dataset"
      ],
      "metadata": {
        "id": "xY5bb2rDEuqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cs_dataset = Coauthor(\"/tmp/coauthor\", name=\"CS\")\n",
        "cs_data = cs_dataset[0]\n",
        "cs_data"
      ],
      "metadata": {
        "id": "g9ZXzG7SE4oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create manual split, do 60:20:20 across classes\n",
        "num_classes_cs = 15\n",
        "train_mask_cs_indices = []\n",
        "val_mask_cs_indices = []\n",
        "test_mask_cs_indices = []\n",
        "cs_labels = cs_data.y\n",
        "for i in range(num_classes_cs):\n",
        "\n",
        "  class_i = np.where(cs_labels == i)[0]\n",
        "  np.random.seed(0)\n",
        "  np.random.shuffle(class_i)\n",
        "\n",
        "  num_samples = len(class_i)\n",
        "  train_mask_cs_indices += (class_i[:int(num_samples*0.6)]).tolist() \n",
        "  val_mask_cs_indices += (class_i[int(num_samples*0.6):int(num_samples*0.8)]).tolist() \n",
        "  test_mask_cs_indices += (class_i[int(num_samples*0.8):]).tolist() \n",
        "\n",
        "print(len(train_mask_cs_indices), len(val_mask_cs_indices), len(test_mask_cs_indices))\n",
        "# Create the masks for training\n",
        "# Test mask \n",
        "train_mask_cs = torch.full((len(cs_labels),), False)\n",
        "train_mask_cs[train_mask_cs_indices] = True\n",
        "# Val mask\n",
        "val_mask_cs = torch.full((len(cs_labels),), False)\n",
        "val_mask_cs[val_mask_cs_indices] = True\n",
        "# Train mask\n",
        "test_mask_cs = torch.full((len(cs_labels),), False)\n",
        "test_mask_cs[test_mask_cs_indices] = True"
      ],
      "metadata": {
        "id": "KO3BwlveIuNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training class sizes\")\n",
        "print(torch.bincount(cs_data.y[train_mask_cs]))\n",
        "print(\"Validation class sizes\")\n",
        "print(torch.bincount(cs_data.y[val_mask_cs]))\n",
        "print(\"Test class sizes\")\n",
        "print(torch.bincount(cs_data.y[test_mask_cs]))"
      ],
      "metadata": {
        "id": "lO6lZz9SE6kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL8gKs07J9JP"
      },
      "source": [
        "# Data saving / loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z80lU1V-Isky"
      },
      "outputs": [],
      "source": [
        "# use google drive for saving and loading information\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/MyDrive/L45_project/'\n",
        "# create folder if it does not exist already\n",
        "if not os.path.exists(file_path):\n",
        "  os.mkdir(file_path) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "VhUfVQAZTWHu"
      },
      "outputs": [],
      "source": [
        "def save_training_info(training_stats: dict, node_embedding: torch.Tensor, filename: str):\n",
        "  # write training data info to a file\n",
        "  with open(file_path + filename + \".pkl\", 'wb') as fp:\n",
        "    pickle.dump(training_stats, fp)\n",
        "    print('Training stats saved successfully to file: ' + filename)\n",
        "  # write node embedding to a file\n",
        "  torch.save(node_embedding, file_path + filename + \"_emb.pt\")\n",
        "  print('Node embedding saved successfully to file: ' + filename)\n",
        "\n",
        "\n",
        "def load_training_info(filename: str):\n",
        "  # load training stats dictionary \n",
        "  with open(file_path + filename + \".pkl\", 'rb') as fp:\n",
        "    train_stats = pickle.load(fp)\n",
        "    print('Training stats successfully loaded from file: ' + filename)\n",
        "  # load node embedding\n",
        "  node_embedding = torch.load(file_path + filename + \"_emb.pt\")\n",
        "  print('Node embedding successfully loaded from file: ' + filename)\n",
        "  return train_stats, node_embedding\n",
        "\n",
        "# Final results is a list [seed, test result, [test per class accuracy], [training per class accuracy], [val per class accuracy]]\n",
        "def save_final_results(final_results: List, filename: str):\n",
        "  # write training data info to a file\n",
        "  with open(file_path + filename + \".pkl\", 'ab') as fp:\n",
        "    pickle.dump(final_results, fp)\n",
        "    print('Final results saved successfully to file: ' + filename)\n",
        "\n",
        "# Returns an iterator which contains all the results from our various runs\n",
        "def load_final_results(filename: str):\n",
        "  with open(file_path + filename + \".pkl\", 'rb') as fp:\n",
        "    print('Final results found in file: ' + filename)\n",
        "    while True:\n",
        "      try:\n",
        "        # This notation creates a generator, which we can then iterate through\n",
        "        yield pickle.load(fp)\n",
        "      except EOFError:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZECGPy9JTZrT"
      },
      "outputs": [],
      "source": [
        "test_dict = {'c':[1,2,3], 'b':[4,5,6]}\n",
        "test_tensor = torch.tensor([[1., -1.], [1., -1.]])\n",
        "save_training_info(test_dict, test_tensor, \"testing\")\n",
        "recovered_val1, recovered_val2 = load_training_info(\"testing\")\n",
        "print(recovered_val1, recovered_val2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVyPiw_TBMj7"
      },
      "source": [
        "# Model Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "m_yBLcOs6V7v"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCN\n",
        "\n",
        "class GCNModelWrapper(GCN):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n",
        "    # use one less layer as our final graph layer can downsize for us\n",
        "    # super().__init__(in_channels, hidden_channels, num_layers-1)\n",
        "    super().__init__(in_channels, hidden_channels, num_layers)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "M9xNcjhyBRmX"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GAT\n",
        "\n",
        "class GATModelWrapper(GAT):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int, v2: bool):\n",
        "    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n",
        "    super().__init__(in_channels, hidden_channels, num_layers, v2=v2)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "S3upq1VfKQAQ"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GraphSAGE\n",
        "\n",
        "class GraphSAGEModelWrapper(GraphSAGE):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n",
        "    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n",
        "    super().__init__(in_channels, hidden_channels, num_layers)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output, x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import Node2Vec\n",
        "from torch import Tensor\n",
        "\n",
        "class Node2VecWrapper(Node2Vec):\n",
        "  def __init__(self, edge_index, embedding_size, walk_length, context_size, walks_per_node, num_negative_samples, p, q, sparse, out_channels):\n",
        "    super().__init__(edge_index, embedding_dim=embedding_size, walk_length=walk_length,\n",
        "                     context_size=context_size, walks_per_node=walks_per_node,\n",
        "                     num_negative_samples=num_negative_samples, p=p, q=q, sparse=sparse)\n",
        "    self.final_layer = nn.Linear(embedding_size, out_channels)\n",
        "  def forward(self):\n",
        "    x = super().forward()\n",
        "    output = F.softmax(self.final_layer(x), dim=1)\n",
        "    return output, x\n",
        "  def test(\n",
        "    self,\n",
        "    train_z: Tensor,\n",
        "    train_y: Tensor,\n",
        "    test_z: Tensor,\n",
        "    test_y: Tensor,\n",
        "    solver: str = 'lbfgs',\n",
        "    multi_class: str = 'auto',\n",
        "    *args,\n",
        "    **kwargs,\n",
        "    ) -> float:\n",
        "    r\"\"\"Evaluates latent space quality via a logistic regression downstream\n",
        "    task.\"\"\"\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "    clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n",
        "                            **kwargs).fit(train_z.detach().cpu().numpy(),\n",
        "                                          train_y.detach().cpu().numpy())\n",
        "    y_pred = clf.predict(test_z.detach().cpu().numpy())\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "7htrR7C1jc3a"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GIN\n",
        "\n",
        "class GINWrapper(GIN):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n",
        "    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n",
        "    super().__init__(in_channels, hidden_channels, num_layers)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output, x"
      ],
      "metadata": {
        "id": "NIrBZ7ssNBzW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mLwBJNywK-9"
      },
      "source": [
        "# Training code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "-BLISzysQkdA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title [RUN] Hyperparameters GNN\n",
        "\n",
        "NUM_EPOCHS_CORA =  10 #@param {type:\"integer\"}\n",
        "NUM_EPOCHS_ARVIX =  110 #@param {type:\"integer\"}\n",
        "LR         = 0.01 #@param {type:\"number\"}\n",
        "HIDDEN_DIM = 128  #@param {type:\"integer\"}\n",
        "HIDDEN_LAYERS = 10 #@param {type:\"integer\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "AFTSH-Vuv4gk"
      },
      "outputs": [],
      "source": [
        "# Code taken from L45 practical notebook\n",
        "def train_gnn(X, edge_indices, y, mask, model, optimiser, device):\n",
        "    model.train()\n",
        "    # Put data on device\n",
        "    X = X.to(device)\n",
        "    edge_indices = edge_indices.to(device)\n",
        "    y = y.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Train\n",
        "    optimiser.zero_grad()\n",
        "    y_out, _ = model(X, edge_indices)\n",
        "    y_hat = y_out[mask]\n",
        "    loss = F.cross_entropy(y_hat, y)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    return loss.data\n",
        "\n",
        "# Training loop using subgraph batching from paper 'Inductive Representation Learning on Large Graphs' https://arxiv.org/pdf/1706.02216.pdf\n",
        "def train_gnn_subgraph(data_batch, model, optimiser, device):\n",
        "  total_loss = 0\n",
        "  for batch in data_batch:\n",
        "    # Put batch in device\n",
        "    batch = batch.to(device)\n",
        "    # Do training loop\n",
        "    batch_size = batch.batch_size\n",
        "    optimiser.zero_grad()\n",
        "    y_out, _ = model(batch.x, batch.edge_index)\n",
        "    y_out = y_out[:batch_size]\n",
        "    batch_y = batch.y[:batch_size]\n",
        "    batch_y = torch.reshape(batch_y, (-1,))\n",
        "    loss = F.cross_entropy(y_out, batch_y)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    # Keep a running total of the loss\n",
        "    total_loss += float(loss)\n",
        "\n",
        "  # Get the average loss across all the batches\n",
        "  loss = total_loss / len(data_batch)\n",
        "  return loss\n",
        "\n",
        "def evaluate_gnn(X, edge_indices, y, mask, model, num_classes, device):\n",
        "    model.eval()\n",
        "    # Put data on device\n",
        "    X = X.to(device)\n",
        "    edge_indices = edge_indices.to(device)\n",
        "    y = y.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Evaluate\n",
        "    with torch.no_grad():\n",
        "      y_out, node_embeddings = model(X, edge_indices)\n",
        "    y_hat = y_out[mask]\n",
        "    y_hat = y_hat.data.max(1)[1]\n",
        "    num_correct = y_hat.eq(y.data).sum()\n",
        "    num_total = len(y)\n",
        "    accuracy = 100.0 * (num_correct/num_total)\n",
        "\n",
        "    # calculate per class accuracy\n",
        "    values, counts = torch.unique(y_hat[y_hat == y.data], return_counts=True)\n",
        "    per_class_counts = torch.zeros(num_classes)\n",
        "    # make sure per_class_counts is on the correct device\n",
        "    per_class_counts = per_class_counts.to(device)\n",
        "    # allocate the number of counts per class\n",
        "    for i, x in enumerate(values):\n",
        "      per_class_counts[x] = counts[i]\n",
        "    # find total number of data points per class in the split\n",
        "    total_per_class = torch.bincount(y.data)\n",
        "    per_class_accuracy = torch.div(per_class_counts, total_per_class)\n",
        "\n",
        "    return accuracy, per_class_accuracy, node_embeddings\n",
        "    \n",
        "# Training loop\n",
        "def train_eval_loop_gnn(model, edge_indices, train_x, train_y, train_mask, valid_x, valid_y, valid_mask, \n",
        "                             test_x, test_y, test_mask, num_classes, seed, filename, device, Cora, subgraph_batches=None):\n",
        "    optimiser = optim.Adam(model.parameters(), lr=LR)\n",
        "    training_stats = None\n",
        "    # Choose number of epochs\n",
        "    NUM_EPOCHS = NUM_EPOCHS_CORA if Cora else NUM_EPOCHS_ARVIX\n",
        "    # Training loop\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # If subgraph batching is not provided, use the full graph for training. Otherwise use subgraph batch training regime\n",
        "        if subgraph_batches is None:\n",
        "          train_loss = train_gnn(train_x, edge_indices, train_y, train_mask, model, optimiser, device)\n",
        "        else:\n",
        "          train_loss = train_gnn_subgraph(subgraph_batches, model, optimiser, device)\n",
        "        # Calculate accuracy on full graph  \n",
        "        train_acc, train_class_acc, _ = evaluate_gnn(train_x, edge_indices, train_y, train_mask, model, num_classes, device)\n",
        "        valid_acc, valid_class_acc, _ = evaluate_gnn(valid_x, edge_indices, valid_y, valid_mask, model, num_classes, device)\n",
        "        if epoch % 10 == 0 or epoch == (NUM_EPOCHS-1):\n",
        "            print(f\"Epoch {epoch} with train loss: {train_loss:.3f} train accuracy: {train_acc:.3f} validation accuracy: {valid_acc:.3f}\")\n",
        "            print(\"Per class train accuracy: \", train_class_acc)\n",
        "            print(\"Per class val accuracy: \", valid_class_acc)\n",
        "        # store the loss and the accuracy for the final plot\n",
        "        epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n",
        "        training_stats = update_stats(training_stats, epoch_stats)\n",
        "\n",
        "    # Lets look at our final test performance\n",
        "    # Only need to get the node embeddings once, take from the training evaluation call\n",
        "    test_acc, test_class_acc, node_embeddings = evaluate_gnn(test_x, edge_indices, test_y, test_mask, model, num_classes, device)\n",
        "    print(f\"Our final test accuracy for the GNN is: {test_acc:.3f}\")\n",
        "    print(\"Final per class accuracy on test set: \", test_class_acc)\n",
        "\n",
        "    # Save training stats if on final iteration of the run\n",
        "    save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n",
        "    # Save final results\n",
        "    final_results_list = [seed, test_acc, test_class_acc, train_class_acc, valid_class_acc]\n",
        "    save_final_results(final_results_list, filename)\n",
        "    # Save final model weights incase we want to do further inference later\n",
        "    torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n",
        "    return training_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "A_O5m_YIaKY-"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed):\n",
        "  print(\"SETTING SEEDS TO: \", str(seed))\n",
        "  # seed the potential sources of randomness\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDvu1pVpKQAR"
      },
      "outputs": [],
      "source": [
        "# CHANGE: To name of model being tested\n",
        "dataset = \"Coauthor\"\n",
        "# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n",
        "seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963, 1818027900, 2135956485, 3710910636, 1517964140, 4083009686, 2455059856, 400225693, 89475662, 361232447, 3647665043, 1221215631, 2036056847, 1860537279, 516507873, 3692371949, 3300171104, 2794978777, 3303475786, 2952735006, 572297925]\n",
        "\n",
        "if dataset == \"Cora\":\n",
        "  print(\"Using Cora dataset\")\n",
        "  # Get the edge indices and node features for our model. General set up variables for running with all the models\n",
        "  edge_indices = cora_data.edge_index\n",
        "  node_features = cora_data.x\n",
        "  neighbour_dataset = cora_data\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = cora_data.train_mask\n",
        "  train_y = cora_data.y[train_mask]\n",
        "  valid_mask = cora_data.val_mask\n",
        "  valid_y = cora_data.y[valid_mask]\n",
        "  test_mask = cora_data.test_mask\n",
        "  test_y = cora_data.y[test_mask]\n",
        "\n",
        "  num_classes = 7\n",
        "  is_cora=True\n",
        "\n",
        "elif dataset==\"Coauthor\":\n",
        "  print(\"Using Coauthor dataset\")\n",
        "  # Get the edge indices and node features for our model. General set up variables for running with all the models\n",
        "  edge_indices = cs_data.edge_index\n",
        "  node_features = cs_data.x\n",
        "  neighbour_dataset = cs_data\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = train_mask_cs\n",
        "  train_y = cs_data.y[train_mask]\n",
        "  valid_mask = val_mask_cs\n",
        "  valid_y = cs_data.y[valid_mask]\n",
        "  test_mask = test_mask_cs\n",
        "  test_y = cs_data.y[test_mask]\n",
        "\n",
        "  num_classes = 15\n",
        "  is_cora=True\n",
        "\n",
        "# Otherwise we are using arvix dataset\n",
        "else:\n",
        "  print(\"Using Arvix dataset\")\n",
        "  # Get the edge indices and node features for our model\n",
        "  edge_indices = arxiv_data.edge_index\n",
        "  node_features = arxiv_data.x\n",
        "  neighbour_dataset = arxiv_data\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = train_idx\n",
        "  train_y = arxiv_data.y[train_mask]\n",
        "  valid_mask = valid_idx\n",
        "  valid_y = arxiv_data.y[valid_mask]\n",
        "  test_mask = test_idx\n",
        "  test_y = arxiv_data.y[test_mask]\n",
        "\n",
        "  num_classes = 40\n",
        "  is_cora = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgiIp_fKQAR"
      },
      "source": [
        "# Training Loops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use to flush GPU memory if it gets too full\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "jKAFIuc3YlIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad3acb0-028a-4371-89ff-430cf95e2c82"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9279"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl6KVverQy7C"
      },
      "outputs": [],
      "source": [
        "filename = \"GAT_\" + dataset + \"_Layers_\" + str(HIDDEN_LAYERS)\n",
        "# create folder for saving all model info into if it does not exist already\n",
        "if not os.path.exists(file_path+filename+\"/\"):\n",
        "  os.mkdir(file_path+filename+\"/\")\n",
        "\n",
        "# General training loop for all models except GraphSAGE, using the whole graph in training instead of using subgraph batching\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Create the model\n",
        "  model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=HIDDEN_LAYERS, out_channels=num_classes, v2=True)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Run training loop\n",
        "  print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n",
        "                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora)\n",
        "  # Print out graphs if not using GPU\n",
        "  if device == torch.device('cpu'):\n",
        "    plot_stats(train_stats_cora, name=filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVevwMOOKQAS"
      },
      "outputs": [],
      "source": [
        "filename = \"GraphSAGE_\" + dataset + \"_Layers_\" + str(HIDDEN_LAYERS)\n",
        "# create folder for saving all model info into if it does not exist already\n",
        "if not os.path.exists(file_path+filename+\"/\"):\n",
        "  os.mkdir(file_path+filename+\"/\")\n",
        "\n",
        "# Training loop for GraphSAGE which using subgraph batches instead of the entire graph\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Original paper uses neighbourhood sizes  S1 = 25 and S2 = 10 so this is what we use\n",
        "  train_loader = NeighborLoader(neighbour_dataset, num_neighbors = [25, 10], batch_size=1024, input_nodes=train_mask)\n",
        "\n",
        "  # Create the model\n",
        "  model = GraphSAGEModelWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=HIDDEN_LAYERS, out_channels=num_classes)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Run training loop\n",
        "  print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n",
        "                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora, subgraph_batches=train_loader)\n",
        "  # Print out graphs if not using GPU\n",
        "  if device == torch.device('cpu'):\n",
        "    plot_stats(train_stats_cora, name=filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"GIN_\" + dataset + \"_Layers_\" + str(HIDDEN_LAYERS)\n",
        "# create folder for saving all model info into if it does not exist already\n",
        "if not os.path.exists(file_path+filename+\"/\"):\n",
        "  os.mkdir(file_path+filename+\"/\")\n",
        "\n",
        "# Training loop for GraphSAGE which using subgraph batches instead of the entire graph\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "\n",
        "  # Create the model\n",
        "  model = GINWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=HIDDEN_LAYERS, out_channels=num_classes)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Run training loop\n",
        "  print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n",
        "                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora)\n",
        "  # Print out graphs if not using GPU\n",
        "  if device == torch.device('cpu'):\n",
        "    plot_stats(train_stats_cora, name=filename)"
      ],
      "metadata": {
        "id": "VLY-SWcPOjRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4GMM4D5dLoB"
      },
      "source": [
        "# TESTING LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35RkMAgK8EHw"
      },
      "outputs": [],
      "source": [
        "final_results = load_final_results(filename)\n",
        "for r in final_results:\n",
        "  print(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ3GQQld9b97"
      },
      "outputs": [],
      "source": [
        "training_stats_1, embedding = load_training_info(filename+\"_1\")\n",
        "plot_stats(training_stats_1, name=\"Testing\")\n",
        "print(embedding)\n",
        "print(node_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1nR7AjndQJn"
      },
      "outputs": [],
      "source": [
        "# Loading stored model weights\n",
        "model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n",
        "model.load_state_dict(torch.load(file_path+filename+\"/\"+\"GATV2_1_model.pt\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GubPzc9IQyP3"
      },
      "source": [
        "- Plot graph with average training stats\n",
        "- Save node embeddings for each run\n",
        "- Save training stats for each run\n",
        "- Save test accuracy for each run\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5HLu-NNuJ88"
      },
      "source": [
        "# Node2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5LtiwsyJPxpt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "78ab5ff4-58fd-4068-e394-1f48038a002f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SETTING SEEDS TO:  4193977854\n",
            "Epoch: 00, Loss: 6.8225, Acc_train: 0.3552, Acc_val: 0.3081, Acc_test: 0.3118\n",
            "Per class train accuracy:  [0.0495283  0.03610108 0.28536585 0.07782101 0.34569378 0.28821293\n",
            " 0.05405405 0.27978339 0.08817204 0.17142857 0.35681293 0.33716161\n",
            " 0.26587302 0.65900846 0.36761905]\n",
            "Epoch: 01, Loss: 3.2918, Acc_train: 0.6302, Acc_val: 0.5709, Acc_test: 0.5956\n",
            "Per class train accuracy:  [0.21933962 0.23826715 0.59349593 0.40077821 0.63397129 0.62053232\n",
            " 0.23423423 0.64620939 0.30107527 0.31428571 0.76443418 0.66037736\n",
            " 0.68650794 0.80088674 0.7447619 ]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-58902c415366>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mnode_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_class_train_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_class_val_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_class_test_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc_train: {acc_train:.4f}, Acc_val: {acc_val:.4f}, Acc_test: {acc_test:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Per class train accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_class_train_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-58902c415366>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m                       z[valid_mask], data.y[valid_mask])\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     acc_test, per_class_test_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n\u001b[0m\u001b[1;32m    109\u001b[0m                       z[test_mask], data.y[test_mask])\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-58902c415366>\u001b[0m in \u001b[0;36mfind_model_acc\u001b[0;34m(model, train_z, train_y, test_z, test_y, solver, multi_class, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfind_model_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-c25da88b0e9b>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, train_z, train_y, test_z, test_y, solver, multi_class, *args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n\u001b[0m\u001b[1;32m     30\u001b[0m                             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                           train_y.detach().cpu().numpy())\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n\u001b[0m\u001b[1;32m   1292\u001b[0m             path_func(\n\u001b[1;32m   1293\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             ]\n\u001b[0;32m--> 450\u001b[0;31m             opt_res = optimize.minimize(\n\u001b[0m\u001b[1;32m    451\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    694\u001b[0m                                  **options)\n\u001b[1;32m    695\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    697\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    698\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;31m# Make sure the function returns a true scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;34m\"\"\" returns the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_linear_loss.py\u001b[0m in \u001b[0;36mloss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"F\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;31m# grad_pointwise.shape = (n_samples, n_classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_pointwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2_reg_strength\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_pointwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch_geometric.nn import Node2Vec\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "data_name = \"Coauthor\"\n",
        "\n",
        "# Get masks and training labels for each split\n",
        "if data_name == \"Cora\":\n",
        "  num_classes = 7\n",
        "  data = cora_data\n",
        "  # Get the edge indices and node features for our model\n",
        "  edge_indices = data.edge_index\n",
        "  node_features = data.x\n",
        "  # CHANGE: To name of model being tested\n",
        "  filename =  \"Node2Vec_Cora\"\n",
        "  train_mask = data.train_mask\n",
        "  train_y = data.y[train_mask]\n",
        "  valid_mask = data.val_mask\n",
        "  valid_y = data.y[valid_mask]\n",
        "  test_mask = data.test_mask\n",
        "  test_y = data.y[test_mask]\n",
        "elif data_name == \"Coauthor\":\n",
        "  data = cs_data\n",
        "  # Get the edge indices and node features for our model\n",
        "  edge_indices = data.edge_index\n",
        "  node_features = data.x\n",
        "  num_classes = 15\n",
        "  filename =  \"Node2Vec_Coauthor_CS\"\n",
        "  train_mask = train_mask_cs\n",
        "  train_y = data.y[train_mask]\n",
        "  valid_mask = val_mask_cs\n",
        "  valid_y = data.y[valid_mask]\n",
        "  test_mask = test_mask_cs\n",
        "  test_y = data.y[test_mask]\n",
        "elif data_name == \"Arxiv\":\n",
        "  data = arxiv_data\n",
        "  edge_indices = arxiv_data.edge_index\n",
        "  node_features = arxiv_data.x\n",
        "  neighbour_dataset = arxiv_data\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = train_idx\n",
        "  train_y = arxiv_data.y[train_mask]\n",
        "  valid_mask = valid_idx\n",
        "  valid_y = arxiv_data.y[valid_mask]\n",
        "  test_mask = test_idx\n",
        "  test_y = arxiv_data.y[test_mask]\n",
        "\n",
        "  num_classes = 40\n",
        "  is_cora = False\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n",
        "seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963, 1818027900, 2135956485, 3710910636, 1517964140, 4083009686, 2455059856, 400225693, 89475662, 361232447, 3647665043, 1221215631, 2036056847, 1860537279, 516507873, 3692371949, 3300171104, 2794978777, 3303475786, 2952735006, 572297925]\n",
        "\n",
        "# create folder for saving all model info into if it does not exist already\n",
        "if not os.path.exists(file_path+filename+\"/\"):\n",
        "  os.mkdir(file_path+filename+\"/\")\n",
        "\n",
        "filename = filename + \"/\" + filename\n",
        "\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Create the model\n",
        "  #model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n",
        "  model = Node2VecWrapper(data.edge_index.to(device), embedding_size=128, walk_length=20,\n",
        "                     context_size=10, walks_per_node=10,\n",
        "                     num_negative_samples=1, p=1, q=1, sparse=True, out_channels=num_classes).to(device)\n",
        "  loader = model.loader(batch_size=128, shuffle=True,\n",
        "                      num_workers=0)\n",
        "  optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
        "\n",
        "  def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for pos_rw, neg_rw in loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def find_model_acc(model, train_z, train_y, test_z, test_y, solver: str = 'lbfgs', multi_class: str = 'auto', *args, **kwargs):\n",
        "    pred_y = model.test(train_z, train_y, test_z, test_y, solver=solver, multi_class=multi_class, *args, **kwargs)\n",
        "    acc = accuracy_score(test_y.detach().cpu().numpy(), pred_y)\n",
        "    matrix = confusion_matrix(test_y.detach().cpu().numpy(), pred_y)\n",
        "    per_class_acc = matrix.diagonal()/matrix.sum(axis=1)\n",
        "    #print(m)\n",
        "    #report = classification_report(test_y.detach().cpu().numpy(), pred_y)\n",
        "    #print(report)\n",
        "    return acc, per_class_acc\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test():\n",
        "    model.eval()\n",
        "    \n",
        "    pred, z = model()\n",
        "    acc_train, per_class_train_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n",
        "                      z[train_mask], data.y[train_mask])\n",
        "  \n",
        "    acc_val, per_class_val_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n",
        "                      z[valid_mask], data.y[valid_mask])\n",
        "\n",
        "    acc_test, per_class_test_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n",
        "                      z[test_mask], data.y[test_mask])\n",
        "\n",
        "    return z, acc_train, per_class_train_acc, acc_val, per_class_val_acc, acc_test, per_class_test_acc\n",
        "\n",
        "  training_stats = None\n",
        "  for epoch in range(0, 10):\n",
        "    loss = train()\n",
        "    node_embeddings, acc_train, per_class_train_acc, acc_val, per_class_val_acc, acc_test, per_class_test_acc = test()\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc_train: {acc_train:.4f}, Acc_val: {acc_val:.4f}, Acc_test: {acc_test:.4f}')\n",
        "    print(f'Per class train accuracy: ', per_class_train_acc)\n",
        "    epoch_stats = {'train_acc': acc_train, 'val_acc': acc_val, 'test_acc': acc_test, 'epoch':epoch}\n",
        "    training_stats = update_stats(training_stats, epoch_stats)\n",
        "  \n",
        "  # Save training stats if on final iteration of the run\n",
        "  save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n",
        "  # Save final results\n",
        "  final_results_list = [seed, acc_test, per_class_test_acc, per_class_train_acc, per_class_val_acc]\n",
        "  save_final_results(final_results_list, filename)\n",
        "  # Save final model weights incase we want to do further inference later\n",
        "  torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n",
        "\n",
        "  plot_stats(training_stats, name=filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGtX2ycNQa-H"
      },
      "source": [
        "# Similarity tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eesbQaUQfd4"
      },
      "source": [
        "https://github.com/SGDE2020/embedding_stability/blob/master/similarity_tests/similarity_tests.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NGtX2ycNQa-H"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}