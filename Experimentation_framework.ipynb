{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU87TNON39IV"
      },
      "source": [
        "# **Preliminaries:** Install and import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mpygj8TTZ-ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c023bd5e-e0ed-4e0b-a207-82a7c83f3706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mycolorpy\n",
            "  Downloading mycolorpy-1.5.1.tar.gz (2.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mycolorpy) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mycolorpy) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (8.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (4.39.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (1.4.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mycolorpy) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mycolorpy) (1.15.0)\n",
            "Building wheels for collected packages: mycolorpy\n",
            "  Building wheel for mycolorpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mycolorpy: filename=mycolorpy-1.5.1-py3-none-any.whl size=3874 sha256=3ce81b12c519fc9f74d233a681b7c9bf847b264f9a260b5bf56db8bb5345dcf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/56/d6/a163bcbec3bb69f3f7797b1b542870b18d7e31ff5dbc0b87e3\n",
            "Successfully built mycolorpy\n",
            "Installing collected packages: mycolorpy\n",
            "Successfully installed mycolorpy-1.5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.26.15)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.2.2)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.13.1+cu116)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.4.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.15.0)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (63.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=ac77a773c1de341bb819e947662ac1b9318e538223a41012689433e89ab77556\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/bb/0d/2d02ec45f29c48d6192476bfb59c5a0e64b605e7212374dd15\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.5 outdated-0.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.1%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.17%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_cluster-1.6.1%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (5.9.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.1.1)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=980b5847bf6085119d41c861ad02aeb870eac166497b0bb35cf7b0a3d18f3ccc\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/b2/8c/9b4bb72a4384eabd1ffeab2b7ead692c9165e35711f8a9dc72\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-scatter, torch-sparse, torch-cluster, torch-geometric\n",
            "Successfully installed torch-cluster-1.6.1+pt113cu116 torch-geometric-2.2.0 torch-scatter-2.1.1+pt113cu116 torch-sparse-0.6.17+pt113cu116\n"
          ]
        }
      ],
      "source": [
        "#@title [RUN] install\n",
        "!pip install networkx\n",
        "!pip install mycolorpy\n",
        "!pip install colorama\n",
        "!pip install ogb\n",
        "\n",
        "import torch\n",
        "import os\n",
        "!pip install torch-geometric torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZLrrWpkk6xv-"
      },
      "outputs": [],
      "source": [
        "#@title [RUN] Import modules\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import math\n",
        "import itertools\n",
        "import scipy as sp\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_geometric\n",
        "from torch_geometric.datasets import Planetoid, Coauthor\n",
        "from torch_scatter import scatter_mean, scatter_max, scatter_sum\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "from torch.nn import Embedding\n",
        "from torch_geometric.typing import Adj\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "\n",
        "\n",
        "import pdb\n",
        "from datetime import datetime\n",
        "\n",
        "#for nice visualisations\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mycolorpy import colorlist as mcp\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from typing import Mapping, Tuple, Sequence, List\n",
        "import colorama\n",
        "\n",
        "import scipy.linalg\n",
        "from scipy.linalg import block_diag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VLrKgQEuwgtb"
      },
      "outputs": [],
      "source": [
        "####### PLOTS #######\n",
        "\n",
        "def update_stats(training_stats, epoch_stats):\n",
        "    \"\"\" Store metrics along the training\n",
        "    Args:\n",
        "      epoch_stats: dict containg metrics about one epoch\n",
        "      training_stats: dict containing lists of metrics along training\n",
        "    Returns:\n",
        "      updated training_stats\n",
        "    \"\"\"\n",
        "    if training_stats is None:\n",
        "        training_stats = {}\n",
        "        for key in epoch_stats.keys():\n",
        "            training_stats[key] = []\n",
        "    for key,val in epoch_stats.items():\n",
        "        training_stats[key].append(val)\n",
        "    return training_stats\n",
        "\n",
        "def plot_stats(training_stats, figsize=(5, 5), name=\"\"):\n",
        "    \"\"\" Create one plot for each metric stored in training_stats\n",
        "    \"\"\"\n",
        "    stats_names = [key[6:] for key in training_stats.keys() if key.startswith('train_')]\n",
        "    f, ax = plt.subplots(len(stats_names), 1, figsize=figsize)\n",
        "    if len(stats_names)==1:\n",
        "        ax = np.array([ax])\n",
        "    for key, axx in zip(stats_names, ax.reshape(-1,)):\n",
        "        axx.plot(\n",
        "            training_stats['epoch'],\n",
        "            training_stats[f'train_{key}'],\n",
        "            label=f\"Training {key}\")\n",
        "        axx.plot(\n",
        "            training_stats['epoch'],\n",
        "            training_stats[f'val_{key}'],\n",
        "            label=f\"Validation {key}\")\n",
        "        axx.set_xlabel(\"Training epoch\")\n",
        "        axx.set_ylabel(key)\n",
        "        axx.legend()\n",
        "    plt.title(name)\n",
        "\n",
        "\n",
        "def get_color_coded_str(i, color):\n",
        "    return \"\\033[3{}m{}\\033[0m\".format(int(color), int(i))\n",
        "\n",
        "def print_color_numpy(map, list_graphs):\n",
        "    \"\"\" print matrix map in color according to list_graphs\n",
        "    \"\"\"\n",
        "    list_blocks = []\n",
        "    for i,graph in enumerate(list_graphs):\n",
        "        block_i = (i+1)*np.ones((graph.num_nodes,graph.num_nodes))\n",
        "        list_blocks += [block_i]\n",
        "    block_color = block_diag(*list_blocks)\n",
        "    \n",
        "    map_modified = np.vectorize(get_color_coded_str)(map, block_color)\n",
        "    print(\"\\n\".join([\" \".join([\"{}\"]*map.shape[0])]*map.shape[1]).format(*[x for y in map_modified.tolist() for x in y]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82mrcZX0A3QR"
      },
      "source": [
        "# Cora dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bBTnJEZWA-Iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba061729-b927-427f-b5fb-37891bbbabd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "cora_dataset = Planetoid(\"/tmp/cora\", name=\"cora\", split=\"full\")\n",
        "cora_data = cora_dataset[0]\n",
        "cora_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UuZwDeBwJPZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "907c8d8d-28ea-40f5-b221-03c566543198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training class sizes\n",
            "tensor([160,  90, 196, 341, 196, 138,  87])\n",
            "Validation class sizes\n",
            "tensor([ 61,  36,  78, 158,  81,  57,  29])\n",
            "Test class sizes\n",
            "tensor([130,  91, 144, 319, 149, 103,  64])\n"
          ]
        }
      ],
      "source": [
        "print(\"Training class sizes\")\n",
        "print(torch.bincount(cora_dataset[0].y[cora_dataset[0].train_mask]))\n",
        "print(\"Validation class sizes\")\n",
        "print(torch.bincount(cora_dataset[0].y[cora_dataset[0].val_mask]))\n",
        "print(\"Test class sizes\")\n",
        "print(torch.bincount(cora_dataset[0].y[cora_dataset[0].test_mask]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9AoJuMmKQAO"
      },
      "source": [
        "# OBGN-ARVIX dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Jswepg0KKQAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09e0673-f605-4ede-a589-87e719d2ee31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:07<00:00, 10.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 240.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 206.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343], y=[169343])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "d_name = \"ogbn-arxiv\"\n",
        "\n",
        "dataset = PygNodePropPredDataset(name = d_name)\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "arxiv_data = dataset[0]\n",
        "arxiv_data.y = arxiv_data.y.squeeze()\n",
        "arxiv_data.node_year = arxiv_data.node_year.squeeze()\n",
        "arxiv_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training class sizes\")\n",
        "print(torch.bincount(arxiv_data.y[train_idx]))\n",
        "print(\"Validation class sizes\")\n",
        "print(torch.bincount(arxiv_data.y[valid_idx]))\n",
        "print(\"Test class sizes\")\n",
        "print(torch.bincount(arxiv_data.y[test_idx]))"
      ],
      "metadata": {
        "id": "xj0Rb5CQKyLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af1f8400-d9b5-4898-bee3-1f52ac61fc62"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training class sizes\n",
            "tensor([  437,   382,  3604,  1014,  2864,  2933,   703,   380,  4056,  2245,\n",
            "         5182,   391,    21,  1290,   473,   248,  9998,   202,   402,  1873,\n",
            "         1495,   304,  1268,  1539,  6989,   457,  2854,  1661, 16284,   239,\n",
            "         4334,  1350,   270,   926,  5426,    75,  2506,  1615,  1100,  1551])\n",
            "Validation class sizes\n",
            "tensor([  74,  118,  502,  412, 1129,  779,  293,   75,  926,  230, 1232,  120,\n",
            "           3,  440,   53,   68, 6846,  110,  138,  585,  268,   38,  249,  487,\n",
            "        4458,  325,  710, 1074, 2273,   57, 2849,  586,   58,  125, 1027,   16,\n",
            "         391,  273,  193,  209])\n",
            "Test class sizes\n",
            "tensor([   54,   187,   733,   654,  1869,  1246,   622,   134,  1250,   345,\n",
            "         1455,   239,     5,   628,    71,    87, 10477,   203,   209,   419,\n",
            "          313,    51,   386,   808, 10740,   475,  1041,  2066,  2849,   120,\n",
            "         4631,   892,    83,   220,  1414,    36,   627,   481,   214,   269])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Coauthor dataset"
      ],
      "metadata": {
        "id": "xY5bb2rDEuqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cs_dataset = Coauthor(\"/tmp/coauthor\", name=\"CS\")\n",
        "cs_data = cs_dataset[0]\n",
        "cs_data"
      ],
      "metadata": {
        "id": "g9ZXzG7SE4oO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24874f47-c6d9-4b7f-ed87-93b8db972dc7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ms_academic_cs.npz\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[18333, 6805], edge_index=[2, 163788], y=[18333])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create manual split, do 60:20:20 across classes\n",
        "num_classes_cs = 15\n",
        "train_mask_cs_indices = []\n",
        "val_mask_cs_indices = []\n",
        "test_mask_cs_indices = []\n",
        "cs_labels = cs_data.y\n",
        "for i in range(num_classes_cs):\n",
        "\n",
        "  class_i = np.where(cs_labels == i)[0]\n",
        "  np.random.seed(0)\n",
        "  np.random.shuffle(class_i)\n",
        "\n",
        "  num_samples = len(class_i)\n",
        "  train_mask_cs_indices += (class_i[:int(num_samples*0.6)]).tolist() \n",
        "  val_mask_cs_indices += (class_i[int(num_samples*0.6):int(num_samples*0.8)]).tolist() \n",
        "  test_mask_cs_indices += (class_i[int(num_samples*0.8):]).tolist() \n",
        "\n",
        "print(len(train_mask_cs_indices), len(val_mask_cs_indices), len(test_mask_cs_indices))\n",
        "# Create the masks for training\n",
        "# Test mask \n",
        "train_mask_cs = torch.full((len(cs_labels),), False)\n",
        "train_mask_cs[train_mask_cs_indices] = True\n",
        "# Val mask\n",
        "val_mask_cs = torch.full((len(cs_labels),), False)\n",
        "val_mask_cs[val_mask_cs_indices] = True\n",
        "# Train mask\n",
        "test_mask_cs = torch.full((len(cs_labels),), False)\n",
        "test_mask_cs[test_mask_cs_indices] = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO3BwlveIuNv",
        "outputId": "fd07e23b-8b40-4a55-c55f-f3896d589f27"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10993 3668 3672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training class sizes\")\n",
        "print(torch.bincount(cs_data.y[train_mask_cs]))\n",
        "print(\"Validation class sizes\")\n",
        "print(torch.bincount(cs_data.y[val_mask_cs]))\n",
        "print(\"Test class sizes\")\n",
        "print(torch.bincount(cs_data.y[test_mask_cs]))"
      ],
      "metadata": {
        "id": "lO6lZz9SE6kK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdc46b68-74c6-483a-df53-68406389b511"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training class sizes\n",
            "tensor([ 424,  277, 1230,  257,  836, 1315,  222,  554,  465,   70,  866, 1219,\n",
            "         252, 2481,  525])\n",
            "Validation class sizes\n",
            "tensor([142,  92, 410,  86, 279, 439,  74, 185, 155,  24, 289, 407,  84, 827,\n",
            "        175])\n",
            "Test class sizes\n",
            "tensor([142,  93, 410,  86, 279, 439,  75, 185, 155,  24, 289, 407,  84, 828,\n",
            "        176])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL8gKs07J9JP"
      },
      "source": [
        "# Data saving / loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Z80lU1V-Isky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c7d943-b3f5-4fd5-d878-6370a489cf8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# use google drive for saving and loading information\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/MyDrive/L45_project/'\n",
        "# create folder if it does not exist already\n",
        "if not os.path.exists(file_path):\n",
        "  os.mkdir(file_path) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VhUfVQAZTWHu"
      },
      "outputs": [],
      "source": [
        "def save_training_info(training_stats: dict, node_embedding: torch.Tensor, filename: str):\n",
        "  # write training data info to a file\n",
        "  with open(file_path + filename + \".pkl\", 'wb') as fp:\n",
        "    pickle.dump(training_stats, fp)\n",
        "    print('Training stats saved successfully to file: ' + filename)\n",
        "  # write node embedding to a file\n",
        "  torch.save(node_embedding, file_path + filename + \"_emb.pt\")\n",
        "  print('Node embedding saved successfully to file: ' + filename)\n",
        "\n",
        "\n",
        "def load_training_info(filename: str):\n",
        "  # load training stats dictionary \n",
        "  with open(file_path + filename + \".pkl\", 'rb') as fp:\n",
        "    train_stats = pickle.load(fp)\n",
        "    print('Training stats successfully loaded from file: ' + filename)\n",
        "  # load node embedding\n",
        "  node_embedding = torch.load(file_path + filename + \"_emb.pt\")\n",
        "  print('Node embedding successfully loaded from file: ' + filename)\n",
        "  return train_stats, node_embedding\n",
        "\n",
        "# Final results is a list [seed, test result, [test per class accuracy], [training per class accuracy], [val per class accuracy]]\n",
        "def save_final_results(final_results: List, filename: str):\n",
        "  # write training data info to a file\n",
        "  with open(file_path + filename + \".pkl\", 'ab') as fp:\n",
        "    pickle.dump(final_results, fp)\n",
        "    print('Final results saved successfully to file: ' + filename)\n",
        "\n",
        "# Returns an iterator which contains all the results from our various runs\n",
        "def load_final_results(filename: str):\n",
        "  with open(file_path + filename + \".pkl\", 'rb') as fp:\n",
        "    print('Final results found in file: ' + filename)\n",
        "    while True:\n",
        "      try:\n",
        "        # This notation creates a generator, which we can then iterate through\n",
        "        yield pickle.load(fp)\n",
        "      except EOFError:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZECGPy9JTZrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad72ff23-9243-4609-d4e7-6646392b8d11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training stats saved successfully to file: testing\n",
            "Node embedding saved successfully to file: testing\n",
            "Training stats successfully loaded from file: testing\n",
            "Node embedding successfully loaded from file: testing\n",
            "{'c': [1, 2, 3], 'b': [4, 5, 6]} tensor([[ 1., -1.],\n",
            "        [ 1., -1.]])\n"
          ]
        }
      ],
      "source": [
        "test_dict = {'c':[1,2,3], 'b':[4,5,6]}\n",
        "test_tensor = torch.tensor([[1., -1.], [1., -1.]])\n",
        "save_training_info(test_dict, test_tensor, \"testing\")\n",
        "recovered_val1, recovered_val2 = load_training_info(\"testing\")\n",
        "print(recovered_val1, recovered_val2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVyPiw_TBMj7"
      },
      "source": [
        "# Model Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "m_yBLcOs6V7v"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCN\n",
        "\n",
        "class GCNModelWrapper(GCN):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n",
        "    # use one less layer as our final graph layer can downsize for us\n",
        "    # super().__init__(in_channels, hidden_channels, num_layers-1)\n",
        "    super().__init__(in_channels, hidden_channels, num_layers)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "M9xNcjhyBRmX"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GAT\n",
        "\n",
        "class GATModelWrapper(GAT):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int, v2: bool):\n",
        "    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n",
        "    super().__init__(in_channels, hidden_channels, num_layers, v2=v2)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "S3upq1VfKQAQ"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GraphSAGE\n",
        "\n",
        "class GraphSAGEModelWrapper(GraphSAGE):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n",
        "    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n",
        "    super().__init__(in_channels, hidden_channels, num_layers)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output, x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import Node2Vec\n",
        "from torch import Tensor\n",
        "\n",
        "class Node2VecWrapper(Node2Vec):\n",
        "  def __init__(self, edge_index, embedding_size, walk_length, context_size, walks_per_node, num_negative_samples, p, q, sparse, out_channels):\n",
        "    super().__init__(edge_index, embedding_dim=embedding_size, walk_length=walk_length,\n",
        "                     context_size=context_size, walks_per_node=walks_per_node,\n",
        "                     num_negative_samples=num_negative_samples, p=p, q=q, sparse=sparse)\n",
        "    self.final_layer = nn.Linear(embedding_size, out_channels)\n",
        "  def forward(self):\n",
        "    x = super().forward()\n",
        "    output = F.softmax(self.final_layer(x), dim=1)\n",
        "    return output, x\n",
        "  def test(\n",
        "    self,\n",
        "    train_z: Tensor,\n",
        "    train_y: Tensor,\n",
        "    test_z: Tensor,\n",
        "    test_y: Tensor,\n",
        "    solver: str = 'lbfgs',\n",
        "    multi_class: str = 'auto',\n",
        "    *args,\n",
        "    **kwargs,\n",
        "    ) -> float:\n",
        "    r\"\"\"Evaluates latent space quality via a logistic regression downstream\n",
        "    task.\"\"\"\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "    clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n",
        "                            **kwargs).fit(train_z.detach().cpu().numpy(),\n",
        "                                          train_y.detach().cpu().numpy())\n",
        "    y_pred = clf.predict(test_z.detach().cpu().numpy())\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "7htrR7C1jc3a"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GIN\n",
        "\n",
        "class GINWrapper(GIN):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n",
        "    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n",
        "    super().__init__(in_channels, hidden_channels, num_layers)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output, x"
      ],
      "metadata": {
        "id": "NIrBZ7ssNBzW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mLwBJNywK-9"
      },
      "source": [
        "# Training code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-BLISzysQkdA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title [RUN] Hyperparameters GNN\n",
        "\n",
        "NUM_EPOCHS_CORA =  10 #@param {type:\"integer\"}\n",
        "NUM_EPOCHS_ARVIX =  110 #@param {type:\"integer\"}\n",
        "LR         = 0.01 #@param {type:\"number\"}\n",
        "HIDDEN_DIM = 128  #@param {type:\"integer\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AFTSH-Vuv4gk"
      },
      "outputs": [],
      "source": [
        "# Code taken from L45 practical notebook\n",
        "def train_gnn(X, edge_indices, y, mask, model, optimiser, device):\n",
        "    model.train()\n",
        "    # Put data on device\n",
        "    X = X.to(device)\n",
        "    edge_indices = edge_indices.to(device)\n",
        "    y = y.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Train\n",
        "    optimiser.zero_grad()\n",
        "    y_out, _ = model(X, edge_indices)\n",
        "    y_hat = y_out[mask]\n",
        "    loss = F.cross_entropy(y_hat, y)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    return loss.data\n",
        "\n",
        "# Training loop using subgraph batching from paper 'Inductive Representation Learning on Large Graphs' https://arxiv.org/pdf/1706.02216.pdf\n",
        "def train_gnn_subgraph(data_batch, model, optimiser, device):\n",
        "  total_loss = 0\n",
        "  for batch in data_batch:\n",
        "    # Put batch in device\n",
        "    batch = batch.to(device)\n",
        "    # Do training loop\n",
        "    batch_size = batch.batch_size\n",
        "    optimiser.zero_grad()\n",
        "    y_out, _ = model(batch.x, batch.edge_index)\n",
        "    y_out = y_out[:batch_size]\n",
        "    batch_y = batch.y[:batch_size]\n",
        "    batch_y = torch.reshape(batch_y, (-1,))\n",
        "    loss = F.cross_entropy(y_out, batch_y)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    # Keep a running total of the loss\n",
        "    total_loss += float(loss)\n",
        "\n",
        "  # Get the average loss across all the batches\n",
        "  loss = total_loss / len(data_batch)\n",
        "  return loss\n",
        "\n",
        "def evaluate_gnn(X, edge_indices, y, mask, model, num_classes, device):\n",
        "    model.eval()\n",
        "    # Put data on device\n",
        "    X = X.to(device)\n",
        "    edge_indices = edge_indices.to(device)\n",
        "    y = y.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Evaluate\n",
        "    with torch.no_grad():\n",
        "      y_out, node_embeddings = model(X, edge_indices)\n",
        "    y_hat = y_out[mask]\n",
        "    y_hat = y_hat.data.max(1)[1]\n",
        "    num_correct = y_hat.eq(y.data).sum()\n",
        "    num_total = len(y)\n",
        "    accuracy = 100.0 * (num_correct/num_total)\n",
        "\n",
        "    # calculate per class accuracy\n",
        "    values, counts = torch.unique(y_hat[y_hat == y.data], return_counts=True)\n",
        "    per_class_counts = torch.zeros(num_classes)\n",
        "    # make sure per_class_counts is on the correct device\n",
        "    per_class_counts = per_class_counts.to(device)\n",
        "    # allocate the number of counts per class\n",
        "    for i, x in enumerate(values):\n",
        "      per_class_counts[x] = counts[i]\n",
        "    # find total number of data points per class in the split\n",
        "    total_per_class = torch.bincount(y.data)\n",
        "    per_class_accuracy = torch.div(per_class_counts, total_per_class)\n",
        "\n",
        "    return accuracy, per_class_accuracy, node_embeddings\n",
        "    \n",
        "# Training loop\n",
        "def train_eval_loop_gnn(model, edge_indices, train_x, train_y, train_mask, valid_x, valid_y, valid_mask, \n",
        "                             test_x, test_y, test_mask, num_classes, seed, filename, device, Cora, subgraph_batches=None):\n",
        "    optimiser = optim.Adam(model.parameters(), lr=LR)\n",
        "    training_stats = None\n",
        "    # Choose number of epochs\n",
        "    NUM_EPOCHS = NUM_EPOCHS_CORA if Cora else NUM_EPOCHS_ARVIX\n",
        "    # Training loop\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # If subgraph batching is not provided, use the full graph for training. Otherwise use subgraph batch training regime\n",
        "        if subgraph_batches is None:\n",
        "          train_loss = train_gnn(train_x, edge_indices, train_y, train_mask, model, optimiser, device)\n",
        "        else:\n",
        "          train_loss = train_gnn_subgraph(subgraph_batches, model, optimiser, device)\n",
        "        # Calculate accuracy on full graph  \n",
        "        train_acc, train_class_acc, _ = evaluate_gnn(train_x, edge_indices, train_y, train_mask, model, num_classes, device)\n",
        "        valid_acc, valid_class_acc, _ = evaluate_gnn(valid_x, edge_indices, valid_y, valid_mask, model, num_classes, device)\n",
        "        if epoch % 10 == 0 or epoch == (NUM_EPOCHS-1):\n",
        "            print(f\"Epoch {epoch} with train loss: {train_loss:.3f} train accuracy: {train_acc:.3f} validation accuracy: {valid_acc:.3f}\")\n",
        "            print(\"Per class train accuracy: \", train_class_acc)\n",
        "            print(\"Per class val accuracy: \", valid_class_acc)\n",
        "        # store the loss and the accuracy for the final plot\n",
        "        epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n",
        "        training_stats = update_stats(training_stats, epoch_stats)\n",
        "\n",
        "    # Lets look at our final test performance\n",
        "    # Only need to get the node embeddings once, take from the training evaluation call\n",
        "    test_acc, test_class_acc, node_embeddings = evaluate_gnn(test_x, edge_indices, test_y, test_mask, model, num_classes, device)\n",
        "    print(f\"Our final test accuracy for the GNN is: {test_acc:.3f}\")\n",
        "    print(\"Final per class accuracy on test set: \", test_class_acc)\n",
        "\n",
        "    # Save training stats if on final iteration of the run\n",
        "    save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n",
        "    # Save final results\n",
        "    final_results_list = [seed, test_acc, test_class_acc, train_class_acc, valid_class_acc]\n",
        "    save_final_results(final_results_list, filename)\n",
        "    # Save final model weights incase we want to do further inference later\n",
        "    torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n",
        "    return training_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "A_O5m_YIaKY-"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed):\n",
        "  print(\"SETTING SEEDS TO: \", str(seed))\n",
        "  # seed the potential sources of randomness\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDvu1pVpKQAR"
      },
      "outputs": [],
      "source": [
        "# CHANGE: To name of model being tested\n",
        "filename = \"GIN_Coauthor\"\n",
        "dataset = \"Coauthor\"\n",
        "# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n",
        "seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963, 1818027900, 2135956485, 3710910636, 1517964140, 4083009686, 2455059856, 400225693, 89475662, 361232447, 3647665043, 1221215631, 2036056847, 1860537279, 516507873, 3692371949, 3300171104, 2794978777, 3303475786, 2952735006, 572297925]\n",
        "\n",
        "# create folder for saving all model info into if it does not exist already\n",
        "if not os.path.exists(file_path+filename+\"/\"):\n",
        "  os.mkdir(file_path+filename+\"/\")\n",
        "\n",
        "if dataset == \"Cora\":\n",
        "  print(\"Using Cora dataset\")\n",
        "  # Get the edge indices and node features for our model. General set up variables for running with all the models\n",
        "  edge_indices = cora_data.edge_index\n",
        "  node_features = cora_data.x\n",
        "  neighbour_dataset = cora_data\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = cora_data.train_mask\n",
        "  train_y = cora_data.y[train_mask]\n",
        "  valid_mask = cora_data.val_mask\n",
        "  valid_y = cora_data.y[valid_mask]\n",
        "  test_mask = cora_data.test_mask\n",
        "  test_y = cora_data.y[test_mask]\n",
        "\n",
        "  num_classes = 7\n",
        "  is_cora=True\n",
        "\n",
        "elif dataset==\"Coauthor\":\n",
        "  print(\"Using Coauthor dataset\")\n",
        "  # Get the edge indices and node features for our model. General set up variables for running with all the models\n",
        "  edge_indices = cs_data.edge_index\n",
        "  node_features = cs_data.x\n",
        "  neighbour_dataset = cs_data\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = train_mask_cs\n",
        "  train_y = cs_data.y[train_mask]\n",
        "  valid_mask = val_mask_cs\n",
        "  valid_y = cs_data.y[valid_mask]\n",
        "  test_mask = test_mask_cs\n",
        "  test_y = cs_data.y[test_mask]\n",
        "\n",
        "  num_classes = 15\n",
        "  is_cora=True\n",
        "\n",
        "# Otherwise we are using arvix dataset\n",
        "else:\n",
        "  print(\"Using Arvix dataset\")\n",
        "  # Get the edge indices and node features for our model\n",
        "  edge_indices = arxiv_data.edge_index\n",
        "  node_features = arxiv_data.x\n",
        "  neighbour_dataset = arxiv_data\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = train_idx\n",
        "  train_y = arxiv_data.y[train_mask]\n",
        "  valid_mask = valid_idx\n",
        "  valid_y = arxiv_data.y[valid_mask]\n",
        "  test_mask = test_idx\n",
        "  test_y = arxiv_data.y[test_mask]\n",
        "\n",
        "  num_classes = 40\n",
        "  is_cora = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgiIp_fKQAR"
      },
      "source": [
        "# Training Loops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use to flush GPU memory if it gets too full\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "jKAFIuc3YlIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl6KVverQy7C"
      },
      "outputs": [],
      "source": [
        "# General training loop for all models except GraphSAGE, using the whole graph in training instead of using subgraph batching\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Create the model\n",
        "  model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes, v2=True)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Run training loop\n",
        "  print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n",
        "                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora)\n",
        "  # Print out graphs if not using GPU\n",
        "  if device == torch.device('cpu'):\n",
        "    plot_stats(train_stats_cora, name=filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVevwMOOKQAS"
      },
      "outputs": [],
      "source": [
        "# Training loop for GraphSAGE which using subgraph batches instead of the entire graph\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Original paper uses neighbourhood sizes  S1 = 25 and S2 = 10 so this is what we use\n",
        "  train_loader = NeighborLoader(neighbour_dataset, num_neighbors = [25, 10], batch_size=1024, input_nodes=train_mask)\n",
        "\n",
        "  # Create the model\n",
        "  model = GraphSAGEModelWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Run training loop\n",
        "  print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n",
        "                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora, subgraph_batches=train_loader)\n",
        "  # Print out graphs if not using GPU\n",
        "  if device == torch.device('cpu'):\n",
        "    plot_stats(train_stats_cora, name=filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for GraphSAGE which using subgraph batches instead of the entire graph\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Original paper uses neighbourhood sizes  S1 = 25 and S2 = 10 so this is what we use\n",
        "  train_loader = NeighborLoader(neighbour_dataset, num_neighbors = [25, 10], batch_size=1024, input_nodes=train_mask)\n",
        "\n",
        "  # Create the model\n",
        "  model = GINWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Run training loop\n",
        "  print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n",
        "                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora, subgraph_batches=train_loader)\n",
        "  # Print out graphs if not using GPU\n",
        "  if device == torch.device('cpu'):\n",
        "    plot_stats(train_stats_cora, name=filename)"
      ],
      "metadata": {
        "id": "VLY-SWcPOjRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4GMM4D5dLoB"
      },
      "source": [
        "# TESTING LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35RkMAgK8EHw"
      },
      "outputs": [],
      "source": [
        "final_results = load_final_results(filename)\n",
        "for r in final_results:\n",
        "  print(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ3GQQld9b97"
      },
      "outputs": [],
      "source": [
        "training_stats_1, embedding = load_training_info(filename+\"_1\")\n",
        "plot_stats(training_stats_1, name=\"Testing\")\n",
        "print(embedding)\n",
        "print(node_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1nR7AjndQJn"
      },
      "outputs": [],
      "source": [
        "# Loading stored model weights\n",
        "model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n",
        "model.load_state_dict(torch.load(file_path+filename+\"/\"+\"GATV2_1_model.pt\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GubPzc9IQyP3"
      },
      "source": [
        "- Plot graph with average training stats\n",
        "- Save node embeddings for each run\n",
        "- Save training stats for each run\n",
        "- Save test accuracy for each run\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5HLu-NNuJ88"
      },
      "source": [
        "# Node2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LtiwsyJPxpt"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import Node2Vec\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "data_name = \"Arxiv\"\n",
        "\n",
        "# Get masks and training labels for each split\n",
        "if data_name == \"Cora\":\n",
        "  num_classes = 7\n",
        "  data = cora_data\n",
        "  # Get the edge indices and node features for our model\n",
        "  edge_indices = data.edge_index\n",
        "  node_features = data.x\n",
        "  # CHANGE: To name of model being tested\n",
        "  filename =  \"Node2Vec_Cora\"\n",
        "  train_mask = data.train_mask\n",
        "  train_y = data.y[train_mask]\n",
        "  valid_mask = data.val_mask\n",
        "  valid_y = data.y[valid_mask]\n",
        "  test_mask = data.test_mask\n",
        "  test_y = data.y[test_mask]\n",
        "elif data_name == \"Coauthor\":\n",
        "  data = cs_data\n",
        "  # Get the edge indices and node features for our model\n",
        "  edge_indices = data.edge_index\n",
        "  node_features = data.x\n",
        "  num_classes = 15\n",
        "  filename =  \"Node2Vec_Coauthor_CS\"\n",
        "  train_mask = train_mask_cs\n",
        "  train_y = data.y[train_mask]\n",
        "  valid_mask = val_mask_cs\n",
        "  valid_y = data.y[valid_mask]\n",
        "  test_mask = test_mask_cs\n",
        "  test_y = data.y[test_mask]\n",
        "elif data_name == \"Arxiv\":\n",
        "  data = arxiv_data\n",
        "  edge_indices = arxiv_data.edge_index\n",
        "  node_features = arxiv_data.x\n",
        "  neighbour_dataset = arxiv_data\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = train_idx\n",
        "  train_y = arxiv_data.y[train_mask]\n",
        "  valid_mask = valid_idx\n",
        "  valid_y = arxiv_data.y[valid_mask]\n",
        "  test_mask = test_idx\n",
        "  test_y = arxiv_data.y[test_mask]\n",
        "\n",
        "  num_classes = 40\n",
        "  is_cora = False\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n",
        "seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963, 1818027900, 2135956485, 3710910636, 1517964140, 4083009686, 2455059856, 400225693, 89475662, 361232447, 3647665043, 1221215631, 2036056847, 1860537279, 516507873, 3692371949, 3300171104, 2794978777, 3303475786, 2952735006, 572297925]\n",
        "\n",
        "# create folder for saving all model info into if it does not exist already\n",
        "if not os.path.exists(file_path+filename+\"/\"):\n",
        "  os.mkdir(file_path+filename+\"/\")\n",
        "\n",
        "filename = filename + \"/\" + filename\n",
        "\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Create the model\n",
        "  #model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n",
        "  model = Node2VecWrapper(data.edge_index.to(device), embedding_size=128, walk_length=20,\n",
        "                     context_size=10, walks_per_node=10,\n",
        "                     num_negative_samples=1, p=1, q=1, sparse=True, out_channels=num_classes).to(device)\n",
        "  loader = model.loader(batch_size=128, shuffle=True,\n",
        "                      num_workers=0)\n",
        "  optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
        "\n",
        "  def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for pos_rw, neg_rw in loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def find_model_acc(model, train_z, train_y, test_z, test_y, solver: str = 'lbfgs', multi_class: str = 'auto', *args, **kwargs):\n",
        "    pred_y = model.test(train_z, train_y, test_z, test_y, solver=solver, multi_class=multi_class, *args, **kwargs)\n",
        "    acc = accuracy_score(test_y.detach().cpu().numpy(), pred_y)\n",
        "    matrix = confusion_matrix(test_y.detach().cpu().numpy(), pred_y)\n",
        "    per_class_acc = matrix.diagonal()/matrix.sum(axis=1)\n",
        "    #print(m)\n",
        "    #report = classification_report(test_y.detach().cpu().numpy(), pred_y)\n",
        "    #print(report)\n",
        "    return acc, per_class_acc\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test():\n",
        "    model.eval()\n",
        "    \n",
        "    pred, z = model()\n",
        "    acc_train, per_class_train_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n",
        "                      z[train_mask], data.y[train_mask])\n",
        "  \n",
        "    acc_val, per_class_val_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n",
        "                      z[valid_mask], data.y[valid_mask])\n",
        "\n",
        "    acc_test, per_class_test_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n",
        "                      z[test_mask], data.y[test_mask])\n",
        "\n",
        "    return z, acc_train, per_class_train_acc, acc_val, per_class_val_acc, acc_test, per_class_test_acc\n",
        "\n",
        "  training_stats = None\n",
        "  for epoch in range(0, 10):\n",
        "    loss = train()\n",
        "    node_embeddings, acc_train, per_class_train_acc, acc_val, per_class_val_acc, acc_test, per_class_test_acc = test()\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc_train: {acc_train:.4f}, Acc_val: {acc_val:.4f}, Acc_test: {acc_test:.4f}')\n",
        "    print(f'Per class train accuracy: ', per_class_train_acc)\n",
        "    epoch_stats = {'train_acc': acc_train, 'val_acc': acc_val, 'test_acc': acc_test, 'epoch':epoch}\n",
        "    training_stats = update_stats(training_stats, epoch_stats)\n",
        "  \n",
        "  # Save training stats if on final iteration of the run\n",
        "  save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n",
        "  # Save final results\n",
        "  final_results_list = [seed, acc_test, per_class_test_acc, per_class_train_acc, per_class_val_acc]\n",
        "  save_final_results(final_results_list, filename)\n",
        "  # Save final model weights incase we want to do further inference later\n",
        "  torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n",
        "\n",
        "  plot_stats(training_stats, name=filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGtX2ycNQa-H"
      },
      "source": [
        "# Similarity tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eesbQaUQfd4"
      },
      "source": [
        "https://github.com/SGDE2020/embedding_stability/blob/master/similarity_tests/similarity_tests.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NGtX2ycNQa-H"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}