{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU87TNON39IV"
      },
      "source": [
        "# **Preliminaries:** Install and import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpygj8TTZ-ur",
        "outputId": "c023bd5e-e0ed-4e0b-a207-82a7c83f3706"
      },
      "outputs": [],
      "source": [
        "#@title [RUN] install\n",
        "!pip install networkx\n",
        "!pip install mycolorpy\n",
        "!pip install colorama\n",
        "!pip install ogb\n",
        "\n",
        "import torch\n",
        "import os\n",
        "!pip install torch-geometric torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZLrrWpkk6xv-"
      },
      "outputs": [],
      "source": [
        "#@title [RUN] Import modules\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import math\n",
        "import itertools\n",
        "import scipy as sp\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_geometric\n",
        "from torch_geometric.datasets import Planetoid, Coauthor\n",
        "from torch_scatter import scatter_mean, scatter_max, scatter_sum\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "from torch.nn import Embedding\n",
        "from torch_geometric.typing import Adj\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "\n",
        "#For FastRP\n",
        "from scipy.sparse import coo_matrix, csr_matrix, csc_matrix, spdiags\n",
        "from sklearn.preprocessing import normalize, scale, MultiLabelBinarizer\n",
        "from sklearn import random_projection\n",
        "\n",
        "\n",
        "import pdb\n",
        "from datetime import datetime\n",
        "\n",
        "#for nice visualisations\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mycolorpy import colorlist as mcp\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from typing import Mapping, Tuple, Sequence, List\n",
        "import colorama\n",
        "\n",
        "import scipy.linalg\n",
        "from scipy.linalg import block_diag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VLrKgQEuwgtb"
      },
      "outputs": [],
      "source": [
        "####### PLOTS #######\n",
        "\n",
        "def update_stats(training_stats, epoch_stats):\n",
        "    \"\"\" Store metrics along the training\n",
        "    Args:\n",
        "      epoch_stats: dict containg metrics about one epoch\n",
        "      training_stats: dict containing lists of metrics along training\n",
        "    Returns:\n",
        "      updated training_stats\n",
        "    \"\"\"\n",
        "    if training_stats is None:\n",
        "        training_stats = {}\n",
        "        for key in epoch_stats.keys():\n",
        "            training_stats[key] = []\n",
        "    for key,val in epoch_stats.items():\n",
        "        training_stats[key].append(val)\n",
        "    return training_stats\n",
        "\n",
        "def plot_stats(training_stats, figsize=(5, 5), name=\"\"):\n",
        "    \"\"\" Create one plot for each metric stored in training_stats\n",
        "    \"\"\"\n",
        "    stats_names = [key[6:] for key in training_stats.keys() if key.startswith('train_')]\n",
        "    f, ax = plt.subplots(len(stats_names), 1, figsize=figsize)\n",
        "    if len(stats_names)==1:\n",
        "        ax = np.array([ax])\n",
        "    for key, axx in zip(stats_names, ax.reshape(-1,)):\n",
        "        axx.plot(\n",
        "            training_stats['epoch'],\n",
        "            training_stats[f'train_{key}'],\n",
        "            label=f\"Training {key}\")\n",
        "        axx.plot(\n",
        "            training_stats['epoch'],\n",
        "            training_stats[f'val_{key}'],\n",
        "            label=f\"Validation {key}\")\n",
        "        axx.set_xlabel(\"Training epoch\")\n",
        "        axx.set_ylabel(key)\n",
        "        axx.legend()\n",
        "    plt.title(name)\n",
        "\n",
        "\n",
        "def get_color_coded_str(i, color):\n",
        "    return \"\\033[3{}m{}\\033[0m\".format(int(color), int(i))\n",
        "\n",
        "def print_color_numpy(map, list_graphs):\n",
        "    \"\"\" print matrix map in color according to list_graphs\n",
        "    \"\"\"\n",
        "    list_blocks = []\n",
        "    for i,graph in enumerate(list_graphs):\n",
        "        block_i = (i+1)*np.ones((graph.num_nodes,graph.num_nodes))\n",
        "        list_blocks += [block_i]\n",
        "    block_color = block_diag(*list_blocks)\n",
        "    \n",
        "    map_modified = np.vectorize(get_color_coded_str)(map, block_color)\n",
        "    print(\"\\n\".join([\" \".join([\"{}\"]*map.shape[0])]*map.shape[1]).format(*[x for y in map_modified.tolist() for x in y]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82mrcZX0A3QR"
      },
      "source": [
        "# Cora dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBTnJEZWA-Iq",
        "outputId": "ba061729-b927-427f-b5fb-37891bbbabd8"
      },
      "outputs": [],
      "source": [
        "cora_dataset = Planetoid(\"/tmp/cora\", name=\"cora\", split=\"full\")\n",
        "cora_data = cora_dataset[0]\n",
        "cora_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuZwDeBwJPZg",
        "outputId": "907c8d8d-28ea-40f5-b221-03c566543198"
      },
      "outputs": [],
      "source": [
        "print(\"Training class sizes\")\n",
        "print(torch.bincount(cora_dataset[0].y[cora_dataset[0].train_mask]))\n",
        "print(\"Validation class sizes\")\n",
        "print(torch.bincount(cora_dataset[0].y[cora_dataset[0].val_mask]))\n",
        "print(\"Test class sizes\")\n",
        "print(torch.bincount(cora_dataset[0].y[cora_dataset[0].test_mask]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9AoJuMmKQAO"
      },
      "source": [
        "# OBGN-ARVIX dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jswepg0KKQAP",
        "outputId": "d09e0673-f605-4ede-a589-87e719d2ee31"
      },
      "outputs": [],
      "source": [
        "d_name = \"ogbn-arxiv\"\n",
        "\n",
        "dataset = PygNodePropPredDataset(name = d_name)\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "arxiv_data = dataset[0]\n",
        "arxiv_data.y = arxiv_data.y.squeeze()\n",
        "arxiv_data.node_year = arxiv_data.node_year.squeeze()\n",
        "arxiv_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj0Rb5CQKyLB",
        "outputId": "af1f8400-d9b5-4898-bee3-1f52ac61fc62"
      },
      "outputs": [],
      "source": [
        "print(\"Training class sizes\")\n",
        "print(torch.bincount(arxiv_data.y[train_idx]))\n",
        "print(\"Validation class sizes\")\n",
        "print(torch.bincount(arxiv_data.y[valid_idx]))\n",
        "print(\"Test class sizes\")\n",
        "print(torch.bincount(arxiv_data.y[test_idx]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY5bb2rDEuqM"
      },
      "source": [
        "#Coauthor dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9ZXzG7SE4oO",
        "outputId": "24874f47-c6d9-4b7f-ed87-93b8db972dc7"
      },
      "outputs": [],
      "source": [
        "cs_dataset = Coauthor(\"/tmp/coauthor\", name=\"CS\")\n",
        "cs_data = cs_dataset[0]\n",
        "cs_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO3BwlveIuNv",
        "outputId": "fd07e23b-8b40-4a55-c55f-f3896d589f27"
      },
      "outputs": [],
      "source": [
        "# Create manual split, do 60:20:20 across classes\n",
        "num_classes_cs = 15\n",
        "train_mask_cs_indices = []\n",
        "val_mask_cs_indices = []\n",
        "test_mask_cs_indices = []\n",
        "cs_labels = cs_data.y\n",
        "for i in range(num_classes_cs):\n",
        "\n",
        "  class_i = np.where(cs_labels == i)[0]\n",
        "  np.random.seed(0)\n",
        "  np.random.shuffle(class_i)\n",
        "\n",
        "  num_samples = len(class_i)\n",
        "  train_mask_cs_indices += (class_i[:int(num_samples*0.6)]).tolist() \n",
        "  val_mask_cs_indices += (class_i[int(num_samples*0.6):int(num_samples*0.8)]).tolist() \n",
        "  test_mask_cs_indices += (class_i[int(num_samples*0.8):]).tolist() \n",
        "\n",
        "print(len(train_mask_cs_indices), len(val_mask_cs_indices), len(test_mask_cs_indices))\n",
        "# Create the masks for training\n",
        "# Test mask \n",
        "train_mask_cs = torch.full((len(cs_labels),), False)\n",
        "train_mask_cs[train_mask_cs_indices] = True\n",
        "# Val mask\n",
        "val_mask_cs = torch.full((len(cs_labels),), False)\n",
        "val_mask_cs[val_mask_cs_indices] = True\n",
        "# Train mask\n",
        "test_mask_cs = torch.full((len(cs_labels),), False)\n",
        "test_mask_cs[test_mask_cs_indices] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO6lZz9SE6kK",
        "outputId": "fdc46b68-74c6-483a-df53-68406389b511"
      },
      "outputs": [],
      "source": [
        "print(\"Training class sizes\")\n",
        "print(torch.bincount(cs_data.y[train_mask_cs]))\n",
        "print(\"Validation class sizes\")\n",
        "print(torch.bincount(cs_data.y[val_mask_cs]))\n",
        "print(\"Test class sizes\")\n",
        "print(torch.bincount(cs_data.y[test_mask_cs]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL8gKs07J9JP"
      },
      "source": [
        "# Data saving / loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z80lU1V-Isky",
        "outputId": "c9c7d943-b3f5-4fd5-d878-6370a489cf8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# use google drive for saving and loading information\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/MyDrive/L45_project/'\n",
        "# create folder if it does not exist already\n",
        "if not os.path.exists(file_path):\n",
        "  os.mkdir(file_path) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VhUfVQAZTWHu"
      },
      "outputs": [],
      "source": [
        "def save_training_info(training_stats: dict, node_embedding: torch.Tensor, filename: str):\n",
        "  # write training data info to a file\n",
        "  with open(file_path + filename + \".pkl\", 'wb') as fp:\n",
        "    pickle.dump(training_stats, fp)\n",
        "    print('Training stats saved successfully to file: ' + filename)\n",
        "  # write node embedding to a file\n",
        "  torch.save(node_embedding, file_path + filename + \"_emb.pt\")\n",
        "  print('Node embedding saved successfully to file: ' + filename)\n",
        "\n",
        "\n",
        "def load_training_info(filename: str):\n",
        "  # load training stats dictionary \n",
        "  with open(file_path + filename + \".pkl\", 'rb') as fp:\n",
        "    train_stats = pickle.load(fp)\n",
        "    print('Training stats successfully loaded from file: ' + filename)\n",
        "  # load node embedding\n",
        "  node_embedding = torch.load(file_path + filename + \"_emb.pt\")\n",
        "  print('Node embedding successfully loaded from file: ' + filename)\n",
        "  return train_stats, node_embedding\n",
        "\n",
        "# Final results is a list [seed, test result, [test per class accuracy], [training per class accuracy], [val per class accuracy]]\n",
        "def save_final_results(final_results: List, filename: str):\n",
        "  # write training data info to a file\n",
        "  with open(file_path + filename + \".pkl\", 'ab') as fp:\n",
        "    pickle.dump(final_results, fp)\n",
        "    print('Final results saved successfully to file: ' + filename)\n",
        "\n",
        "# Returns an iterator which contains all the results from our various runs\n",
        "def load_final_results(filename: str):\n",
        "  with open(file_path + filename + \".pkl\", 'rb') as fp:\n",
        "    print('Final results found in file: ' + filename)\n",
        "    while True:\n",
        "      try:\n",
        "        # This notation creates a generator, which we can then iterate through\n",
        "        yield pickle.load(fp)\n",
        "      except EOFError:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZECGPy9JTZrT",
        "outputId": "ad72ff23-9243-4609-d4e7-6646392b8d11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training stats saved successfully to file: testing\n",
            "Node embedding saved successfully to file: testing\n",
            "Training stats successfully loaded from file: testing\n",
            "Node embedding successfully loaded from file: testing\n",
            "{'c': [1, 2, 3], 'b': [4, 5, 6]} tensor([[ 1., -1.],\n",
            "        [ 1., -1.]])\n"
          ]
        }
      ],
      "source": [
        "test_dict = {'c':[1,2,3], 'b':[4,5,6]}\n",
        "test_tensor = torch.tensor([[1., -1.], [1., -1.]])\n",
        "save_training_info(test_dict, test_tensor, \"testing\")\n",
        "recovered_val1, recovered_val2 = load_training_info(\"testing\")\n",
        "print(recovered_val1, recovered_val2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVyPiw_TBMj7"
      },
      "source": [
        "# Model Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "m_yBLcOs6V7v"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCN\n",
        "\n",
        "class GCNModelWrapper(GCN):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n",
        "    # use one less layer as our final graph layer can downsize for us\n",
        "    # super().__init__(in_channels, hidden_channels, num_layers-1)\n",
        "    super().__init__(in_channels, hidden_channels, num_layers)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "M9xNcjhyBRmX"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GAT\n",
        "\n",
        "class GATModelWrapper(GAT):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int, v2: bool):\n",
        "    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n",
        "    super().__init__(in_channels, hidden_channels, num_layers, v2=v2)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "S3upq1VfKQAQ"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GraphSAGE\n",
        "\n",
        "class GraphSAGEModelWrapper(GraphSAGE):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n",
        "    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n",
        "    super().__init__(in_channels, hidden_channels, num_layers)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7htrR7C1jc3a"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import Node2Vec\n",
        "from torch import Tensor\n",
        "\n",
        "class Node2VecWrapper(Node2Vec):\n",
        "  def __init__(self, edge_index, embedding_size, walk_length, context_size, walks_per_node, num_negative_samples, p, q, sparse, out_channels):\n",
        "    super().__init__(edge_index, embedding_dim=embedding_size, walk_length=walk_length,\n",
        "                     context_size=context_size, walks_per_node=walks_per_node,\n",
        "                     num_negative_samples=num_negative_samples, p=p, q=q, sparse=sparse)\n",
        "    self.final_layer = nn.Linear(embedding_size, out_channels)\n",
        "  def forward(self):\n",
        "    x = super().forward()\n",
        "    output = F.softmax(self.final_layer(x), dim=1)\n",
        "    return output, x\n",
        "  def test(\n",
        "    self,\n",
        "    train_z: Tensor,\n",
        "    train_y: Tensor,\n",
        "    test_z: Tensor,\n",
        "    test_y: Tensor,\n",
        "    solver: str = 'lbfgs',\n",
        "    multi_class: str = 'auto',\n",
        "    *args,\n",
        "    **kwargs,\n",
        "    ) -> float:\n",
        "    r\"\"\"Evaluates latent space quality via a logistic regression downstream\n",
        "    task.\"\"\"\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "    clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n",
        "                            **kwargs).fit(train_z.detach().cpu().numpy(),\n",
        "                                          train_y.detach().cpu().numpy())\n",
        "    y_pred = clf.predict(test_z.detach().cpu().numpy())\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NIrBZ7ssNBzW"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GIN\n",
        "\n",
        "class GINWrapper(GIN):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n",
        "    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n",
        "    super().__init__(in_channels, hidden_channels, num_layers)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output, x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mLwBJNywK-9"
      },
      "source": [
        "# Training code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellView": "form",
        "id": "-BLISzysQkdA"
      },
      "outputs": [],
      "source": [
        "# @title [RUN] Hyperparameters GNN\n",
        "\n",
        "NUM_EPOCHS_CORA =  10 #@param {type:\"integer\"}\n",
        "NUM_EPOCHS_ARVIX =  110 #@param {type:\"integer\"}\n",
        "LR         = 0.01 #@param {type:\"number\"}\n",
        "HIDDEN_DIM = 128  #@param {type:\"integer\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AFTSH-Vuv4gk"
      },
      "outputs": [],
      "source": [
        "# Code taken from L45 practical notebook\n",
        "def train_gnn(X, edge_indices, y, mask, model, optimiser, device):\n",
        "    model.train()\n",
        "    # Put data on device\n",
        "    X = X.to(device)\n",
        "    edge_indices = edge_indices.to(device)\n",
        "    y = y.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Train\n",
        "    optimiser.zero_grad()\n",
        "    y_out, _ = model(X, edge_indices)\n",
        "    y_hat = y_out[mask]\n",
        "    loss = F.cross_entropy(y_hat, y)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    return loss.data\n",
        "\n",
        "# Training loop using subgraph batching from paper 'Inductive Representation Learning on Large Graphs' https://arxiv.org/pdf/1706.02216.pdf\n",
        "def train_gnn_subgraph(data_batch, model, optimiser, device):\n",
        "  total_loss = 0\n",
        "  for batch in data_batch:\n",
        "    # Put batch in device\n",
        "    batch = batch.to(device)\n",
        "    # Do training loop\n",
        "    batch_size = batch.batch_size\n",
        "    optimiser.zero_grad()\n",
        "    y_out, _ = model(batch.x, batch.edge_index)\n",
        "    y_out = y_out[:batch_size]\n",
        "    batch_y = batch.y[:batch_size]\n",
        "    batch_y = torch.reshape(batch_y, (-1,))\n",
        "    loss = F.cross_entropy(y_out, batch_y)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    # Keep a running total of the loss\n",
        "    total_loss += float(loss)\n",
        "\n",
        "  # Get the average loss across all the batches\n",
        "  loss = total_loss / len(data_batch)\n",
        "  return loss\n",
        "\n",
        "def evaluate_gnn(X, edge_indices, y, mask, model, num_classes, device):\n",
        "    model.eval()\n",
        "    # Put data on device\n",
        "    X = X.to(device)\n",
        "    edge_indices = edge_indices.to(device)\n",
        "    y = y.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Evaluate\n",
        "    with torch.no_grad():\n",
        "      y_out, node_embeddings = model(X, edge_indices)\n",
        "    y_hat = y_out[mask]\n",
        "    y_hat = y_hat.data.max(1)[1]\n",
        "    num_correct = y_hat.eq(y.data).sum()\n",
        "    num_total = len(y)\n",
        "    accuracy = 100.0 * (num_correct/num_total)\n",
        "\n",
        "    # calculate per class accuracy\n",
        "    values, counts = torch.unique(y_hat[y_hat == y.data], return_counts=True)\n",
        "    per_class_counts = torch.zeros(num_classes)\n",
        "    # make sure per_class_counts is on the correct device\n",
        "    per_class_counts = per_class_counts.to(device)\n",
        "    # allocate the number of counts per class\n",
        "    for i, x in enumerate(values):\n",
        "      per_class_counts[x] = counts[i]\n",
        "    # find total number of data points per class in the split\n",
        "    total_per_class = torch.bincount(y.data)\n",
        "    per_class_accuracy = torch.div(per_class_counts, total_per_class)\n",
        "\n",
        "    return accuracy, per_class_accuracy, node_embeddings\n",
        "    \n",
        "# Training loop\n",
        "def train_eval_loop_gnn(model, edge_indices, train_x, train_y, train_mask, valid_x, valid_y, valid_mask, \n",
        "                             test_x, test_y, test_mask, num_classes, seed, filename, device, Cora, subgraph_batches=None):\n",
        "    optimiser = optim.Adam(model.parameters(), lr=LR)\n",
        "    training_stats = None\n",
        "    # Choose number of epochs\n",
        "    NUM_EPOCHS = NUM_EPOCHS_CORA if Cora else NUM_EPOCHS_ARVIX\n",
        "    # Training loop\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # If subgraph batching is not provided, use the full graph for training. Otherwise use subgraph batch training regime\n",
        "        if subgraph_batches is None:\n",
        "          train_loss = train_gnn(train_x, edge_indices, train_y, train_mask, model, optimiser, device)\n",
        "        else:\n",
        "          train_loss = train_gnn_subgraph(subgraph_batches, model, optimiser, device)\n",
        "        # Calculate accuracy on full graph  \n",
        "        train_acc, train_class_acc, _ = evaluate_gnn(train_x, edge_indices, train_y, train_mask, model, num_classes, device)\n",
        "        valid_acc, valid_class_acc, _ = evaluate_gnn(valid_x, edge_indices, valid_y, valid_mask, model, num_classes, device)\n",
        "        if epoch % 10 == 0 or epoch == (NUM_EPOCHS-1):\n",
        "            print(f\"Epoch {epoch} with train loss: {train_loss:.3f} train accuracy: {train_acc:.3f} validation accuracy: {valid_acc:.3f}\")\n",
        "            print(\"Per class train accuracy: \", train_class_acc)\n",
        "            print(\"Per class val accuracy: \", valid_class_acc)\n",
        "        # store the loss and the accuracy for the final plot\n",
        "        epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n",
        "        training_stats = update_stats(training_stats, epoch_stats)\n",
        "\n",
        "    # Lets look at our final test performance\n",
        "    # Only need to get the node embeddings once, take from the training evaluation call\n",
        "    test_acc, test_class_acc, node_embeddings = evaluate_gnn(test_x, edge_indices, test_y, test_mask, model, num_classes, device)\n",
        "    print(f\"Our final test accuracy for the GNN is: {test_acc:.3f}\")\n",
        "    print(\"Final per class accuracy on test set: \", test_class_acc)\n",
        "\n",
        "    # Save training stats if on final iteration of the run\n",
        "    save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n",
        "    # Save final results\n",
        "    final_results_list = [seed, test_acc, test_class_acc, train_class_acc, valid_class_acc]\n",
        "    save_final_results(final_results_list, filename)\n",
        "    # Save final model weights incase we want to do further inference later\n",
        "    torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n",
        "    return training_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "A_O5m_YIaKY-"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed):\n",
        "  print(\"SETTING SEEDS TO: \", str(seed))\n",
        "  # seed the potential sources of randomness\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDvu1pVpKQAR"
      },
      "outputs": [],
      "source": [
        "# CHANGE: To name of model being tested\n",
        "filename = \"GIN_Coauthor\"\n",
        "dataset = \"Coauthor\"\n",
        "# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n",
        "seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963, 1818027900, 2135956485, 3710910636, 1517964140, 4083009686, 2455059856, 400225693, 89475662, 361232447, 3647665043, 1221215631, 2036056847, 1860537279, 516507873, 3692371949, 3300171104, 2794978777, 3303475786, 2952735006, 572297925]\n",
        "\n",
        "# create folder for saving all model info into if it does not exist already\n",
        "if not os.path.exists(file_path+filename+\"/\"):\n",
        "  os.mkdir(file_path+filename+\"/\")\n",
        "\n",
        "if dataset == \"Cora\":\n",
        "  print(\"Using Cora dataset\")\n",
        "  # Get the edge indices and node features for our model. General set up variables for running with all the models\n",
        "  edge_indices = cora_data.edge_index\n",
        "  node_features = cora_data.x\n",
        "  neighbour_dataset = cora_data\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = cora_data.train_mask\n",
        "  train_y = cora_data.y[train_mask]\n",
        "  valid_mask = cora_data.val_mask\n",
        "  valid_y = cora_data.y[valid_mask]\n",
        "  test_mask = cora_data.test_mask\n",
        "  test_y = cora_data.y[test_mask]\n",
        "\n",
        "  num_classes = 7\n",
        "  is_cora=True\n",
        "\n",
        "elif dataset==\"Coauthor\":\n",
        "  print(\"Using Coauthor dataset\")\n",
        "  # Get the edge indices and node features for our model. General set up variables for running with all the models\n",
        "  edge_indices = cs_data.edge_index\n",
        "  node_features = cs_data.x\n",
        "  neighbour_dataset = cs_data\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = train_mask_cs\n",
        "  train_y = cs_data.y[train_mask]\n",
        "  valid_mask = val_mask_cs\n",
        "  valid_y = cs_data.y[valid_mask]\n",
        "  test_mask = test_mask_cs\n",
        "  test_y = cs_data.y[test_mask]\n",
        "\n",
        "  num_classes = 15\n",
        "  is_cora=True\n",
        "\n",
        "# Otherwise we are using arvix dataset\n",
        "else:\n",
        "  print(\"Using Arvix dataset\")\n",
        "  # Get the edge indices and node features for our model\n",
        "  edge_indices = arxiv_data.edge_index\n",
        "  node_features = arxiv_data.x\n",
        "  neighbour_dataset = arxiv_data\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = train_idx\n",
        "  train_y = arxiv_data.y[train_mask]\n",
        "  valid_mask = valid_idx\n",
        "  valid_y = arxiv_data.y[valid_mask]\n",
        "  test_mask = test_idx\n",
        "  test_y = arxiv_data.y[test_mask]\n",
        "\n",
        "  num_classes = 40\n",
        "  is_cora = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgiIp_fKQAR"
      },
      "source": [
        "# Training Loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKAFIuc3YlIf"
      },
      "outputs": [],
      "source": [
        "# Use to flush GPU memory if it gets too full\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl6KVverQy7C"
      },
      "outputs": [],
      "source": [
        "# General training loop for all models except GraphSAGE, using the whole graph in training instead of using subgraph batching\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Create the model\n",
        "  model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes, v2=True)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Run training loop\n",
        "  print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n",
        "                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora)\n",
        "  # Print out graphs if not using GPU\n",
        "  if device == torch.device('cpu'):\n",
        "    plot_stats(train_stats_cora, name=filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVevwMOOKQAS"
      },
      "outputs": [],
      "source": [
        "# Training loop for GraphSAGE which using subgraph batches instead of the entire graph\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Original paper uses neighbourhood sizes  S1 = 25 and S2 = 10 so this is what we use\n",
        "  train_loader = NeighborLoader(neighbour_dataset, num_neighbors = [25, 10], batch_size=1024, input_nodes=train_mask)\n",
        "\n",
        "  # Create the model\n",
        "  model = GraphSAGEModelWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Run training loop\n",
        "  print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n",
        "                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora, subgraph_batches=train_loader)\n",
        "  # Print out graphs if not using GPU\n",
        "  if device == torch.device('cpu'):\n",
        "    plot_stats(train_stats_cora, name=filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLY-SWcPOjRa"
      },
      "outputs": [],
      "source": [
        "# Training loop for GraphSAGE which using subgraph batches instead of the entire graph\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Original paper uses neighbourhood sizes  S1 = 25 and S2 = 10 so this is what we use\n",
        "  train_loader = NeighborLoader(neighbour_dataset, num_neighbors = [25, 10], batch_size=1024, input_nodes=train_mask)\n",
        "\n",
        "  # Create the model\n",
        "  model = GINWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Run training loop\n",
        "  print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n",
        "                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora, subgraph_batches=train_loader)\n",
        "  # Print out graphs if not using GPU\n",
        "  if device == torch.device('cpu'):\n",
        "    plot_stats(train_stats_cora, name=filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4GMM4D5dLoB"
      },
      "source": [
        "# TESTING LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35RkMAgK8EHw"
      },
      "outputs": [],
      "source": [
        "final_results = load_final_results(filename)\n",
        "for r in final_results:\n",
        "  print(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ3GQQld9b97"
      },
      "outputs": [],
      "source": [
        "training_stats_1, embedding = load_training_info(filename+\"_1\")\n",
        "plot_stats(training_stats_1, name=\"Testing\")\n",
        "print(embedding)\n",
        "print(node_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1nR7AjndQJn"
      },
      "outputs": [],
      "source": [
        "# Loading stored model weights\n",
        "model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n",
        "model.load_state_dict(torch.load(file_path+filename+\"/\"+\"GATV2_1_model.pt\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GubPzc9IQyP3"
      },
      "source": [
        "- Plot graph with average training stats\n",
        "- Save node embeddings for each run\n",
        "- Save training stats for each run\n",
        "- Save test accuracy for each run\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FastRP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FastRPEmbeddingWrapper(nn.Module):\n",
        "  def __init__(self, input_dim, num_classes):\n",
        "      super().__init__()\n",
        "      self.linear = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.linear(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copied from https://github.com/GTmac/FastRP/blob/master/fastrp.py\n",
        "# projection method: choose from Gaussian and Sparse\n",
        "# input matrix: choose from adjacency and transition matrix\n",
        "# alpha adjusts the weighting of nodes according to their degree\n",
        "def fastrp_projection(A, q=3, dim=128, projection_method='gaussian', input_matrix='adj', alpha=None):\n",
        "    assert input_matrix == 'adj' or input_matrix == 'trans'\n",
        "    assert projection_method == 'gaussian' or projection_method == 'sparse'\n",
        "    \n",
        "    if input_matrix == 'adj':\n",
        "        M = A\n",
        "    else:\n",
        "        N = A.shape[0]\n",
        "        normalizer = spdiags(np.squeeze(1.0 / csc_matrix.sum(A, axis=1) ), 0, N, N)\n",
        "        M = normalizer @ A\n",
        "    # Gaussian projection matrix\n",
        "    if projection_method == 'gaussian':\n",
        "        transformer = random_projection.GaussianRandomProjection(n_components=dim, random_state=42)\n",
        "    # Sparse projection matrix\n",
        "    else:\n",
        "        transformer = random_projection.SparseRandomProjection(n_components=dim, random_state=42)\n",
        "    Y = transformer.fit(M)\n",
        "    # Random projection for A\n",
        "    if alpha is not None:\n",
        "        Y.components_ = Y.components_ @ spdiags( \\\n",
        "                        np.squeeze(np.power(csc_matrix.sum(A, axis=1), alpha)), 0, N, N)\n",
        "    cur_U = transformer.transform(M)\n",
        "    U_list = [cur_U]\n",
        "    \n",
        "    for i in range(2, q + 1):\n",
        "        cur_U = M @ cur_U\n",
        "        U_list.append(cur_U)\n",
        "    return U_list\n",
        "\n",
        "# When weights is None, concatenate instead of linearly combines the embeddings from different powers of A\n",
        "def fastrp_merge(U_list, weights, normalization=False):\n",
        "    dense_U_list = [_U.todense() for _U in U_list] if type(U_list[0]) == csc_matrix else U_list\n",
        "    _U_list = [normalize(_U, norm='l2', axis=1) for _U in dense_U_list] if normalization else dense_U_list\n",
        "\n",
        "    if weights is None:\n",
        "        return np.concatenate(_U_list, axis=1)\n",
        "    U = np.zeros_like(_U_list[0])\n",
        "    for cur_U, weight in zip(_U_list, weights):\n",
        "        U += cur_U * weight\n",
        "    # U = scale(U.todense())\n",
        "    # U = normalize(U.todense(), norm='l2', axis=1)\n",
        "    return scale(np.asarray(U.todense())) if type(U) == csr_matrix else scale(np.asarray(U))\n",
        "\n",
        "# A is always the adjacency matrix\n",
        "# the choice between adj matrix and trans matrix is decided in the conf\n",
        "def fastrp_wrapper(A, conf):\n",
        "    U_list = fastrp_projection(A,\n",
        "                               q=len(conf['weights']),\n",
        "                               dim=conf['dim'],\n",
        "                               projection_method=conf['projection_method'],\n",
        "                               input_matrix=conf['input_matrix'],\n",
        "                               alpha=conf['alpha'],\n",
        "    )\n",
        "    U = fastrp_merge(U_list, conf['weights'], conf['normalization'])\n",
        "    return U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code adpated from L45 practical notebook\n",
        "def train_embedding_classifier(X, y, mask, model, optimiser, device):\n",
        "    model.train()\n",
        "    # Put data on device\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Train\n",
        "    optimiser.zero_grad()\n",
        "    y_out = model(X)\n",
        "    y_hat = y_out[mask]\n",
        "    loss = F.cross_entropy(y_hat, y)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    return loss.data\n",
        "\n",
        "def evaluate_embedding_classifier(X, y, mask, model, num_classes, device):\n",
        "    model.eval()\n",
        "    # Put data on device\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Evaluate\n",
        "    with torch.no_grad():\n",
        "      y_out = model(X)\n",
        "    y_hat = y_out[mask]\n",
        "    y_hat = y_hat.data.max(1)[1]\n",
        "    num_correct = y_hat.eq(y.data).sum()\n",
        "    num_total = len(y)\n",
        "    accuracy = 100.0 * (num_correct/num_total)\n",
        "\n",
        "    # calculate per class accuracy\n",
        "    values, counts = torch.unique(y_hat[y_hat == y.data], return_counts=True)\n",
        "    per_class_counts = torch.zeros(num_classes)\n",
        "    # make sure per_class_counts is on the correct device\n",
        "    per_class_counts = per_class_counts.to(device)\n",
        "    # allocate the number of counts per class\n",
        "    for i, x in enumerate(values):\n",
        "      per_class_counts[x] = counts[i]\n",
        "    # find total number of data points per class in the split\n",
        "    total_per_class = torch.bincount(y.data)\n",
        "    per_class_accuracy = torch.div(per_class_counts, total_per_class)\n",
        "\n",
        "    return accuracy, per_class_accuracy\n",
        "    \n",
        "# Training loop\n",
        "def train_eval_loop_embedding_classifier(model, embeddings, train_y, train_mask, \n",
        "                                         valid_y, valid_mask, test_y, test_mask, num_classes, seed, filename, device, Cora):\n",
        "    optimiser = optim.Adam(model.parameters(), lr=LR)\n",
        "    training_stats = None\n",
        "    # Choose number of epochs\n",
        "    NUM_EPOCHS = NUM_EPOCHS_CORA if Cora else NUM_EPOCHS_ARVIX\n",
        "    # Training loop\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_loss = train_embedding_classifier(embeddings, train_y, train_mask, model, optimiser, device)\n",
        "        # Calculate accuracy on full graph  \n",
        "        train_acc, train_class_acc = evaluate_embedding_classifier(embeddings, train_y, train_mask, model, num_classes, device)\n",
        "        valid_acc, valid_class_acc = evaluate_embedding_classifier(embeddings, valid_y, valid_mask, model, num_classes, device)\n",
        "        if epoch % 10 == 0 or epoch == (NUM_EPOCHS-1):\n",
        "            print(f\"Epoch {epoch} with train loss: {train_loss:.3f} train accuracy: {train_acc:.3f} validation accuracy: {valid_acc:.3f}\")\n",
        "            print(\"Per class train accuracy: \", train_class_acc)\n",
        "            print(\"Per class val accuracy: \", valid_class_acc)\n",
        "        # store the loss and the accuracy for the final plot\n",
        "        epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n",
        "        training_stats = update_stats(training_stats, epoch_stats)\n",
        "\n",
        "    # Lets look at our final test performance\n",
        "    # Only need to get the node embeddings once, take from the training evaluation call\n",
        "    test_acc, test_class_acc = evaluate_embedding_classifier(embeddings, test_y, test_mask, model, num_classes, device)\n",
        "    print(f\"Our final test accuracy for the GNN is: {test_acc:.3f}\")\n",
        "    print(\"Final per class accuracy on test set: \", test_class_acc)\n",
        "\n",
        "    # Save training stats if on final iteration of the run, the node embeddings are actually passed in for training, where  \n",
        "    node_embeddings = embeddings\n",
        "    save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n",
        "    # Save final results\n",
        "    final_results_list = [seed, test_acc, test_class_acc, train_class_acc, valid_class_acc]\n",
        "    save_final_results(final_results_list, filename)\n",
        "    # Save final model weights incase we want to do further inference later\n",
        "    torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n",
        "    return training_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use parameters from example in https://github.com/GTmac/FastRP/blob/master/fast-random-projection-blogcatalog.ipynb\n",
        "# Except our input matrix in an adjacency matrix and since we are not tuning alpha we just set this to None\n",
        "conf = {\n",
        "        'projection_method': 'sparse',\n",
        "        'input_matrix': 'adj',\n",
        "        'weights': [0.0, 0.0, 1.0, 6.67],\n",
        "        'normalization': True,\n",
        "        'dim': HIDDEN_DIM,\n",
        "        'alpha': None,\n",
        "        'C': 0.1\n",
        "    }\n",
        "\n",
        "num_nodes = node_features.shape[0]\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Convert adjacency matrix to scipy matrix\n",
        "  adj_matrix = to_scipy_sparse_matrix(edge_indices)\n",
        "  embeddings = fastrp_wrapper(adj_matrix, conf)\n",
        "  # convert to tensor \n",
        "  embeddings = torch.from_numpy(embeddings)\n",
        "\n",
        "  # Create the model\n",
        "  model = FastRPEmbeddingWrapper(HIDDEN_DIM, num_classes)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Run training loop\n",
        "  print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  train_stats_cora = train_eval_loop_embedding_classifier(model, embeddings, train_y, train_mask, \n",
        "                                             valid_y, valid_mask, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora)\n",
        "  # Print out graphs if not using GPU\n",
        "  if device == torch.device('cpu'):\n",
        "    plot_stats(train_stats_cora, name=filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5HLu-NNuJ88"
      },
      "source": [
        "# Node2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LtiwsyJPxpt"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import Node2Vec\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "data_name = \"Arxiv\"\n",
        "\n",
        "# Get masks and training labels for each split\n",
        "if data_name == \"Cora\":\n",
        "  num_classes = 7\n",
        "  data = cora_data\n",
        "  # Get the edge indices and node features for our model\n",
        "  edge_indices = data.edge_index\n",
        "  node_features = data.x\n",
        "  # CHANGE: To name of model being tested\n",
        "  filename =  \"Node2Vec_Cora\"\n",
        "  train_mask = data.train_mask\n",
        "  train_y = data.y[train_mask]\n",
        "  valid_mask = data.val_mask\n",
        "  valid_y = data.y[valid_mask]\n",
        "  test_mask = data.test_mask\n",
        "  test_y = data.y[test_mask]\n",
        "elif data_name == \"Coauthor\":\n",
        "  data = cs_data\n",
        "  # Get the edge indices and node features for our model\n",
        "  edge_indices = data.edge_index\n",
        "  node_features = data.x\n",
        "  num_classes = 15\n",
        "  filename =  \"Node2Vec_Coauthor_CS\"\n",
        "  train_mask = train_mask_cs\n",
        "  train_y = data.y[train_mask]\n",
        "  valid_mask = val_mask_cs\n",
        "  valid_y = data.y[valid_mask]\n",
        "  test_mask = test_mask_cs\n",
        "  test_y = data.y[test_mask]\n",
        "elif data_name == \"Arxiv\":\n",
        "  data = arxiv_data\n",
        "  edge_indices = arxiv_data.edge_index\n",
        "  node_features = arxiv_data.x\n",
        "  neighbour_dataset = arxiv_data\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = train_idx\n",
        "  train_y = arxiv_data.y[train_mask]\n",
        "  valid_mask = valid_idx\n",
        "  valid_y = arxiv_data.y[valid_mask]\n",
        "  test_mask = test_idx\n",
        "  test_y = arxiv_data.y[test_mask]\n",
        "\n",
        "  num_classes = 40\n",
        "  is_cora = False\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n",
        "seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963, 1818027900, 2135956485, 3710910636, 1517964140, 4083009686, 2455059856, 400225693, 89475662, 361232447, 3647665043, 1221215631, 2036056847, 1860537279, 516507873, 3692371949, 3300171104, 2794978777, 3303475786, 2952735006, 572297925]\n",
        "\n",
        "# create folder for saving all model info into if it does not exist already\n",
        "if not os.path.exists(file_path+filename+\"/\"):\n",
        "  os.mkdir(file_path+filename+\"/\")\n",
        "\n",
        "filename = filename + \"/\" + filename\n",
        "\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Create the model\n",
        "  #model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n",
        "  model = Node2VecWrapper(data.edge_index.to(device), embedding_size=128, walk_length=20,\n",
        "                     context_size=10, walks_per_node=10,\n",
        "                     num_negative_samples=1, p=1, q=1, sparse=True, out_channels=num_classes).to(device)\n",
        "  loader = model.loader(batch_size=128, shuffle=True,\n",
        "                      num_workers=0)\n",
        "  optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
        "\n",
        "  def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for pos_rw, neg_rw in loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def find_model_acc(model, train_z, train_y, test_z, test_y, solver: str = 'lbfgs', multi_class: str = 'auto', *args, **kwargs):\n",
        "    pred_y = model.test(train_z, train_y, test_z, test_y, solver=solver, multi_class=multi_class, *args, **kwargs)\n",
        "    acc = accuracy_score(test_y.detach().cpu().numpy(), pred_y)\n",
        "    matrix = confusion_matrix(test_y.detach().cpu().numpy(), pred_y)\n",
        "    per_class_acc = matrix.diagonal()/matrix.sum(axis=1)\n",
        "    #print(m)\n",
        "    #report = classification_report(test_y.detach().cpu().numpy(), pred_y)\n",
        "    #print(report)\n",
        "    return acc, per_class_acc\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test():\n",
        "    model.eval()\n",
        "    \n",
        "    pred, z = model()\n",
        "    acc_train, per_class_train_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n",
        "                      z[train_mask], data.y[train_mask])\n",
        "  \n",
        "    acc_val, per_class_val_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n",
        "                      z[valid_mask], data.y[valid_mask])\n",
        "\n",
        "    acc_test, per_class_test_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n",
        "                      z[test_mask], data.y[test_mask])\n",
        "\n",
        "    return z, acc_train, per_class_train_acc, acc_val, per_class_val_acc, acc_test, per_class_test_acc\n",
        "\n",
        "  training_stats = None\n",
        "  for epoch in range(0, 10):\n",
        "    loss = train()\n",
        "    node_embeddings, acc_train, per_class_train_acc, acc_val, per_class_val_acc, acc_test, per_class_test_acc = test()\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc_train: {acc_train:.4f}, Acc_val: {acc_val:.4f}, Acc_test: {acc_test:.4f}')\n",
        "    print(f'Per class train accuracy: ', per_class_train_acc)\n",
        "    epoch_stats = {'train_acc': acc_train, 'val_acc': acc_val, 'test_acc': acc_test, 'epoch':epoch}\n",
        "    training_stats = update_stats(training_stats, epoch_stats)\n",
        "  \n",
        "  # Save training stats if on final iteration of the run\n",
        "  save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n",
        "  # Save final results\n",
        "  final_results_list = [seed, acc_test, per_class_test_acc, per_class_train_acc, per_class_val_acc]\n",
        "  save_final_results(final_results_list, filename)\n",
        "  # Save final model weights incase we want to do further inference later\n",
        "  torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n",
        "\n",
        "  plot_stats(training_stats, name=filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGtX2ycNQa-H"
      },
      "source": [
        "# Similarity tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eesbQaUQfd4"
      },
      "source": [
        "https://github.com/SGDE2020/embedding_stability/blob/master/similarity_tests/similarity_tests.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NGtX2ycNQa-H"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
