{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU87TNON39IV"
      },
      "source": [
        "# **Preliminaries:** Install and import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mpygj8TTZ-ur",
        "outputId": "ad619727-fa70-4313-8a90-0baaf02ca83f"
      },
      "outputs": [],
      "source": [
        "#@title [RUN] install\n",
        "!pip install networkx\n",
        "!pip install mycolorpy\n",
        "!pip install colorama\n",
        "!pip install ogb\n",
        "\n",
        "import torch\n",
        "import os\n",
        "!pip install torch-geometric torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLrrWpkk6xv-"
      },
      "outputs": [],
      "source": [
        "#@title [RUN] Import modules\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import math\n",
        "import itertools\n",
        "import scipy as sp\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_geometric\n",
        "from torch_geometric.datasets import Planetoid, ZINC, GNNBenchmarkDataset\n",
        "from torch_scatter import scatter_mean, scatter_max, scatter_sum\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "from torch.nn import Embedding\n",
        "from torch_geometric.typing import Adj\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "\n",
        "import pdb\n",
        "from datetime import datetime\n",
        "\n",
        "#for nice visualisations\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mycolorpy import colorlist as mcp\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from typing import Mapping, Tuple, Sequence, List\n",
        "import colorama\n",
        "\n",
        "import scipy.linalg\n",
        "from scipy.linalg import block_diag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLrKgQEuwgtb"
      },
      "outputs": [],
      "source": [
        "####### PLOTS #######\n",
        "\n",
        "def update_stats(training_stats, epoch_stats):\n",
        "    \"\"\" Store metrics along the training\n",
        "    Args:\n",
        "      epoch_stats: dict containg metrics about one epoch\n",
        "      training_stats: dict containing lists of metrics along training\n",
        "    Returns:\n",
        "      updated training_stats\n",
        "    \"\"\"\n",
        "    if training_stats is None:\n",
        "        training_stats = {}\n",
        "        for key in epoch_stats.keys():\n",
        "            training_stats[key] = []\n",
        "    for key,val in epoch_stats.items():\n",
        "        training_stats[key].append(val)\n",
        "    return training_stats\n",
        "\n",
        "def plot_stats(training_stats, figsize=(5, 5), name=\"\"):\n",
        "    \"\"\" Create one plot for each metric stored in training_stats\n",
        "    \"\"\"\n",
        "    stats_names = [key[6:] for key in training_stats.keys() if key.startswith('train_')]\n",
        "    f, ax = plt.subplots(len(stats_names), 1, figsize=figsize)\n",
        "    if len(stats_names)==1:\n",
        "        ax = np.array([ax])\n",
        "    for key, axx in zip(stats_names, ax.reshape(-1,)):\n",
        "        axx.plot(\n",
        "            training_stats['epoch'],\n",
        "            training_stats[f'train_{key}'],\n",
        "            label=f\"Training {key}\")\n",
        "        axx.plot(\n",
        "            training_stats['epoch'],\n",
        "            training_stats[f'val_{key}'],\n",
        "            label=f\"Validation {key}\")\n",
        "        axx.set_xlabel(\"Training epoch\")\n",
        "        axx.set_ylabel(key)\n",
        "        axx.legend()\n",
        "    plt.title(name)\n",
        "\n",
        "\n",
        "def get_color_coded_str(i, color):\n",
        "    return \"\\033[3{}m{}\\033[0m\".format(int(color), int(i))\n",
        "\n",
        "def print_color_numpy(map, list_graphs):\n",
        "    \"\"\" print matrix map in color according to list_graphs\n",
        "    \"\"\"\n",
        "    list_blocks = []\n",
        "    for i,graph in enumerate(list_graphs):\n",
        "        block_i = (i+1)*np.ones((graph.num_nodes,graph.num_nodes))\n",
        "        list_blocks += [block_i]\n",
        "    block_color = block_diag(*list_blocks)\n",
        "    \n",
        "    map_modified = np.vectorize(get_color_coded_str)(map, block_color)\n",
        "    print(\"\\n\".join([\" \".join([\"{}\"]*map.shape[0])]*map.shape[1]).format(*[x for y in map_modified.tolist() for x in y]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82mrcZX0A3QR"
      },
      "source": [
        "# Cora dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBTnJEZWA-Iq",
        "outputId": "01f8b079-3a26-4b62-f204-85d56567a9ca"
      },
      "outputs": [],
      "source": [
        "cora_dataset = Planetoid(\"/tmp/cora\", name=\"cora\", split=\"full\")\n",
        "cora_data = cora_dataset[0]\n",
        "cora_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuZwDeBwJPZg",
        "outputId": "c62783c3-bc24-4278-ee46-56cc86b00d81"
      },
      "outputs": [],
      "source": [
        "print(\"Training class sizes\")\n",
        "print(torch.bincount(cora_dataset[0].y[cora_dataset[0].train_mask]))\n",
        "print(\"Validation class sizes\")\n",
        "print(torch.bincount(cora_dataset[0].y[cora_dataset[0].val_mask]))\n",
        "print(\"Test class sizes\")\n",
        "print(torch.bincount(cora_dataset[0].y[cora_dataset[0].test_mask]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OBGN-ARVIX dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d_name = \"ogbn-arxiv\"\n",
        "\n",
        "dataset = PygNodePropPredDataset(name = d_name)\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "arxiv_data = dataset[0]\n",
        "arxiv_data.y = arxiv_data.y.squeeze()\n",
        "arxiv_data.node_year = arxiv_data.node_year.squeeze()\n",
        "arxiv_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL8gKs07J9JP"
      },
      "source": [
        "# Data saving / loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z80lU1V-Isky",
        "outputId": "d406f137-d2f8-485a-ce6e-fa327bcddd18"
      },
      "outputs": [],
      "source": [
        "# use google drive for saving and loading information\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/MyDrive/L45_project/'\n",
        "# create folder if it does not exist already\n",
        "if not os.path.exists(file_path):\n",
        "  os.mkdir(file_path) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhUfVQAZTWHu"
      },
      "outputs": [],
      "source": [
        "def save_training_info(training_stats: dict, node_embedding: torch.Tensor, filename: str):\n",
        "  # write training data info to a file\n",
        "  with open(file_path + filename + \".pkl\", 'wb') as fp:\n",
        "    pickle.dump(training_stats, fp)\n",
        "    print('Training stats saved successfully to file: ' + filename)\n",
        "  # write node embedding to a file\n",
        "  torch.save(node_embedding, file_path + filename + \"_emb.pt\")\n",
        "  print('Node embedding saved successfully to file: ' + filename)\n",
        "\n",
        "\n",
        "def load_training_info(filename: str):\n",
        "  # load training stats dictionary \n",
        "  with open(file_path + filename + \".pkl\", 'rb') as fp:\n",
        "    train_stats = pickle.load(fp)\n",
        "    print('Training stats successfully loaded from file: ' + filename)\n",
        "  # load node embedding\n",
        "  node_embedding = torch.load(file_path + filename + \"_emb.pt\")\n",
        "  print('Node embedding successfully loaded from file: ' + filename)\n",
        "  return train_stats, node_embedding\n",
        "\n",
        "# Final results is a list [seed, test result, [test per class accuracy], [training per class accuracy], [val per class accuracy]]\n",
        "def save_final_results(final_results: List, filename: str):\n",
        "  # write training data info to a file\n",
        "  with open(file_path + filename + \".pkl\", 'ab') as fp:\n",
        "    pickle.dump(final_results, fp)\n",
        "    print('Final results saved successfully to file: ' + filename)\n",
        "\n",
        "# Returns an iterator which contains all the results from our various runs\n",
        "def load_final_results(filename: str):\n",
        "  with open(file_path + filename + \".pkl\", 'rb') as fp:\n",
        "    print('Final results found in file: ' + filename)\n",
        "    while True:\n",
        "      try:\n",
        "        # This notation creates a generator, which we can then iterate through\n",
        "        yield pickle.load(fp)\n",
        "      except EOFError:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZECGPy9JTZrT",
        "outputId": "412f13c4-05cc-402b-d743-b8165d360549"
      },
      "outputs": [],
      "source": [
        "test_dict = {'c':[1,2,3], 'b':[4,5,6]}\n",
        "test_tensor = torch.tensor([[1., -1.], [1., -1.]])\n",
        "save_training_info(test_dict, test_tensor, \"testing\")\n",
        "recovered_val1, recovered_val2 = load_training_info(\"testing\")\n",
        "print(recovered_val1, recovered_val2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVyPiw_TBMj7"
      },
      "source": [
        "# Model Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_yBLcOs6V7v"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCN\n",
        "\n",
        "class GCNModelWrapper(GCN):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n",
        "    # use one less layer as our final graph layer can downsize for us\n",
        "    # super().__init__(in_channels, hidden_channels, num_layers-1)\n",
        "    super().__init__(in_channels, hidden_channels, num_layers)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9xNcjhyBRmX"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GAT\n",
        "\n",
        "class GATModelWrapper(GAT):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int, v2: bool):\n",
        "    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n",
        "    super().__init__(in_channels, hidden_channels, num_layers, v2=v2)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GraphSAGE\n",
        "\n",
        "class GraphSAGEModelWrapper(GraphSAGE):\n",
        "\n",
        "  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n",
        "    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n",
        "    super().__init__(in_channels, hidden_channels, num_layers)\n",
        "    self.out_channels = out_channels\n",
        "    self.final_layer = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, edge_index: Adj):\n",
        "    x = super().forward(x, edge_index)\n",
        "    output = self.final_layer(x)\n",
        "    return output, x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mLwBJNywK-9"
      },
      "source": [
        "# Training code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-BLISzysQkdA"
      },
      "outputs": [],
      "source": [
        "# @title [RUN] Hyperparameters GNN\n",
        "\n",
        "NUM_EPOCHS =  10 #@param {type:\"integer\"}\n",
        "LR         = 0.01 #@param {type:\"number\"}\n",
        "HIDDEN_DIM = 128  #@param {type:\"integer\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFTSH-Vuv4gk"
      },
      "outputs": [],
      "source": [
        "# Code taken from L45 practical notebook\n",
        "def train_gnn(X, edge_indices, y, mask, model, optimiser):\n",
        "    model.train()\n",
        "    optimiser.zero_grad()\n",
        "    y_out, _ = model(X, edge_indices)\n",
        "    y_hat = y_out[mask]\n",
        "    loss = F.cross_entropy(y_hat, y)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    return loss.data\n",
        "\n",
        "# Training loop using subgraph batching from paper 'Inductive Representation Learning on Large Graphs' https://arxiv.org/pdf/1706.02216.pdf\n",
        "def train_gnn_subgraph(data_batch, model, optimiser):\n",
        "  total_loss = 0\n",
        "  for batch in data_batch:\n",
        "    batch_size = batch.batch_size\n",
        "    optimiser.zero_grad()\n",
        "    y_out, _ = model(batch.x, batch.edge_index)\n",
        "    y_out = y_out[:batch_size]\n",
        "    batch_y = batch.y[:batch_size]\n",
        "    batch_y = torch.reshape(batch_y, (-1,))\n",
        "    loss = F.cross_entropy(y_out, batch_y)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    # Keep a running total of the loss\n",
        "    total_loss += float(loss)\n",
        "\n",
        "  # Get the average loss across all the batches\n",
        "  loss = total_loss / len(data_batch)\n",
        "  return loss\n",
        "\n",
        "def evaluate_gnn(X, edge_indices, y, mask, model, num_classes):\n",
        "    model.eval()\n",
        "    y_out, node_embeddings = model(X, edge_indices)\n",
        "    y_hat = y_out[mask]\n",
        "    y_hat = y_hat.data.max(1)[1]\n",
        "    num_correct = y_hat.eq(y.data).sum()\n",
        "    num_total = len(y)\n",
        "    accuracy = 100.0 * (num_correct/num_total)\n",
        "\n",
        "    # calculate per class accuracy\n",
        "    values, counts = torch.unique(y_hat[y_hat == y.data], return_counts=True)\n",
        "    per_class_counts = torch.zeros(num_classes)\n",
        "    # allocate the number of counts per class\n",
        "    for i, x in enumerate(values):\n",
        "      per_class_counts[x] = counts[i]\n",
        "    # find total number of data points per class in the split\n",
        "    total_per_class = torch.bincount(y.data)\n",
        "    per_class_accuracy = torch.div(per_class_counts, total_per_class)\n",
        "\n",
        "    return accuracy, per_class_accuracy, node_embeddings\n",
        "    \n",
        "# Training loop\n",
        "def train_eval_loop_gnn(model, edge_indices, train_x, train_y, train_mask, valid_x, valid_y, valid_mask, \n",
        "                             test_x, test_y, test_mask, num_classes, seed, filename, subgraph_batches=None):\n",
        "    optimiser = optim.Adam(model.parameters(), lr=LR)\n",
        "    training_stats = None\n",
        "    # Training loop\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # If subgraph batching is not provided, use the full graph for training. Otherwise use subgraph batch training regime\n",
        "        if subgraph_batches is None:\n",
        "          train_loss = train_gnn(train_x, edge_indices, train_y, train_mask, model, optimiser)\n",
        "        else:\n",
        "          train_loss = train_gnn_subgraph(subgraph_batches, model, optimiser)\n",
        "        # Calculate accuracy on full graph  \n",
        "        train_acc, train_class_acc, _ = evaluate_gnn(train_x, edge_indices, train_y, train_mask, model, num_classes)\n",
        "        valid_acc, valid_class_acc, _ = evaluate_gnn(valid_x, edge_indices, valid_y, valid_mask, model, num_classes)\n",
        "        if epoch % 10 == 0 or epoch == (NUM_EPOCHS-1):\n",
        "            print(f\"Epoch {epoch} with train loss: {train_loss:.3f} train accuracy: {train_acc:.3f} validation accuracy: {valid_acc:.3f}\")\n",
        "            print(\"Per class train accuracy: \", train_class_acc)\n",
        "            print(\"Per class val accuracy: \", valid_class_acc)\n",
        "        # store the loss and the accuracy for the final plot\n",
        "        epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n",
        "        training_stats = update_stats(training_stats, epoch_stats)\n",
        "\n",
        "    # Lets look at our final test performance\n",
        "    # Only need to get the node embeddings once, take from the training evaluation call\n",
        "    test_acc, test_class_acc, node_embeddings = evaluate_gnn(test_x, edge_indices, test_y, test_mask, model, num_classes)\n",
        "    print(f\"Our final test accuracy for the GNN is: {test_acc:.3f}\")\n",
        "    print(\"Final per class accuracy on test set: \", test_class_acc)\n",
        "\n",
        "    # Save training stats if on final iteration of the run\n",
        "    save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n",
        "    # Save final results\n",
        "    final_results_list = [seed, test_acc, test_class_acc, train_class_acc, valid_class_acc]\n",
        "    save_final_results(final_results_list, filename)\n",
        "    # Save final model weights incase we want to do further inference later\n",
        "    torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n",
        "    return training_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_O5m_YIaKY-"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed):\n",
        "  print(\"SETTING SEEDS TO: \", str(seed))\n",
        "  # seed the potential sources of randomness\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHANGE: To name of model being tested\n",
        "filename = \"GraphSAGE\"\n",
        "dataset = \"Cora\"\n",
        "# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n",
        "seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963, 1818027900, 2135956485, 3710910636, 1517964140, 4083009686, 2455059856, 400225693, 89475662, 361232447, 3647665043, 1221215631, 2036056847, 1860537279, 516507873, 3692371949, 3300171104, 2794978777, 3303475786, 2952735006, 572297925]\n",
        "\n",
        "# create folder for saving all model info into if it does not exist already\n",
        "if not os.path.exists(file_path+filename+\"/\"):\n",
        "  os.mkdir(file_path+filename+\"/\")\n",
        "\n",
        "if dataset == \"Cora\":\n",
        "  print(\"Using Cora data set\")\n",
        "  # Get the edge indices and node features for our model. General set up variables for running with all the models\n",
        "  edge_indices = cora_data.edge_index\n",
        "  node_features = cora_data.x\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = cora_data.train_mask\n",
        "  train_y = cora_data.y[train_mask]\n",
        "  valid_mask = cora_data.val_mask\n",
        "  valid_y = cora_data.y[valid_mask]\n",
        "  test_mask = cora_data.test_mask\n",
        "  test_y = cora_data.y[test_mask]\n",
        "\n",
        "  num_classes = 7\n",
        "# Otherwise we are using arvix dataset\n",
        "else:\n",
        "  # Get the edge indices and node features for our model\n",
        "  edge_indices = arxiv_data.edge_index\n",
        "  node_features = arxiv_data.x\n",
        "\n",
        "  # Get masks and training labels for each split\n",
        "  train_mask = train_idx\n",
        "  train_y = arxiv_data.y[train_mask]\n",
        "  valid_mask = valid_idx\n",
        "  valid_y = arxiv_data.y[valid_mask]\n",
        "  test_mask = test_idx\n",
        "  test_y = arxiv_data.y[test_mask]\n",
        "\n",
        "  num_classes = 40\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rl6KVverQy7C",
        "outputId": "7392ac02-08e5-45ea-8e36-77a2d56793d4"
      },
      "outputs": [],
      "source": [
        "# General training loop for all models except GraphSAGE, using the whole graph in training instead of using subgraph batching\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Create the model\n",
        "  model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes, v2=True)\n",
        "\n",
        "  # Run training loop\n",
        "  print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n",
        "                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename)\n",
        "  plot_stats(train_stats_cora, name=filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop for GraphSAGE which using subgraph batches instead of the entire graph\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Original paper uses neighbourhood sizes  S1 = 25 and S2 = 10 so this is what we use\n",
        "  train_loader = NeighborLoader(cora_data, num_neighbors = [25, 10], batch_size=128, input_nodes=cora_data.train_mask)\n",
        "\n",
        "  # Create the model\n",
        "  model = GraphSAGEModelWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes)\n",
        "\n",
        "  # Run training loop\n",
        "  print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n",
        "                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, subgraph_batches=train_loader)\n",
        "  plot_stats(train_stats_cora, name=filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4GMM4D5dLoB"
      },
      "source": [
        "# TESTING LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35RkMAgK8EHw",
        "outputId": "0ef2d1d1-5e0c-4057-a43f-49d0216cd6a7"
      },
      "outputs": [],
      "source": [
        "final_results = load_final_results(filename)\n",
        "for r in final_results:\n",
        "  print(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "bZ3GQQld9b97",
        "outputId": "36c49aee-27e6-42ed-81ff-d1d40ec0600b"
      },
      "outputs": [],
      "source": [
        "training_stats_1, embedding = load_training_info(filename+\"_1\")\n",
        "plot_stats(training_stats_1, name=\"Testing\")\n",
        "print(embedding)\n",
        "print(node_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1nR7AjndQJn",
        "outputId": "9eccfaf3-7828-431a-b5c6-79d6bc6847e4"
      },
      "outputs": [],
      "source": [
        "# Loading stored model weights\n",
        "model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n",
        "model.load_state_dict(torch.load(file_path+filename+\"/\"+\"GATV2_1_model.pt\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GubPzc9IQyP3"
      },
      "source": [
        "- Plot graph with average training stats\n",
        "- Save node embeddings for each run\n",
        "- Save training stats for each run\n",
        "- Save test accuracy for each run\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5HLu-NNuJ88"
      },
      "source": [
        "# Node2Vec Cora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LtiwsyJPxpt"
      },
      "outputs": [],
      "source": [
        "def save_training_info(training_stats: dict, filename: str):\n",
        "  # write training data info to a file\n",
        "  with open(file_path + filename + \".pkl\", 'wb') as fp:\n",
        "    pickle.dump(training_stats, fp)\n",
        "    print('Training stats saved successfully to file: ' + filename)\n",
        "  # write node embedding to a file\n",
        "  #torch.save(node_embedding, file_path + filename + \".pt\")\n",
        "  #print('Node embedding saved successfully to file: ' + filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp0VtUUmCtco"
      },
      "outputs": [],
      "source": [
        "# Code taken from L45 practical notebook\n",
        "def train_n2v_cora(X, edge_indices, y, mask, model, optimiser, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for pos_rw, neg_rw in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate_n2v_cora(X, edge_indices, y, mask, model, num_classes, loader):\n",
        "    model.eval()\n",
        "    z = model()\n",
        "    acc = model.test(z[cora_data.train_mask], cora_data.y[cora_data.train_mask],\n",
        "                      z[cora_data.val_mask], cora_data.y[cora_data.val_mask],\n",
        "                      max_iter=150)\n",
        "    #y_out = z[cora_data.train_mask]\n",
        "    y_hat = z[cora_data.val_mask].to(device)\n",
        "    y = cora_data.y[cora_data.val_mask].to(device)\n",
        "    \n",
        "    #model.eval()\n",
        "    #y_out, node_embeddings = model(X, edge_indices)\n",
        "    #y_hat = y_out[mask]\n",
        "    y_hat = y_hat.data.max(1)[1]\n",
        "    num_correct = y_hat.eq(y.data).sum()\n",
        "    num_total = len(y)\n",
        "    accuracy = 100.0 * (num_correct/num_total)\n",
        "\n",
        "    # calculate per class accuracy\n",
        "    values, counts = torch.unique(y_hat[y_hat == y.data], return_counts=True)\n",
        "    per_class_counts = torch.zeros(num_classes)\n",
        "    # allocate the number of counts per class\n",
        "    for i, x in enumerate(values):\n",
        "      per_class_counts[x] = counts[i]\n",
        "    per_class_counts = per_class_counts.to(device)\n",
        "    # find total number of data points per class in the split\n",
        "    total_per_class = torch.bincount(y.data).to(device)\n",
        "    per_class_accuracy = torch.div(per_class_counts, total_per_class)\n",
        "\n",
        "    return accuracy, per_class_accuracy\n",
        "    \n",
        "# Training loop\n",
        "def train_eval_loop_n2v_cora(model, edge_indices, train_x, train_y, train_mask, valid_x, valid_y, valid_mask, \n",
        "                             test_x, test_y, test_mask, num_classes, seed, filename, loader):\n",
        "    optimiser = optim.Adam(model.parameters(), lr=LR)\n",
        "    training_stats = None\n",
        "    # Training loop\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_loss = train_n2v_cora(train_x, edge_indices, train_y, train_mask, model, optimiser, loader)\n",
        "        train_acc, train_class_acc = evaluate_n2v_cora(train_x, edge_indices, train_y, train_mask, model, num_classes, loader)\n",
        "        valid_acc, valid_class_acc = evaluate_n2v_cora(valid_x, edge_indices, valid_y, valid_mask, model, num_classes, loader)\n",
        "        if epoch % 10 == 0 or epoch == (NUM_EPOCHS-1):\n",
        "            print(f\"Epoch {epoch} with train loss: {train_loss:.3f} train accuracy: {train_acc:.3f} validation accuracy: {valid_acc:.3f}\")\n",
        "            #print(\"Per class train accuracy: \", train_class_acc)\n",
        "            #print(\"Per class val accuracy: \", valid_class_acc)\n",
        "        # store the loss and the accuracy for the final plot\n",
        "        epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n",
        "        training_stats = update_stats(training_stats, epoch_stats)\n",
        "\n",
        "    # Lets look at our final test performance\n",
        "    # Only need to get the node embeddings once, take from the training evaluation call\n",
        "    test_acc, test_class_acc = evaluate_n2v_cora(test_x, edge_indices, test_y, test_mask, model, num_classes, loader)\n",
        "    print(f\"Our final test accuracy for the GNN is: {test_acc:.3f}\")\n",
        "    print(\"Final per class accuracy on test set: \", test_class_acc)\n",
        "\n",
        "    # Save training stats if on final iteration of the run\n",
        "    save_training_info(training_stats, filename+\"_\"+str(seed))\n",
        "    # Save final results\n",
        "    final_results_list = [seed, test_acc, test_class_acc, train_class_acc, valid_class_acc]\n",
        "    save_final_results(final_results_list, filename)\n",
        "    # Save final model weights incase we want to do further inference later\n",
        "    torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n",
        "    return training_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAdzhL_4AB9j",
        "outputId": "f4f11291-375a-497d-8bc2-4d6097e99d0b"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import Node2Vec\n",
        "\n",
        "# Get the edge indices and node features for our model\n",
        "edge_indices = cora_data.edge_index\n",
        "node_features = cora_data.x\n",
        "\n",
        "# Get masks and training labels for each split\n",
        "train_mask = cora_data.train_mask\n",
        "train_y = cora_data.y[train_mask]\n",
        "valid_mask = cora_data.val_mask\n",
        "valid_y = cora_data.y[valid_mask]\n",
        "test_mask = cora_data.test_mask\n",
        "test_y = cora_data.y[test_mask]\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "num_classes = 7\n",
        "# CHANGE: To name of model being tested\n",
        "filename = \"Node2Vec\"\n",
        "# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n",
        "seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963, 1818027900, 2135956485, 3710910636, 1517964140, 4083009686, 2455059856, 400225693, 89475662, 361232447, 3647665043, 1221215631, 2036056847, 1860537279, 516507873, 3692371949, 3300171104, 2794978777, 3303475786, 2952735006, 572297925]\n",
        "\n",
        "# create folder for saving all model info into if it does not exist already\n",
        "if not os.path.exists(file_path+filename+\"/\"):\n",
        "  os.mkdir(file_path+filename+\"/\")\n",
        "\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Create the model\n",
        "  #model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n",
        "  model = Node2Vec(cora_data.edge_index.to(device), embedding_dim=128, walk_length=20,\n",
        "                     context_size=10, walks_per_node=10,\n",
        "                     num_negative_samples=1, p=1, q=1, sparse=True).to(device)\n",
        "  loader = model.loader(batch_size=128, shuffle=True,\n",
        "                          num_workers=0)\n",
        "  optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
        "\n",
        "  #train_stats_cora = train_eval_loop_gnn_cora(model, edge_indices, node_features, train_y, train_mask, \n",
        "                                            #node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, loader)\n",
        "\n",
        "  def train():\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      for pos_rw, neg_rw in loader:\n",
        "          optimizer.zero_grad()\n",
        "          loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += loss.item()\n",
        "          #print(loss.data)\n",
        "      #epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n",
        "      return total_loss / len(loader)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test():\n",
        "    model.eval()\n",
        "    z = model()\n",
        "    acc_train = model.test(z[cora_data.train_mask], cora_data.y[cora_data.train_mask],\n",
        "                      z[cora_data.train_mask], cora_data.y[cora_data.train_mask],\n",
        "                      max_iter=150)\n",
        "    acc_val = model.test(z[cora_data.train_mask], cora_data.y[cora_data.train_mask],\n",
        "                      z[cora_data.val_mask], cora_data.y[cora_data.val_mask],\n",
        "                      max_iter=150)\n",
        "    acc_test = model.test(z[cora_data.train_mask], cora_data.y[cora_data.train_mask],\n",
        "                      z[cora_data.test_mask], cora_data.y[cora_data.test_mask],\n",
        "                      max_iter=150)\n",
        "    y_train = cora_data.y[cora_data.train_mask].to(device)\n",
        "    y_hat_train = z[cora_data.train_mask].to(device)\n",
        "    y_val = cora_data.y[cora_data.val_mask].to(device)\n",
        "    y_hat_val = z[cora_data.val_mask].to(device)\n",
        "    y_test = cora_data.y[cora_data.test_mask].to(device)\n",
        "    y_hat_test = z[cora_data.test_mask].to(device)\n",
        "\n",
        "    loss_train = F.cross_entropy(y_hat_train, y_train)\n",
        "    loss_val = F.cross_entropy(y_hat_val, y_val)\n",
        "    loss_test = F.cross_entropy(y_hat_test, y_test)\n",
        "\n",
        "    return acc_train, acc_val, acc_test, loss_train, loss_val, loss_test\n",
        "\n",
        "  training_stats = None\n",
        "  for epoch in range(1, 40):\n",
        "    loss = train()\n",
        "    acc_train, acc_val, acc_test, loss_train, loss_val, loss_test = test()\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc_train: {acc_train:.4f}, Acc_val: {acc_val:.4f}, Acc_test: {acc_test:.4f}')\n",
        "    epoch_stats = {'train_acc': acc_train, 'val_acc': acc_val, 'test_acc': acc_test, 'epoch':epoch}\n",
        "    training_stats = update_stats(training_stats, epoch_stats)\n",
        "  \n",
        "  # Save training stats if on final iteration of the run\n",
        "  save_training_info(training_stats, filename+\"_\"+str(seed))\n",
        "  # Save final results\n",
        "  #final_results_list = [seed, acc_test, test_class_acc, train_class_acc, valid_class_acc]\n",
        "  #save_final_results(final_results_list, filename)\n",
        "  # Save final model weights incase we want to do further inference later\n",
        "  torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n",
        "  # Run training loop\n",
        "  #print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  #train_stats_cora = train_eval_loop_gnn_cora(model, edge_indices, node_features, train_y, train_mask, \n",
        "  #                                          node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename)\n",
        "  #plot_stats(train_stats_cora, name=filename+\"_Cora\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "pICUdsJ31YO6",
        "outputId": "dd7b2963-59c6-46be-da20-9c0a7a0806bb"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import Node2Vec\n",
        "\n",
        "# Get the edge indices and node features for our model\n",
        "edge_indices = cora_data.edge_index\n",
        "node_features = cora_data.x\n",
        "\n",
        "# Get masks and training labels for each split\n",
        "train_mask = cora_data.train_mask\n",
        "train_y = cora_data.y[train_mask]\n",
        "valid_mask = cora_data.val_mask\n",
        "valid_y = cora_data.y[valid_mask]\n",
        "test_mask = cora_data.test_mask\n",
        "test_y = cora_data.y[test_mask]\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "num_classes = 7\n",
        "# CHANGE: To name of model being tested\n",
        "filename = \"Node2Vec\"\n",
        "# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n",
        "seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963, 1818027900, 2135956485, 3710910636, 1517964140, 4083009686, 2455059856, 400225693, 89475662, 361232447, 3647665043, 1221215631, 2036056847, 1860537279, 516507873, 3692371949, 3300171104, 2794978777, 3303475786, 2952735006, 572297925]\n",
        "\n",
        "# create folder for saving all model info into if it does not exist already\n",
        "if not os.path.exists(file_path+filename+\"/\"):\n",
        "  os.mkdir(file_path+filename+\"/\")\n",
        "\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Create the model\n",
        "  #model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n",
        "  model = Node2Vec(cora_data.edge_index.to(device), embedding_dim=128, walk_length=20,\n",
        "                     context_size=10, walks_per_node=10,\n",
        "                     num_negative_samples=1, p=1, q=1, sparse=True).to(device)\n",
        "  loader = model.loader(batch_size=128, shuffle=True,\n",
        "                          num_workers=0)\n",
        "  optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
        "\n",
        "  #train_stats_cora = train_eval_loop_gnn_cora(model, edge_indices, node_features, train_y, train_mask, \n",
        "                                            #node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, loader)\n",
        "\n",
        "  def train():\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      for pos_rw, neg_rw in loader:\n",
        "          optimizer.zero_grad()\n",
        "          loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += loss.item()\n",
        "          #print(loss.data)\n",
        "      #epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n",
        "      return total_loss / len(loader)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test():\n",
        "    model.eval()\n",
        "    z = model()\n",
        "    print(z[cora_data.train_mask].shape, cora_data.y[cora_data.train_mask].shape,\n",
        "                      z[cora_data.train_mask].shape, cora_data.y[cora_data.train_mask].shape)\n",
        "    acc_train = model.test(z[cora_data.train_mask], cora_data.y[cora_data.train_mask],\n",
        "                      z[cora_data.train_mask], cora_data.y[cora_data.train_mask],\n",
        "                      max_iter=150)\n",
        "    acc_val = model.test(z[cora_data.train_mask], cora_data.y[cora_data.train_mask],\n",
        "                      z[cora_data.val_mask], cora_data.y[cora_data.val_mask],\n",
        "                      max_iter=150)\n",
        "    acc_test = model.test(z[cora_data.train_mask], cora_data.y[cora_data.train_mask],\n",
        "                      z[cora_data.test_mask], cora_data.y[cora_data.test_mask],\n",
        "                      max_iter=150)\n",
        "    \n",
        "    y_train = cora_data.y[cora_data.train_mask].to(device)\n",
        "    y_hat_train = z[cora_data.train_mask].to(device)\n",
        "    y_val = cora_data.y[cora_data.val_mask].to(device)\n",
        "    y_hat_val = z[cora_data.val_mask].to(device)\n",
        "    y_test = cora_data.y[cora_data.test_mask].to(device)\n",
        "    y_hat_test = z[cora_data.test_mask].to(device)\n",
        "    print(y_hat_train.shape, y_train.shape)\n",
        "\n",
        "    loss_train = F.cross_entropy(y_hat_train, y_train)\n",
        "    loss_val = F.cross_entropy(y_hat_val, y_val)\n",
        "    loss_test = F.cross_entropy(y_hat_test, y_test)\n",
        "\n",
        "    return acc_train, acc_val, acc_test, loss_train, loss_val, loss_test\n",
        "\n",
        "  training_stats = None\n",
        "  for epoch in range(1, 40):\n",
        "    loss = train()\n",
        "    acc_train, acc_val, acc_test, loss_train, loss_val, loss_test = test()\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc_train: {acc_train:.4f}, Acc_val: {acc_val:.4f}, Acc_test: {acc_test:.4f}')\n",
        "    epoch_stats = {'train_acc': acc_train, 'val_acc': acc_val, 'test_acc': acc_test, 'epoch':epoch}\n",
        "    training_stats = update_stats(training_stats, epoch_stats)\n",
        "  \n",
        "  # Save training stats if on final iteration of the run\n",
        "  save_training_info(training_stats, filename+\"_\"+str(seed))\n",
        "  # Save final results\n",
        "  #final_results_list = [seed, acc_test, test_class_acc, train_class_acc, valid_class_acc]\n",
        "  #save_final_results(final_results_list, filename)\n",
        "  # Save final model weights incase we want to do further inference later\n",
        "  torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n",
        "  # Run training loop\n",
        "  #print(\"TRAINING WITH SEED: \", str(seed))\n",
        "  #train_stats_cora = train_eval_loop_gnn_cora(model, edge_indices, node_features, train_y, train_mask, \n",
        "  #                                          node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename)\n",
        "  #plot_stats(train_stats_cora, name=filename+\"_Cora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hsOHfeMuYDl"
      },
      "source": [
        "# FastRP (attempt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c87sFYfgwi-"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import itertools\n",
        "import math\n",
        "import matplotlib\n",
        "import time\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn import random_projection\n",
        "from sklearn.preprocessing import normalize, scale, MultiLabelBinarizer\n",
        "from scipy.sparse import coo_matrix, csr_matrix, csc_matrix, spdiags\n",
        "\n",
        "# projection method: choose from Gaussian and Sparse\n",
        "# input matrix: choose from adjacency and transition matrix\n",
        "# alpha adjusts the weighting of nodes according to their degree\n",
        "def fastrp_projection(A, q=3, dim=128, projection_method='gaussian', input_matrix='adj', alpha=None):\n",
        "    assert input_matrix == 'adj' or input_matrix == 'trans'\n",
        "    assert projection_method == 'gaussian' or projection_method == 'sparse'\n",
        "    \n",
        "    if input_matrix == 'adj':\n",
        "        M = A\n",
        "    else:\n",
        "        N = A.shape[0]\n",
        "        normalizer = spdiags(np.squeeze(1.0 / csc_matrix.sum(A, axis=1) ), 0, N, N)\n",
        "        M = normalizer @ A\n",
        "    # Gaussian projection matrix\n",
        "    if projection_method == 'gaussian':\n",
        "        transformer = random_projection.GaussianRandomProjection(n_components=dim, random_state=42)\n",
        "    # Sparse projection matrix\n",
        "    else:\n",
        "        transformer = random_projection.SparseRandomProjection(n_components=dim, random_state=42)\n",
        "    Y = transformer.fit(M)\n",
        "    # Random projection for A\n",
        "    if alpha is not None:\n",
        "        Y.components_ = Y.components_ @ spdiags( \\\n",
        "                        np.squeeze(np.power(csc_matrix.sum(A, axis=1), alpha)), 0, N, N)\n",
        "    cur_U = transformer.transform(M)\n",
        "    U_list = [cur_U]\n",
        "    \n",
        "    for i in range(2, q + 1):\n",
        "        cur_U = M @ cur_U\n",
        "        U_list.append(cur_U)\n",
        "    return U_list\n",
        "\n",
        "# When weights is None, concatenate instead of linearly combines the embeddings from different powers of A\n",
        "def fastrp_merge(U_list, weights, normalization=False):\n",
        "    dense_U_list = [_U.todense() for _U in U_list] if type(U_list[0]) == csc_matrix else U_list\n",
        "    _U_list = [normalize(_U, norm='l2', axis=1) for _U in dense_U_list] if normalization else dense_U_list\n",
        "\n",
        "    if weights is None:\n",
        "        return np.concatenate(_U_list, axis=1)\n",
        "    U = np.zeros_like(_U_list[0])\n",
        "    for cur_U, weight in zip(_U_list, weights):\n",
        "        U += cur_U * weight\n",
        "    # U = scale(U.todense())\n",
        "    # U = normalize(U.todense(), norm='l2', axis=1)\n",
        "    return scale(U.todense()) if type(U) == csr_matrix else scale(U)\n",
        "\n",
        "# A is always the adjacency matrix\n",
        "# the choice between adj matrix and trans matrix is decided in the conf\n",
        "def fastrp_wrapper(A, conf):\n",
        "    U_list = fastrp_projection(A,\n",
        "                               q=len(conf['weights']),\n",
        "                               dim=conf['dim'],\n",
        "                               projection_method=conf['projection_method'],\n",
        "                               input_matrix=conf['input_matrix'],\n",
        "                               alpha=conf['alpha'],\n",
        "    )\n",
        "    U = fastrp_merge(U_list, conf['weights'], conf['normalization'])\n",
        "    return U\n",
        "\n",
        "def get_emb_filename(prefix, conf):\n",
        "    return prefix + '-dim=' + str(conf['dim']) + ',projection_method=' + conf['projection_method'] \\\n",
        "        + ',input_matrix=' + conf['input_matrix'] + ',normalization=' + str(conf['normalization']) \\\n",
        "        + ',weights=' + (','.join(map(str, conf['weights'])) if conf['weights'] is not None else 'None') \\\n",
        "        + ',alpha=' + (str(conf['alpha']) if 'alpha' in conf else '') \\\n",
        "        + ',C=' + (str(conf['C']) if 'alpha' in conf else '1.0') \\\n",
        "        + '.mat'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xcjxh_EhBrr"
      },
      "outputs": [],
      "source": [
        "fastrp_projection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rYJ3891ucjj"
      },
      "source": [
        "#  OGBN-archiv dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "vZ2WM2WDycNu",
        "outputId": "4840fdda-e74b-471e-84b7-a0de4d8e96d6"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import Node2Vec\n",
        "\n",
        "# Get the edge indices and node features for our model\n",
        "edge_indices = arxiv_data.edge_index\n",
        "node_features = arxiv_data.x\n",
        "\n",
        "# Get masks and training labels for each split\n",
        "train_mask = train_idx\n",
        "train_y = arxiv_data.y[train_mask]\n",
        "valid_mask = valid_idx\n",
        "valid_y = arxiv_data.y[valid_mask]\n",
        "test_mask = test_idx\n",
        "test_y = arxiv_data.y[test_mask]\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "num_classes = 40\n",
        "# CHANGE: To name of model being tested\n",
        "filename = \"Node2Vec_arxiv\"\n",
        "# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n",
        "seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963]\n",
        "\n",
        "# create folder for saving all model info into if it does not exist already\n",
        "if not os.path.exists(file_path+filename+\"/\"):\n",
        "  os.mkdir(file_path+filename+\"/\")\n",
        "\n",
        "for seed in seeds:\n",
        "  set_seeds(seed)\n",
        "  # Create the model\n",
        "  #model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n",
        "  model = Node2Vec(arxiv_data.edge_index.to(device), embedding_dim=128, walk_length=20,\n",
        "                     context_size=10, walks_per_node=10,\n",
        "                     num_negative_samples=1, p=1, q=1, sparse=True).to(device)\n",
        "  loader = model.loader(batch_size=128, shuffle=True,\n",
        "                          num_workers=0)\n",
        "  optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
        "\n",
        "  def train():\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      for pos_rw, neg_rw in loader:\n",
        "          optimizer.zero_grad()\n",
        "          loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += loss.item()\n",
        "          #print(loss.data)\n",
        "      #epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n",
        "      return total_loss / len(loader)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test():\n",
        "    model.eval()\n",
        "    z = model()\n",
        "    print(z[train_mask].shape, arxiv_data.y[train_mask].shape,\n",
        "                      z[train_mask].shape, arxiv_data.y[train_mask].shape)\n",
        "    acc_train = model.test(z[train_mask], arxiv_data.y[train_mask],\n",
        "                      z[train_mask], arxiv_data.y[train_mask],\n",
        "                      max_iter=150)\n",
        "    acc_val = model.test(z[train_mask], arxiv_data.y[train_mask],\n",
        "                      z[valid_mask], arxiv_data.y[valid_mask],\n",
        "                      max_iter=150)\n",
        "    acc_test = model.test(z[train_mask], arxiv_data.y[train_mask],\n",
        "                      z[test_mask], arxiv_data.y[test_mask],\n",
        "                      max_iter=150)\n",
        "    y_train = arxiv_data.y[train_mask].to(device).squeeze()\n",
        "    y_hat_train = z[train_mask].to(device)\n",
        "    print(y_hat_train.shape, y_train.shape)\n",
        "    y_val = arxiv_data.y[valid_mask].to(device).squeeze()\n",
        "    y_hat_val = z[valid_mask].to(device)\n",
        "    print(y_hat_val.shape, y_val.shape)\n",
        "    y_test = arxiv_data.y[test_mask].to(device).squeeze()\n",
        "    y_hat_test = z[test_mask].to(device)\n",
        "    print(y_hat_test.shape, y_test.shape)\n",
        "\n",
        "    loss_train = F.cross_entropy(y_hat_train, y_train)\n",
        "    loss_val = F.cross_entropy(y_hat_val, y_val)\n",
        "    loss_test = F.cross_entropy(y_hat_test, y_test)\n",
        "\n",
        "    return acc_train, acc_val, acc_test, loss_train, loss_val, loss_test\n",
        "\n",
        "  training_stats = None\n",
        "  for epoch in range(1, 40):\n",
        "    loss = train()\n",
        "    acc_train, acc_val, acc_test, loss_train, loss_val, loss_test = test()\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc_train: {acc_train:.4f}, Acc_val: {acc_val:.4f}, Acc_test: {acc_test:.4f}')\n",
        "    epoch_stats = {'train_acc': acc_train, 'val_acc': acc_val, 'test_acc': acc_test, 'epoch':epoch}\n",
        "    training_stats = update_stats(training_stats, epoch_stats)\n",
        "  \n",
        "  # Save training stats if on final iteration of the run\n",
        "  save_training_info(training_stats, filename+\"_\"+str(seed))\n",
        "  # Save final results\n",
        "  #final_results_list = [seed, acc_test, test_class_acc, train_class_acc, valid_class_acc]\n",
        "  #save_final_results(final_results_list, filename)\n",
        "  # Save final model weights incase we want to do further inference later\n",
        "  torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTOH4-23gSaE",
        "outputId": "af9bc343-f8b7-4959-a879-626ec2aa1bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'builtin_function_or_method'>\n"
          ]
        }
      ],
      "source": [
        "#print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc_train: {acc_train:.4f}, Acc_val: {acc_val:.4f}, Acc_test: {acc_test:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGtX2ycNQa-H"
      },
      "source": [
        "# Similarity tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eesbQaUQfd4"
      },
      "source": [
        "https://github.com/SGDE2020/embedding_stability/blob/master/similarity_tests/similarity_tests.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NGtX2ycNQa-H"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
