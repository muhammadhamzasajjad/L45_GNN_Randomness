{"cells":[{"cell_type":"markdown","metadata":{"id":"qU87TNON39IV"},"source":["# **Preliminaries:** Install and import modules"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mpygj8TTZ-ur","outputId":"e2a548f0-d5fb-4ed1-af68-89e942088292","executionInfo":{"status":"ok","timestamp":1679310090819,"user_tz":0,"elapsed":51879,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mycolorpy\n","  Downloading mycolorpy-1.5.1.tar.gz (2.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mycolorpy) (1.22.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mycolorpy) (3.7.1)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (5.12.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (8.4.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (2.8.2)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (4.39.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (1.4.4)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (3.0.9)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (0.11.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (23.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mycolorpy) (3.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mycolorpy) (1.15.0)\n","Building wheels for collected packages: mycolorpy\n","  Building wheel for mycolorpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mycolorpy: filename=mycolorpy-1.5.1-py3-none-any.whl size=3874 sha256=e70cf7ca4a431bfdffb62d8bf0a26bc9b22abd749d3fe1670682dab80fcfefd8\n","  Stored in directory: /root/.cache/pip/wheels/b9/56/d6/a163bcbec3bb69f3f7797b1b542870b18d7e31ff5dbc0b87e3\n","Successfully built mycolorpy\n","Installing collected packages: mycolorpy\n","Successfully installed mycolorpy-1.5.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting colorama\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Installing collected packages: colorama\n","Successfully installed colorama-0.4.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ogb\n","  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.13.1+cu116)\n","Collecting outdated>=0.2.0\n","  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.4.4)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.26.15)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.2.2)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.22.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.15.0)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (4.65.0)\n","Collecting littleutils\n","  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (63.4.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n","Building wheels for collected packages: littleutils\n","  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=40714e7e3fff41bd1fec59e36a383ff0d356805deef0952bb0e6b2d40690ca1e\n","  Stored in directory: /root/.cache/pip/wheels/04/bb/0d/2d02ec45f29c48d6192476bfb59c5a0e64b605e7212374dd15\n","Successfully built littleutils\n","Installing collected packages: littleutils, outdated, ogb\n","Successfully installed littleutils-0.2.2 ogb-1.3.5 outdated-0.2.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 KB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.1%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (9.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.17%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_cluster-1.6.1%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (3.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.27.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (5.9.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (3.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.1.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=378dc6bade3f98482552d3793ee5f08a19db9c0b01005f8b228c82ba1167a94b\n","  Stored in directory: /root/.cache/pip/wheels/31/b2/8c/9b4bb72a4384eabd1ffeab2b7ead692c9165e35711f8a9dc72\n","Successfully built torch-geometric\n","Installing collected packages: torch-scatter, torch-sparse, torch-cluster, torch-geometric\n","Successfully installed torch-cluster-1.6.1+pt113cu116 torch-geometric-2.2.0 torch-scatter-2.1.1+pt113cu116 torch-sparse-0.6.17+pt113cu116\n"]}],"source":["#@title [RUN] install\n","!pip install networkx\n","!pip install mycolorpy\n","!pip install colorama\n","!pip install ogb\n","\n","import torch\n","import os\n","!pip install torch-geometric torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"ZLrrWpkk6xv-","executionInfo":{"status":"ok","timestamp":1679310359011,"user_tz":0,"elapsed":298,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["#@title [RUN] Import modules\n","import numpy as np\n","import seaborn as sns\n","import math\n","import itertools\n","import scipy as sp\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch_geometric\n","from torch_geometric.datasets import Planetoid, Coauthor\n","from torch_scatter import scatter_mean, scatter_max, scatter_sum\n","from torch_geometric.utils import to_dense_adj\n","from torch.nn import Embedding\n","from torch_geometric.typing import Adj\n","from ogb.nodeproppred import PygNodePropPredDataset\n","from torch_geometric.loader import NeighborLoader\n","from torch_geometric.utils import to_scipy_sparse_matrix\n","\n","#For FastRP\n","from scipy.sparse import coo_matrix, csr_matrix, csc_matrix, spdiags\n","from sklearn.preprocessing import normalize, scale, MultiLabelBinarizer\n","from sklearn import random_projection\n","\n","\n","import pdb\n","from datetime import datetime\n","\n","#for nice visualisations\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","\n","from mycolorpy import colorlist as mcp\n","import matplotlib.cm as cm\n","\n","from typing import Mapping, Tuple, Sequence, List\n","import colorama\n","\n","import scipy.linalg\n","from scipy.linalg import block_diag"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"VLrKgQEuwgtb","executionInfo":{"status":"ok","timestamp":1679310106896,"user_tz":0,"elapsed":355,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["####### PLOTS #######\n","\n","def update_stats(training_stats, epoch_stats):\n","    \"\"\" Store metrics along the training\n","    Args:\n","      epoch_stats: dict containg metrics about one epoch\n","      training_stats: dict containing lists of metrics along training\n","    Returns:\n","      updated training_stats\n","    \"\"\"\n","    if training_stats is None:\n","        training_stats = {}\n","        for key in epoch_stats.keys():\n","            training_stats[key] = []\n","    for key,val in epoch_stats.items():\n","        training_stats[key].append(val)\n","    return training_stats\n","\n","def plot_stats(training_stats, figsize=(5, 5), name=\"\"):\n","    \"\"\" Create one plot for each metric stored in training_stats\n","    \"\"\"\n","    stats_names = [key[6:] for key in training_stats.keys() if key.startswith('train_')]\n","    f, ax = plt.subplots(len(stats_names), 1, figsize=figsize)\n","    if len(stats_names)==1:\n","        ax = np.array([ax])\n","    for key, axx in zip(stats_names, ax.reshape(-1,)):\n","        axx.plot(\n","            training_stats['epoch'],\n","            training_stats[f'train_{key}'],\n","            label=f\"Training {key}\")\n","        axx.plot(\n","            training_stats['epoch'],\n","            training_stats[f'val_{key}'],\n","            label=f\"Validation {key}\")\n","        axx.set_xlabel(\"Training epoch\")\n","        axx.set_ylabel(key)\n","        axx.legend()\n","    plt.title(name)\n","\n","\n","def get_color_coded_str(i, color):\n","    return \"\\033[3{}m{}\\033[0m\".format(int(color), int(i))\n","\n","def print_color_numpy(map, list_graphs):\n","    \"\"\" print matrix map in color according to list_graphs\n","    \"\"\"\n","    list_blocks = []\n","    for i,graph in enumerate(list_graphs):\n","        block_i = (i+1)*np.ones((graph.num_nodes,graph.num_nodes))\n","        list_blocks += [block_i]\n","    block_color = block_diag(*list_blocks)\n","    \n","    map_modified = np.vectorize(get_color_coded_str)(map, block_color)\n","    print(\"\\n\".join([\" \".join([\"{}\"]*map.shape[0])]*map.shape[1]).format(*[x for y in map_modified.tolist() for x in y]))"]},{"cell_type":"markdown","metadata":{"id":"82mrcZX0A3QR"},"source":["# Cora dataset\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bBTnJEZWA-Iq","outputId":"04c85f2b-19cb-454b-ac70-11d9750846a7","executionInfo":{"status":"ok","timestamp":1679310113355,"user_tz":0,"elapsed":3640,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n","Processing...\n","Done!\n"]},{"output_type":"execute_result","data":{"text/plain":["Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"]},"metadata":{},"execution_count":4}],"source":["cora_dataset = Planetoid(\"/tmp/cora\", name=\"cora\", split=\"full\")\n","cora_data = cora_dataset[0]\n","cora_data"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UuZwDeBwJPZg","outputId":"013881ec-f785-412c-f807-8c0b906bc55d","executionInfo":{"status":"ok","timestamp":1679310115438,"user_tz":0,"elapsed":567,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Training class sizes\n","tensor([160,  90, 196, 341, 196, 138,  87])\n","Validation class sizes\n","tensor([ 61,  36,  78, 158,  81,  57,  29])\n","Test class sizes\n","tensor([130,  91, 144, 319, 149, 103,  64])\n"]}],"source":["print(\"Training class sizes\")\n","print(torch.bincount(cora_dataset[0].y[cora_dataset[0].train_mask]))\n","print(\"Validation class sizes\")\n","print(torch.bincount(cora_dataset[0].y[cora_dataset[0].val_mask]))\n","print(\"Test class sizes\")\n","print(torch.bincount(cora_dataset[0].y[cora_dataset[0].test_mask]))"]},{"cell_type":"markdown","metadata":{"id":"I9AoJuMmKQAO"},"source":["# OBGN-ARVIX dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jswepg0KKQAP","outputId":"b9ad4d34-ac36-478c-bf85-dd51cbf983f5","executionInfo":{"status":"ok","timestamp":1679310195818,"user_tz":0,"elapsed":78163,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"]},{"output_type":"stream","name":"stderr","text":["Downloaded 0.08 GB: 100%|██████████| 81/81 [00:07<00:00, 11.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting dataset/arxiv.zip\n"]},{"output_type":"stream","name":"stderr","text":["Processing...\n"]},{"output_type":"stream","name":"stdout","text":["Loading necessary files...\n","This might take a while.\n","Processing graphs...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00, 1792.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Converting graphs into PyG objects...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00, 4275.54it/s]"]},{"output_type":"stream","name":"stdout","text":["Saving...\n"]},{"output_type":"stream","name":"stderr","text":["\n","Done!\n"]},{"output_type":"execute_result","data":{"text/plain":["Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343], y=[169343])"]},"metadata":{},"execution_count":6}],"source":["d_name = \"ogbn-arxiv\"\n","\n","dataset = PygNodePropPredDataset(name = d_name)\n","\n","split_idx = dataset.get_idx_split()\n","train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n","arxiv_data = dataset[0]\n","arxiv_data.y = arxiv_data.y.squeeze()\n","arxiv_data.node_year = arxiv_data.node_year.squeeze()\n","arxiv_data"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xj0Rb5CQKyLB","outputId":"0f1b3b56-1402-4f27-8e05-6513b0873b25","executionInfo":{"status":"ok","timestamp":1679310198809,"user_tz":0,"elapsed":290,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Training class sizes\n","tensor([  437,   382,  3604,  1014,  2864,  2933,   703,   380,  4056,  2245,\n","         5182,   391,    21,  1290,   473,   248,  9998,   202,   402,  1873,\n","         1495,   304,  1268,  1539,  6989,   457,  2854,  1661, 16284,   239,\n","         4334,  1350,   270,   926,  5426,    75,  2506,  1615,  1100,  1551])\n","Validation class sizes\n","tensor([  74,  118,  502,  412, 1129,  779,  293,   75,  926,  230, 1232,  120,\n","           3,  440,   53,   68, 6846,  110,  138,  585,  268,   38,  249,  487,\n","        4458,  325,  710, 1074, 2273,   57, 2849,  586,   58,  125, 1027,   16,\n","         391,  273,  193,  209])\n","Test class sizes\n","tensor([   54,   187,   733,   654,  1869,  1246,   622,   134,  1250,   345,\n","         1455,   239,     5,   628,    71,    87, 10477,   203,   209,   419,\n","          313,    51,   386,   808, 10740,   475,  1041,  2066,  2849,   120,\n","         4631,   892,    83,   220,  1414,    36,   627,   481,   214,   269])\n"]}],"source":["print(\"Training class sizes\")\n","print(torch.bincount(arxiv_data.y[train_idx]))\n","print(\"Validation class sizes\")\n","print(torch.bincount(arxiv_data.y[valid_idx]))\n","print(\"Test class sizes\")\n","print(torch.bincount(arxiv_data.y[test_idx]))"]},{"cell_type":"markdown","metadata":{"id":"xY5bb2rDEuqM"},"source":["#Coauthor dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g9ZXzG7SE4oO","outputId":"cc663744-b60d-400e-b923-6c93bd238687","executionInfo":{"status":"ok","timestamp":1679310206789,"user_tz":0,"elapsed":4041,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ms_academic_cs.npz\n","Processing...\n","Done!\n"]},{"output_type":"execute_result","data":{"text/plain":["Data(x=[18333, 6805], edge_index=[2, 163788], y=[18333])"]},"metadata":{},"execution_count":8}],"source":["cs_dataset = Coauthor(\"/tmp/coauthor\", name=\"CS\")\n","cs_data = cs_dataset[0]\n","cs_data"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KO3BwlveIuNv","outputId":"14f69318-d8b0-4fcd-84ea-ff810b728b77","executionInfo":{"status":"ok","timestamp":1679310209268,"user_tz":0,"elapsed":293,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["10993 3668 3672\n"]}],"source":["# Create manual split, do 60:20:20 across classes\n","num_classes_cs = 15\n","train_mask_cs_indices = []\n","val_mask_cs_indices = []\n","test_mask_cs_indices = []\n","cs_labels = cs_data.y\n","for i in range(num_classes_cs):\n","\n","  class_i = np.where(cs_labels == i)[0]\n","  np.random.seed(0)\n","  np.random.shuffle(class_i)\n","\n","  num_samples = len(class_i)\n","  train_mask_cs_indices += (class_i[:int(num_samples*0.6)]).tolist() \n","  val_mask_cs_indices += (class_i[int(num_samples*0.6):int(num_samples*0.8)]).tolist() \n","  test_mask_cs_indices += (class_i[int(num_samples*0.8):]).tolist() \n","\n","print(len(train_mask_cs_indices), len(val_mask_cs_indices), len(test_mask_cs_indices))\n","# Create the masks for training\n","# Test mask \n","train_mask_cs = torch.full((len(cs_labels),), False)\n","train_mask_cs[train_mask_cs_indices] = True\n","# Val mask\n","val_mask_cs = torch.full((len(cs_labels),), False)\n","val_mask_cs[val_mask_cs_indices] = True\n","# Train mask\n","test_mask_cs = torch.full((len(cs_labels),), False)\n","test_mask_cs[test_mask_cs_indices] = True"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lO6lZz9SE6kK","outputId":"adbf639e-3a43-46ea-931c-f1f90cd056a8","executionInfo":{"status":"ok","timestamp":1679310214402,"user_tz":0,"elapsed":344,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Training class sizes\n","tensor([ 424,  277, 1230,  257,  836, 1315,  222,  554,  465,   70,  866, 1219,\n","         252, 2481,  525])\n","Validation class sizes\n","tensor([142,  92, 410,  86, 279, 439,  74, 185, 155,  24, 289, 407,  84, 827,\n","        175])\n","Test class sizes\n","tensor([142,  93, 410,  86, 279, 439,  75, 185, 155,  24, 289, 407,  84, 828,\n","        176])\n"]}],"source":["print(\"Training class sizes\")\n","print(torch.bincount(cs_data.y[train_mask_cs]))\n","print(\"Validation class sizes\")\n","print(torch.bincount(cs_data.y[val_mask_cs]))\n","print(\"Test class sizes\")\n","print(torch.bincount(cs_data.y[test_mask_cs]))"]},{"cell_type":"markdown","metadata":{"id":"KL8gKs07J9JP"},"source":["# Data saving / loading"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z80lU1V-Isky","outputId":"149fe655-c829-4def-9339-110ca748b524","executionInfo":{"status":"ok","timestamp":1679310236780,"user_tz":0,"elapsed":19517,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# use google drive for saving and loading information\n","from google.colab import drive\n","import pickle\n","import os\n","\n","drive.mount('/content/drive')\n","file_path = '/content/drive/MyDrive/L45_project/'\n","# create folder if it does not exist already\n","if not os.path.exists(file_path):\n","  os.mkdir(file_path) "]},{"cell_type":"code","execution_count":12,"metadata":{"id":"VhUfVQAZTWHu","executionInfo":{"status":"ok","timestamp":1679310239272,"user_tz":0,"elapsed":569,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["def save_training_info(training_stats: dict, node_embedding: torch.Tensor, filename: str):\n","  # write training data info to a file\n","  with open(file_path + filename + \".pkl\", 'wb') as fp:\n","    pickle.dump(training_stats, fp)\n","    print('Training stats saved successfully to file: ' + filename)\n","  # write node embedding to a file\n","  torch.save(node_embedding, file_path + filename + \"_emb.pt\")\n","  print('Node embedding saved successfully to file: ' + filename)\n","\n","\n","def load_training_info(filename: str):\n","  # load training stats dictionary \n","  with open(file_path + filename + \".pkl\", 'rb') as fp:\n","    train_stats = pickle.load(fp)\n","    print('Training stats successfully loaded from file: ' + filename)\n","  # load node embedding\n","  node_embedding = torch.load(file_path + filename + \"_emb.pt\")\n","  print('Node embedding successfully loaded from file: ' + filename)\n","  return train_stats, node_embedding\n","\n","# Final results is a list [seed, test result, [test per class accuracy], [training per class accuracy], [val per class accuracy]]\n","def save_final_results(final_results: List, filename: str):\n","  # write training data info to a file\n","  with open(file_path + filename + \".pkl\", 'ab') as fp:\n","    pickle.dump(final_results, fp)\n","    print('Final results saved successfully to file: ' + filename)\n","\n","# Returns an iterator which contains all the results from our various runs\n","def load_final_results(filename: str):\n","  with open(file_path + filename + \".pkl\", 'rb') as fp:\n","    print('Final results found in file: ' + filename)\n","    while True:\n","      try:\n","        # This notation creates a generator, which we can then iterate through\n","        yield pickle.load(fp)\n","      except EOFError:\n","        break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZECGPy9JTZrT","outputId":"ad72ff23-9243-4609-d4e7-6646392b8d11"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training stats saved successfully to file: testing\n","Node embedding saved successfully to file: testing\n","Training stats successfully loaded from file: testing\n","Node embedding successfully loaded from file: testing\n","{'c': [1, 2, 3], 'b': [4, 5, 6]} tensor([[ 1., -1.],\n","        [ 1., -1.]])\n"]}],"source":["test_dict = {'c':[1,2,3], 'b':[4,5,6]}\n","test_tensor = torch.tensor([[1., -1.], [1., -1.]])\n","save_training_info(test_dict, test_tensor, \"testing\")\n","recovered_val1, recovered_val2 = load_training_info(\"testing\")\n","print(recovered_val1, recovered_val2)"]},{"cell_type":"markdown","metadata":{"id":"yVyPiw_TBMj7"},"source":["# Model Wrappers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_yBLcOs6V7v"},"outputs":[],"source":["from torch_geometric.nn import GCN\n","\n","class GCNModelWrapper(GCN):\n","\n","  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n","    # use one less layer as our final graph layer can downsize for us\n","    # super().__init__(in_channels, hidden_channels, num_layers-1)\n","    super().__init__(in_channels, hidden_channels, num_layers)\n","    self.out_channels = out_channels\n","    self.final_layer = nn.Linear(hidden_channels, out_channels)\n","\n","  def forward(self, x: torch.Tensor, edge_index: Adj):\n","    x = super().forward(x, edge_index)\n","    output = self.final_layer(x)\n","    return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9xNcjhyBRmX"},"outputs":[],"source":["from torch_geometric.nn import GAT\n","\n","class GATModelWrapper(GAT):\n","\n","  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int, v2: bool):\n","    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n","    super().__init__(in_channels, hidden_channels, num_layers, v2=v2)\n","    self.out_channels = out_channels\n","    self.final_layer = nn.Linear(hidden_channels, out_channels)\n","\n","  def forward(self, x: torch.Tensor, edge_index: Adj):\n","    x = super().forward(x, edge_index)\n","    output = self.final_layer(x)\n","    return output, x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3upq1VfKQAQ"},"outputs":[],"source":["from torch_geometric.nn import GraphSAGE\n","\n","class GraphSAGEModelWrapper(GraphSAGE):\n","\n","  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n","    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n","    super().__init__(in_channels, hidden_channels, num_layers)\n","    self.out_channels = out_channels\n","    self.final_layer = nn.Linear(hidden_channels, out_channels)\n","\n","  def forward(self, x: torch.Tensor, edge_index: Adj):\n","    x = super().forward(x, edge_index)\n","    output = self.final_layer(x)\n","    return output, x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7htrR7C1jc3a"},"outputs":[],"source":["from torch_geometric.nn import Node2Vec\n","from torch import Tensor\n","\n","class Node2VecWrapper(Node2Vec):\n","  def __init__(self, edge_index, embedding_size, walk_length, context_size, walks_per_node, num_negative_samples, p, q, sparse, out_channels):\n","    super().__init__(edge_index, embedding_dim=embedding_size, walk_length=walk_length,\n","                     context_size=context_size, walks_per_node=walks_per_node,\n","                     num_negative_samples=num_negative_samples, p=p, q=q, sparse=sparse)\n","    self.final_layer = nn.Linear(embedding_size, out_channels)\n","  def forward(self):\n","    x = super().forward()\n","    output = F.softmax(self.final_layer(x), dim=1)\n","    return output, x\n","  def test(\n","    self,\n","    train_z: Tensor,\n","    train_y: Tensor,\n","    test_z: Tensor,\n","    test_y: Tensor,\n","    solver: str = 'lbfgs',\n","    multi_class: str = 'auto',\n","    *args,\n","    **kwargs,\n","    ) -> float:\n","    r\"\"\"Evaluates latent space quality via a logistic regression downstream\n","    task.\"\"\"\n","    from sklearn.linear_model import LogisticRegression\n","\n","    clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n","                            **kwargs).fit(train_z.detach().cpu().numpy(),\n","                                          train_y.detach().cpu().numpy())\n","    y_pred = clf.predict(test_z.detach().cpu().numpy())\n","    return y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIrBZ7ssNBzW"},"outputs":[],"source":["from torch_geometric.nn import GIN\n","\n","class GINWrapper(GIN):\n","\n","  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n","    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n","    super().__init__(in_channels, hidden_channels, num_layers)\n","    self.out_channels = out_channels\n","    self.final_layer = nn.Linear(hidden_channels, out_channels)\n","\n","  def forward(self, x: torch.Tensor, edge_index: Adj):\n","    x = super().forward(x, edge_index)\n","    output = self.final_layer(x)\n","    return output, x"]},{"cell_type":"markdown","metadata":{"id":"5mLwBJNywK-9"},"source":["# Training code\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"cellView":"form","id":"-BLISzysQkdA","executionInfo":{"status":"ok","timestamp":1679310249042,"user_tz":0,"elapsed":292,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["# @title [RUN] Hyperparameters GNN\n","\n","NUM_EPOCHS_CORA =  10 #@param {type:\"integer\"}\n","NUM_EPOCHS_ARVIX =  110 #@param {type:\"integer\"}\n","LR         = 0.01 #@param {type:\"number\"}\n","HIDDEN_DIM = 128  #@param {type:\"integer\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFTSH-Vuv4gk"},"outputs":[],"source":["# Code taken from L45 practical notebook\n","def train_gnn(X, edge_indices, y, mask, model, optimiser, device):\n","    model.train()\n","    # Put data on device\n","    X = X.to(device)\n","    edge_indices = edge_indices.to(device)\n","    y = y.to(device)\n","    mask = mask.to(device)\n","    # Train\n","    optimiser.zero_grad()\n","    y_out, _ = model(X, edge_indices)\n","    y_hat = y_out[mask]\n","    loss = F.cross_entropy(y_hat, y)\n","    loss.backward()\n","    optimiser.step()\n","    return loss.data\n","\n","# Training loop using subgraph batching from paper 'Inductive Representation Learning on Large Graphs' https://arxiv.org/pdf/1706.02216.pdf\n","def train_gnn_subgraph(data_batch, model, optimiser, device):\n","  total_loss = 0\n","  for batch in data_batch:\n","    # Put batch in device\n","    batch = batch.to(device)\n","    # Do training loop\n","    batch_size = batch.batch_size\n","    optimiser.zero_grad()\n","    y_out, _ = model(batch.x, batch.edge_index)\n","    y_out = y_out[:batch_size]\n","    batch_y = batch.y[:batch_size]\n","    batch_y = torch.reshape(batch_y, (-1,))\n","    loss = F.cross_entropy(y_out, batch_y)\n","    loss.backward()\n","    optimiser.step()\n","    # Keep a running total of the loss\n","    total_loss += float(loss)\n","\n","  # Get the average loss across all the batches\n","  loss = total_loss / len(data_batch)\n","  return loss\n","\n","def evaluate_gnn(X, edge_indices, y, mask, model, num_classes, device):\n","    model.eval()\n","    # Put data on device\n","    X = X.to(device)\n","    edge_indices = edge_indices.to(device)\n","    y = y.to(device)\n","    mask = mask.to(device)\n","    # Evaluate\n","    with torch.no_grad():\n","      y_out, node_embeddings = model(X, edge_indices)\n","    y_hat = y_out[mask]\n","    y_hat = y_hat.data.max(1)[1]\n","    num_correct = y_hat.eq(y.data).sum()\n","    num_total = len(y)\n","    accuracy = 100.0 * (num_correct/num_total)\n","\n","    # calculate per class accuracy\n","    values, counts = torch.unique(y_hat[y_hat == y.data], return_counts=True)\n","    per_class_counts = torch.zeros(num_classes)\n","    # make sure per_class_counts is on the correct device\n","    per_class_counts = per_class_counts.to(device)\n","    # allocate the number of counts per class\n","    for i, x in enumerate(values):\n","      per_class_counts[x] = counts[i]\n","    # find total number of data points per class in the split\n","    total_per_class = torch.bincount(y.data)\n","    per_class_accuracy = torch.div(per_class_counts, total_per_class)\n","\n","    return accuracy, per_class_accuracy, node_embeddings\n","    \n","# Training loop\n","def train_eval_loop_gnn(model, edge_indices, train_x, train_y, train_mask, valid_x, valid_y, valid_mask, \n","                             test_x, test_y, test_mask, num_classes, seed, filename, device, Cora, subgraph_batches=None):\n","    optimiser = optim.Adam(model.parameters(), lr=LR)\n","    training_stats = None\n","    # Choose number of epochs\n","    NUM_EPOCHS = NUM_EPOCHS_CORA if Cora else NUM_EPOCHS_ARVIX\n","    # Training loop\n","    for epoch in range(NUM_EPOCHS):\n","        # If subgraph batching is not provided, use the full graph for training. Otherwise use subgraph batch training regime\n","        if subgraph_batches is None:\n","          train_loss = train_gnn(train_x, edge_indices, train_y, train_mask, model, optimiser, device)\n","        else:\n","          train_loss = train_gnn_subgraph(subgraph_batches, model, optimiser, device)\n","        # Calculate accuracy on full graph  \n","        train_acc, train_class_acc, _ = evaluate_gnn(train_x, edge_indices, train_y, train_mask, model, num_classes, device)\n","        valid_acc, valid_class_acc, _ = evaluate_gnn(valid_x, edge_indices, valid_y, valid_mask, model, num_classes, device)\n","        if epoch % 10 == 0 or epoch == (NUM_EPOCHS-1):\n","            print(f\"Epoch {epoch} with train loss: {train_loss:.3f} train accuracy: {train_acc:.3f} validation accuracy: {valid_acc:.3f}\")\n","            print(\"Per class train accuracy: \", train_class_acc)\n","            print(\"Per class val accuracy: \", valid_class_acc)\n","        # store the loss and the accuracy for the final plot\n","        epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n","        training_stats = update_stats(training_stats, epoch_stats)\n","\n","    # Lets look at our final test performance\n","    # Only need to get the node embeddings once, take from the training evaluation call\n","    test_acc, test_class_acc, node_embeddings = evaluate_gnn(test_x, edge_indices, test_y, test_mask, model, num_classes, device)\n","    print(f\"Our final test accuracy for the GNN is: {test_acc:.3f}\")\n","    print(\"Final per class accuracy on test set: \", test_class_acc)\n","\n","    # Save training stats if on final iteration of the run\n","    save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n","    # Save final results\n","    final_results_list = [seed, test_acc, test_class_acc, train_class_acc, valid_class_acc]\n","    save_final_results(final_results_list, filename)\n","    # Save final model weights incase we want to do further inference later\n","    torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n","    return training_stats"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"A_O5m_YIaKY-","executionInfo":{"status":"ok","timestamp":1679310254447,"user_tz":0,"elapsed":357,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["def set_seeds(seed):\n","  print(\"SETTING SEEDS TO: \", str(seed))\n","  # seed the potential sources of randomness\n","  torch.manual_seed(seed)\n","  np.random.seed(seed)\n","  random.seed(seed)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"BDvu1pVpKQAR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679310273300,"user_tz":0,"elapsed":620,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"9204f173-8b17-4a51-af79-ec5af6c026b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using Cora dataset\n"]}],"source":["# CHANGE: To name of model being tested\n","filename = \"FastRP\"\n","dataset = \"Cora\"\n","# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n","seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963, 1818027900, 2135956485, 3710910636, 1517964140, 4083009686, 2455059856, 400225693, 89475662, 361232447, 3647665043, 1221215631, 2036056847, 1860537279, 516507873, 3692371949, 3300171104, 2794978777, 3303475786, 2952735006, 572297925]\n","\n","# create folder for saving all model info into if it does not exist already\n","if not os.path.exists(file_path+filename+\"/\"):\n","  os.mkdir(file_path+filename+\"/\")\n","\n","if dataset == \"Cora\":\n","  print(\"Using Cora dataset\")\n","  # Get the edge indices and node features for our model. General set up variables for running with all the models\n","  edge_indices = cora_data.edge_index\n","  node_features = cora_data.x\n","  neighbour_dataset = cora_data\n","\n","  # Get masks and training labels for each split\n","  train_mask = cora_data.train_mask\n","  train_y = cora_data.y[train_mask]\n","  valid_mask = cora_data.val_mask\n","  valid_y = cora_data.y[valid_mask]\n","  test_mask = cora_data.test_mask\n","  test_y = cora_data.y[test_mask]\n","\n","  num_classes = 7\n","  is_cora=True\n","\n","elif dataset==\"Coauthor\":\n","  print(\"Using Coauthor dataset\")\n","  # Get the edge indices and node features for our model. General set up variables for running with all the models\n","  edge_indices = cs_data.edge_index\n","  node_features = cs_data.x\n","  neighbour_dataset = cs_data\n","\n","  # Get masks and training labels for each split\n","  train_mask = train_mask_cs\n","  train_y = cs_data.y[train_mask]\n","  valid_mask = val_mask_cs\n","  valid_y = cs_data.y[valid_mask]\n","  test_mask = test_mask_cs\n","  test_y = cs_data.y[test_mask]\n","\n","  num_classes = 15\n","  is_cora=True\n","\n","# Otherwise we are using arvix dataset\n","else:\n","  print(\"Using Arvix dataset\")\n","  # Get the edge indices and node features for our model\n","  edge_indices = arxiv_data.edge_index\n","  node_features = arxiv_data.x\n","  neighbour_dataset = arxiv_data\n","\n","  # Get masks and training labels for each split\n","  train_mask = train_idx\n","  train_y = arxiv_data.y[train_mask]\n","  valid_mask = valid_idx\n","  valid_y = arxiv_data.y[valid_mask]\n","  test_mask = test_idx\n","  test_y = arxiv_data.y[test_mask]\n","\n","  num_classes = 40\n","  is_cora = False\n"]},{"cell_type":"markdown","metadata":{"id":"fGgiIp_fKQAR"},"source":["# Training Loops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKAFIuc3YlIf"},"outputs":[],"source":["# Use to flush GPU memory if it gets too full\n","import gc\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rl6KVverQy7C"},"outputs":[],"source":["# General training loop for all models except GraphSAGE, using the whole graph in training instead of using subgraph batching\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for seed in seeds:\n","  set_seeds(seed)\n","  # Create the model\n","  model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes, v2=True)\n","  model = model.to(device)\n","\n","  # Run training loop\n","  print(\"TRAINING WITH SEED: \", str(seed))\n","  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n","                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora)\n","  # Print out graphs if not using GPU\n","  if device == torch.device('cpu'):\n","    plot_stats(train_stats_cora, name=filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVevwMOOKQAS"},"outputs":[],"source":["# Training loop for GraphSAGE which using subgraph batches instead of the entire graph\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for seed in seeds:\n","  set_seeds(seed)\n","  # Original paper uses neighbourhood sizes  S1 = 25 and S2 = 10 so this is what we use\n","  train_loader = NeighborLoader(neighbour_dataset, num_neighbors = [25, 10], batch_size=1024, input_nodes=train_mask)\n","\n","  # Create the model\n","  model = GraphSAGEModelWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes)\n","  model = model.to(device)\n","\n","  # Run training loop\n","  print(\"TRAINING WITH SEED: \", str(seed))\n","  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n","                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora, subgraph_batches=train_loader)\n","  # Print out graphs if not using GPU\n","  if device == torch.device('cpu'):\n","    plot_stats(train_stats_cora, name=filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLY-SWcPOjRa"},"outputs":[],"source":["# Training loop for GraphSAGE which using subgraph batches instead of the entire graph\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for seed in seeds:\n","  set_seeds(seed)\n","  # Original paper uses neighbourhood sizes  S1 = 25 and S2 = 10 so this is what we use\n","  train_loader = NeighborLoader(neighbour_dataset, num_neighbors = [25, 10], batch_size=1024, input_nodes=train_mask)\n","\n","  # Create the model\n","  model = GINWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes)\n","  model = model.to(device)\n","\n","  # Run training loop\n","  print(\"TRAINING WITH SEED: \", str(seed))\n","  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n","                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora, subgraph_batches=train_loader)\n","  # Print out graphs if not using GPU\n","  if device == torch.device('cpu'):\n","    plot_stats(train_stats_cora, name=filename)"]},{"cell_type":"markdown","metadata":{"id":"u4GMM4D5dLoB"},"source":["# TESTING LOADING"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35RkMAgK8EHw"},"outputs":[],"source":["final_results = load_final_results(filename)\n","for r in final_results:\n","  print(r)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZ3GQQld9b97"},"outputs":[],"source":["training_stats_1, embedding = load_training_info(filename+\"_1\")\n","plot_stats(training_stats_1, name=\"Testing\")\n","print(embedding)\n","print(node_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1nR7AjndQJn"},"outputs":[],"source":["# Loading stored model weights\n","model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n","model.load_state_dict(torch.load(file_path+filename+\"/\"+\"GATV2_1_model.pt\"))\n","model.eval()"]},{"cell_type":"markdown","metadata":{"id":"GubPzc9IQyP3"},"source":["- Plot graph with average training stats\n","- Save node embeddings for each run\n","- Save training stats for each run\n","- Save test accuracy for each run\n"]},{"cell_type":"markdown","metadata":{"id":"4tdBQvy4rVQq"},"source":["# FastRP"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"IHOX1AH0rVQq","executionInfo":{"status":"ok","timestamp":1679310282861,"user_tz":0,"elapsed":350,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["class FastRPEmbeddingWrapper(nn.Module):\n","  def __init__(self, input_dim, num_classes):\n","      super().__init__()\n","      self.linear = nn.Linear(input_dim, num_classes)\n","\n","  def forward(self, x):\n","      x = self.linear(x)\n","      return x"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"1Vv7X1p6rVQq","executionInfo":{"status":"ok","timestamp":1679310284927,"user_tz":0,"elapsed":286,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["# Copied from https://github.com/GTmac/FastRP/blob/master/fastrp.py\n","# projection method: choose from Gaussian and Sparse\n","# input matrix: choose from adjacency and transition matrix\n","# alpha adjusts the weighting of nodes according to their degree\n","def fastrp_projection(A, q=3, dim=128, projection_method='gaussian', input_matrix='adj', alpha=None):\n","    assert input_matrix == 'adj' or input_matrix == 'trans'\n","    assert projection_method == 'gaussian' or projection_method == 'sparse'\n","    \n","    if input_matrix == 'adj':\n","        M = A\n","    else:\n","        N = A.shape[0]\n","        normalizer = spdiags(np.squeeze(1.0 / csc_matrix.sum(A, axis=1) ), 0, N, N)\n","        M = normalizer @ A\n","    # Gaussian projection matrix\n","    if projection_method == 'gaussian':\n","        transformer = random_projection.GaussianRandomProjection(n_components=dim, random_state=42)\n","    # Sparse projection matrix\n","    else:\n","        transformer = random_projection.SparseRandomProjection(n_components=dim, random_state=42)\n","    Y = transformer.fit(M)\n","    # Random projection for A\n","    if alpha is not None:\n","        Y.components_ = Y.components_ @ spdiags( \\\n","                        np.squeeze(np.power(csc_matrix.sum(A, axis=1), alpha)), 0, N, N)\n","    cur_U = transformer.transform(M)\n","    U_list = [cur_U]\n","    \n","    for i in range(2, q + 1):\n","        cur_U = M @ cur_U\n","        U_list.append(cur_U)\n","    return U_list\n","\n","# When weights is None, concatenate instead of linearly combines the embeddings from different powers of A\n","def fastrp_merge(U_list, weights, normalization=False):\n","    dense_U_list = [_U.todense() for _U in U_list] if type(U_list[0]) == csc_matrix else U_list\n","    _U_list = [normalize(_U, norm='l2', axis=1) for _U in dense_U_list] if normalization else dense_U_list\n","\n","    if weights is None:\n","        return np.concatenate(_U_list, axis=1)\n","    U = np.zeros_like(_U_list[0])\n","    for cur_U, weight in zip(_U_list, weights):\n","        U += cur_U * weight\n","    # U = scale(U.todense())\n","    # U = normalize(U.todense(), norm='l2', axis=1)\n","    return scale(np.asarray(U.todense())) if type(U) == csr_matrix else scale(np.asarray(U))\n","\n","# A is always the adjacency matrix\n","# the choice between adj matrix and trans matrix is decided in the conf\n","def fastrp_wrapper(A, conf):\n","    U_list = fastrp_projection(A,\n","                               q=len(conf['weights']),\n","                               dim=conf['dim'],\n","                               projection_method=conf['projection_method'],\n","                               input_matrix=conf['input_matrix'],\n","                               alpha=conf['alpha'],\n","    )\n","    U = fastrp_merge(U_list, conf['weights'], conf['normalization'])\n","    return U"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"l-Ch32qNrVQq","executionInfo":{"status":"ok","timestamp":1679310288849,"user_tz":0,"elapsed":402,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["# Code adpated from L45 practical notebook\n","def train_embedding_classifier(X, y, mask, model, optimiser, device):\n","    model.train()\n","    # Put data on device\n","    X = X.to(device)\n","    y = y.to(device)\n","    mask = mask.to(device)\n","    # Train\n","    optimiser.zero_grad()\n","    y_out = model(X)\n","    y_hat = y_out[mask]\n","    loss = F.cross_entropy(y_hat, y)\n","    loss.backward()\n","    optimiser.step()\n","    return loss.data\n","\n","def evaluate_embedding_classifier(X, y, mask, model, num_classes, device):\n","    model.eval()\n","    # Put data on device\n","    X = X.to(device)\n","    y = y.to(device)\n","    mask = mask.to(device)\n","    # Evaluate\n","    with torch.no_grad():\n","      y_out = model(X)\n","    y_hat = y_out[mask]\n","    y_hat = y_hat.data.max(1)[1]\n","    num_correct = y_hat.eq(y.data).sum()\n","    num_total = len(y)\n","    accuracy = 100.0 * (num_correct/num_total)\n","\n","    # calculate per class accuracy\n","    values, counts = torch.unique(y_hat[y_hat == y.data], return_counts=True)\n","    per_class_counts = torch.zeros(num_classes)\n","    # make sure per_class_counts is on the correct device\n","    per_class_counts = per_class_counts.to(device)\n","    # allocate the number of counts per class\n","    for i, x in enumerate(values):\n","      per_class_counts[x] = counts[i]\n","    # find total number of data points per class in the split\n","    total_per_class = torch.bincount(y.data)\n","    per_class_accuracy = torch.div(per_class_counts, total_per_class)\n","\n","    return accuracy, per_class_accuracy\n","    \n","# Training loop\n","def train_eval_loop_embedding_classifier(model, embeddings, train_y, train_mask, \n","                                         valid_y, valid_mask, test_y, test_mask, num_classes, seed, filename, device, Cora):\n","    optimiser = optim.Adam(model.parameters(), lr=LR)\n","    training_stats = None\n","    # Choose number of epochs\n","    NUM_EPOCHS = NUM_EPOCHS_CORA if Cora else NUM_EPOCHS_ARVIX\n","    # Training loop\n","    for epoch in range(NUM_EPOCHS):\n","        train_loss = train_embedding_classifier(embeddings, train_y, train_mask, model, optimiser, device)\n","        # Calculate accuracy on full graph  \n","        train_acc, train_class_acc = evaluate_embedding_classifier(embeddings, train_y, train_mask, model, num_classes, device)\n","        valid_acc, valid_class_acc = evaluate_embedding_classifier(embeddings, valid_y, valid_mask, model, num_classes, device)\n","        if epoch % 10 == 0 or epoch == (NUM_EPOCHS-1):\n","            print(f\"Epoch {epoch} with train loss: {train_loss:.3f} train accuracy: {train_acc:.3f} validation accuracy: {valid_acc:.3f}\")\n","            print(\"Per class train accuracy: \", train_class_acc)\n","            print(\"Per class val accuracy: \", valid_class_acc)\n","        # store the loss and the accuracy for the final plot\n","        epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n","        training_stats = update_stats(training_stats, epoch_stats)\n","\n","    # Lets look at our final test performance\n","    # Only need to get the node embeddings once, take from the training evaluation call\n","    test_acc, test_class_acc = evaluate_embedding_classifier(embeddings, test_y, test_mask, model, num_classes, device)\n","    print(f\"Our final test accuracy for the GNN is: {test_acc:.3f}\")\n","    print(\"Final per class accuracy on test set: \", test_class_acc)\n","\n","    # Save training stats if on final iteration of the run, the node embeddings are actually passed in for training, where  \n","    node_embeddings = embeddings\n","    save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n","    # Save final results\n","    final_results_list = [seed, test_acc, test_class_acc, train_class_acc, valid_class_acc]\n","    save_final_results(final_results_list, filename)\n","    # Save final model weights incase we want to do further inference later\n","    torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n","    return training_stats"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76Sx0lNcrVQq","executionInfo":{"status":"ok","timestamp":1679310386035,"user_tz":0,"elapsed":11371,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"a05360c4-786b-4eb3-f7c3-e3bdf98549ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["SETTING SEEDS TO:  4193977854\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["TRAINING WITH SEED:  4193977854\n","Epoch 0 with train loss: 2.115 train accuracy: 34.934 validation accuracy: 31.400\n","Per class train accuracy:  tensor([0.2875, 0.0222, 0.7908, 0.2757, 0.3929, 0.0942, 0.4023],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3115, 0.0000, 0.7821, 0.2848, 0.1975, 0.0877, 0.3793],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.861 train accuracy: 74.089 validation accuracy: 72.200\n","Per class train accuracy:  tensor([0.6625, 0.7667, 0.8724, 0.7243, 0.7653, 0.6014, 0.7931],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7049, 0.8889, 0.8718, 0.6962, 0.6667, 0.6316, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 68.700\n","Final per class accuracy on test set:  tensor([0.6462, 0.7473, 0.9028, 0.6332, 0.6644, 0.5825, 0.6875],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_4193977854\n","Node embedding saved successfully to file: FastRP/FastRP_4193977854\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  1863727779\n","TRAINING WITH SEED:  1863727779\n","Epoch 0 with train loss: 2.144 train accuracy: 33.113 validation accuracy: 28.000\n","Per class train accuracy:  tensor([0.2812, 0.1000, 0.8520, 0.1730, 0.2908, 0.2319, 0.3563],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3443, 0.1111, 0.8205, 0.1392, 0.1605, 0.1404, 0.2759],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.886 train accuracy: 73.593 validation accuracy: 70.200\n","Per class train accuracy:  tensor([0.6750, 0.7667, 0.8980, 0.6833, 0.7704, 0.6449, 0.7241],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6557, 0.8889, 0.8846, 0.6582, 0.6667, 0.5965, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.000\n","Final per class accuracy on test set:  tensor([0.6385, 0.7582, 0.8958, 0.6332, 0.6913, 0.5922, 0.6719],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_1863727779\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Node embedding saved successfully to file: FastRP/FastRP_1863727779\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  170173784\n","TRAINING WITH SEED:  170173784\n","Epoch 0 with train loss: 2.187 train accuracy: 27.980 validation accuracy: 27.400\n","Per class train accuracy:  tensor([0.1750, 0.6111, 0.4490, 0.2434, 0.2041, 0.2391, 0.1264],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.1475, 0.6944, 0.4359, 0.2722, 0.1605, 0.1579, 0.1379],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.885 train accuracy: 74.172 validation accuracy: 71.000\n","Per class train accuracy:  tensor([0.6938, 0.7889, 0.8827, 0.6862, 0.7908, 0.6159, 0.7701],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7049, 0.9167, 0.8718, 0.6392, 0.7037, 0.6316, 0.5862],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 70.400\n","Final per class accuracy on test set:  tensor([0.6538, 0.7582, 0.8958, 0.6332, 0.7181, 0.6408, 0.7188],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_170173784\n","Node embedding saved successfully to file: FastRP/FastRP_170173784\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2342954646\n","TRAINING WITH SEED:  2342954646\n","Epoch 0 with train loss: 2.053 train accuracy: 37.997 validation accuracy: 36.800\n","Per class train accuracy:  tensor([0.2688, 0.7222, 0.7755, 0.2170, 0.2041, 0.3043, 0.4943],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3443, 0.8611, 0.7821, 0.1772, 0.1852, 0.2456, 0.4828],\n","       device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 with train loss: 0.871 train accuracy: 73.924 validation accuracy: 71.000\n","Per class train accuracy:  tensor([0.6938, 0.7778, 0.8929, 0.6891, 0.7500, 0.6449, 0.7586],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7049, 0.8611, 0.8718, 0.6709, 0.6173, 0.6667, 0.6552],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 68.900\n","Final per class accuracy on test set:  tensor([0.6538, 0.7582, 0.8958, 0.6332, 0.6376, 0.6117, 0.7188],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2342954646\n","Node embedding saved successfully to file: FastRP/FastRP_2342954646\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  116846604\n","TRAINING WITH SEED:  116846604\n","Epoch 0 with train loss: 2.230 train accuracy: 32.450 validation accuracy: 30.600\n","Per class train accuracy:  tensor([0.3063, 0.2889, 0.6582, 0.3050, 0.1071, 0.3768, 0.1264],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3607, 0.2500, 0.6282, 0.3354, 0.0741, 0.1930, 0.1034],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.902 train accuracy: 72.930 validation accuracy: 69.600\n","Per class train accuracy:  tensor([0.6562, 0.7667, 0.8878, 0.6950, 0.7398, 0.6449, 0.7126],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6721, 0.8889, 0.8718, 0.6582, 0.6173, 0.5789, 0.6897],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 67.600\n","Final per class accuracy on test set:  tensor([0.6615, 0.7253, 0.8958, 0.6301, 0.6107, 0.6019, 0.6406],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_116846604\n","Node embedding saved successfully to file: FastRP/FastRP_116846604\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2105922959\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["TRAINING WITH SEED:  2105922959\n","Epoch 0 with train loss: 2.099 train accuracy: 33.775 validation accuracy: 32.000\n","Per class train accuracy:  tensor([0.3125, 0.1556, 0.7551, 0.1906, 0.3980, 0.3116, 0.1149],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.1967, 0.0556, 0.8205, 0.1646, 0.4198, 0.3158, 0.1379],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.894 train accuracy: 73.758 validation accuracy: 73.000\n","Per class train accuracy:  tensor([0.6812, 0.7778, 0.8980, 0.6979, 0.7602, 0.6087, 0.7471],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7213, 0.8889, 0.8974, 0.6709, 0.6914, 0.6667, 0.6552],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 70.200\n","Final per class accuracy on test set:  tensor([0.6462, 0.7692, 0.9097, 0.6395, 0.6779, 0.6408, 0.7188],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2105922959\n","Node embedding saved successfully to file: FastRP/FastRP_2105922959\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2739899259\n","TRAINING WITH SEED:  2739899259\n","Epoch 0 with train loss: 2.073 train accuracy: 39.818 validation accuracy: 35.600\n","Per class train accuracy:  tensor([0.2375, 0.6667, 0.8265, 0.2669, 0.3878, 0.1522, 0.3793],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.1803, 0.7500, 0.7692, 0.2722, 0.2222, 0.1754, 0.3103],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.866 train accuracy: 75.083 validation accuracy: 70.000\n","Per class train accuracy:  tensor([0.7063, 0.7778, 0.8827, 0.7390, 0.7755, 0.5942, 0.7471],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6557, 0.9167, 0.8718, 0.6835, 0.5926, 0.6140, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.900\n","Final per class accuracy on test set:  tensor([0.6231, 0.7692, 0.9097, 0.6803, 0.6309, 0.5825, 0.7188],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2739899259\n","Node embedding saved successfully to file: FastRP/FastRP_2739899259\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  1024258131\n","TRAINING WITH SEED:  1024258131\n","Epoch 0 with train loss: 2.163 train accuracy: 28.974 validation accuracy: 26.800\n","Per class train accuracy:  tensor([0.3125, 0.3444, 0.5867, 0.2053, 0.1378, 0.2101, 0.3218],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.2951, 0.3056, 0.6026, 0.1709, 0.1481, 0.1579, 0.3448],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.890 train accuracy: 73.841 validation accuracy: 68.800\n","Per class train accuracy:  tensor([0.6875, 0.7889, 0.8929, 0.6745, 0.7602, 0.6594, 0.7586],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6557, 0.8889, 0.8718, 0.6519, 0.5926, 0.6140, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 68.500\n","Final per class accuracy on test set:  tensor([0.6538, 0.7692, 0.9028, 0.6113, 0.6779, 0.6117, 0.6406],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_1024258131\n","Node embedding saved successfully to file: FastRP/FastRP_1024258131\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  806299656\n","TRAINING WITH SEED:  806299656\n","Epoch 0 with train loss: 1.945 train accuracy: 43.874 validation accuracy: 46.200\n","Per class train accuracy:  tensor([0.5125, 0.7111, 0.8520, 0.2815, 0.3010, 0.1957, 0.4023],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.5246, 0.7778, 0.8462, 0.3418, 0.3333, 0.2105, 0.4138],\n","       device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 with train loss: 0.862 train accuracy: 75.828 validation accuracy: 72.200\n","Per class train accuracy:  tensor([0.7125, 0.7889, 0.8776, 0.7419, 0.7602, 0.6449, 0.7816],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6885, 0.8889, 0.8718, 0.6899, 0.6790, 0.6316, 0.6552],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.900\n","Final per class accuracy on test set:  tensor([0.6538, 0.7363, 0.8958, 0.6552, 0.6980, 0.5922, 0.6875],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_806299656\n","Node embedding saved successfully to file: FastRP/FastRP_806299656\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  880019963\n","TRAINING WITH SEED:  880019963\n","Epoch 0 with train loss: 2.160 train accuracy: 30.795 validation accuracy: 32.000\n","Per class train accuracy:  tensor([0.3250, 0.7778, 0.5765, 0.1818, 0.1276, 0.2536, 0.1724],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3115, 0.8056, 0.5769, 0.2025, 0.1975, 0.2456, 0.1724],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.894 train accuracy: 72.599 validation accuracy: 69.600\n","Per class train accuracy:  tensor([0.6875, 0.7778, 0.8776, 0.6657, 0.7449, 0.6377, 0.7356],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6885, 0.8611, 0.8718, 0.6582, 0.5926, 0.6491, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 68.600\n","Final per class accuracy on test set:  tensor([0.6308, 0.7692, 0.8958, 0.6144, 0.6309, 0.6505, 0.7500],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_880019963\n","Node embedding saved successfully to file: FastRP/FastRP_880019963\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  1818027900\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["TRAINING WITH SEED:  1818027900\n","Epoch 0 with train loss: 2.209 train accuracy: 29.387 validation accuracy: 28.800\n","Per class train accuracy:  tensor([0.1437, 0.5778, 0.5918, 0.1906, 0.2653, 0.1449, 0.3103],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.1639, 0.6667, 0.5769, 0.1835, 0.2099, 0.1930, 0.2759],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.905 train accuracy: 73.013 validation accuracy: 69.400\n","Per class train accuracy:  tensor([0.6750, 0.7889, 0.8827, 0.6598, 0.7653, 0.6594, 0.7356],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6721, 0.8889, 0.8718, 0.6329, 0.6296, 0.6491, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 68.000\n","Final per class accuracy on test set:  tensor([0.6538, 0.7692, 0.8958, 0.6050, 0.6242, 0.6117, 0.7344],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_1818027900\n","Node embedding saved successfully to file: FastRP/FastRP_1818027900\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2135956485\n","TRAINING WITH SEED:  2135956485\n","Epoch 0 with train loss: 2.163 train accuracy: 29.056 validation accuracy: 29.000\n","Per class train accuracy:  tensor([0.5063, 0.2000, 0.3163, 0.1701, 0.4031, 0.2754, 0.1724],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.5410, 0.1667, 0.2949, 0.1709, 0.3457, 0.3333, 0.3103],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.880 train accuracy: 74.172 validation accuracy: 72.600\n","Per class train accuracy:  tensor([0.6812, 0.7889, 0.8878, 0.7038, 0.7653, 0.6449, 0.7241],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7049, 0.8889, 0.8718, 0.6962, 0.6296, 0.6667, 0.7241],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.500\n","Final per class accuracy on test set:  tensor([0.6538, 0.7692, 0.8958, 0.6489, 0.6577, 0.5825, 0.7188],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2135956485\n","Node embedding saved successfully to file: FastRP/FastRP_2135956485\n","Final results saved successfully to file: FastRP/FastRP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["SETTING SEEDS TO:  3710910636\n","TRAINING WITH SEED:  3710910636\n","Epoch 0 with train loss: 2.004 train accuracy: 41.391 validation accuracy: 39.400\n","Per class train accuracy:  tensor([0.1937, 0.5778, 0.8622, 0.4106, 0.2551, 0.2391, 0.2874],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.0656, 0.6944, 0.8333, 0.4114, 0.1975, 0.3158, 0.1379],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.861 train accuracy: 74.338 validation accuracy: 72.000\n","Per class train accuracy:  tensor([0.6938, 0.7667, 0.8878, 0.7155, 0.7602, 0.6087, 0.7701],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6885, 0.8889, 0.8846, 0.6962, 0.6420, 0.6316, 0.6552],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.800\n","Final per class accuracy on test set:  tensor([0.6846, 0.7363, 0.9028, 0.6583, 0.6443, 0.6019, 0.6875],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_3710910636\n","Node embedding saved successfully to file: FastRP/FastRP_3710910636\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  1517964140\n","TRAINING WITH SEED:  1517964140\n","Epoch 0 with train loss: 1.847 train accuracy: 47.599 validation accuracy: 45.800\n","Per class train accuracy:  tensor([0.5875, 0.7111, 0.8469, 0.3460, 0.3367, 0.3696, 0.1839],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.5738, 0.9167, 0.8718, 0.3544, 0.1852, 0.2807, 0.2069],\n","       device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 with train loss: 0.837 train accuracy: 76.407 validation accuracy: 73.200\n","Per class train accuracy:  tensor([0.6938, 0.7667, 0.8929, 0.7302, 0.7959, 0.6812, 0.7931],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7049, 0.8889, 0.8846, 0.7025, 0.6296, 0.7018, 0.6897],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 70.700\n","Final per class accuracy on test set:  tensor([0.6385, 0.7582, 0.8889, 0.6708, 0.6779, 0.6311, 0.7344],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_1517964140\n","Node embedding saved successfully to file: FastRP/FastRP_1517964140\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  4083009686\n","TRAINING WITH SEED:  4083009686\n","Epoch 0 with train loss: 1.991 train accuracy: 43.791 validation accuracy: 42.000\n","Per class train accuracy:  tensor([0.4187, 0.4000, 0.8163, 0.3402, 0.3929, 0.1812, 0.5517],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.4098, 0.3889, 0.7692, 0.3671, 0.3210, 0.2456, 0.4483],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.845 train accuracy: 75.331 validation accuracy: 72.400\n","Per class train accuracy:  tensor([0.6875, 0.7889, 0.8878, 0.7331, 0.7653, 0.6449, 0.7586],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7049, 0.8889, 0.8718, 0.6899, 0.6914, 0.6316, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 72.300\n","Final per class accuracy on test set:  tensor([0.6615, 0.7802, 0.9028, 0.6708, 0.7315, 0.6505, 0.7188],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_4083009686\n","Node embedding saved successfully to file: FastRP/FastRP_4083009686\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2455059856\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["TRAINING WITH SEED:  2455059856\n","Epoch 0 with train loss: 1.974 train accuracy: 44.205 validation accuracy: 43.200\n","Per class train accuracy:  tensor([0.4938, 0.7111, 0.8214, 0.3050, 0.2857, 0.2826, 0.3563],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3607, 0.7778, 0.7821, 0.3418, 0.2346, 0.4211, 0.2759],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.864 train accuracy: 75.000 validation accuracy: 71.200\n","Per class train accuracy:  tensor([0.7000, 0.7778, 0.8827, 0.7097, 0.7602, 0.6739, 0.7701],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6721, 0.9167, 0.8718, 0.6646, 0.6543, 0.6491, 0.6552],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.500\n","Final per class accuracy on test set:  tensor([0.6308, 0.7363, 0.9097, 0.6364, 0.6711, 0.6214, 0.7500],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2455059856\n","Node embedding saved successfully to file: FastRP/FastRP_2455059856\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  400225693\n","TRAINING WITH SEED:  400225693\n","Epoch 0 with train loss: 2.132 train accuracy: 38.659 validation accuracy: 35.800\n","Per class train accuracy:  tensor([0.2500, 0.6667, 0.7092, 0.3402, 0.0969, 0.3333, 0.5402],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3443, 0.8056, 0.6026, 0.2911, 0.1358, 0.2281, 0.4138],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.892 train accuracy: 73.841 validation accuracy: 72.600\n","Per class train accuracy:  tensor([0.7063, 0.7667, 0.8724, 0.6833, 0.7653, 0.6667, 0.7356],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6885, 0.8889, 0.8718, 0.6962, 0.6914, 0.6667, 0.5862],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 68.500\n","Final per class accuracy on test set:  tensor([0.6615, 0.7143, 0.8819, 0.6238, 0.6980, 0.6117, 0.6406],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_400225693\n","Node embedding saved successfully to file: FastRP/FastRP_400225693\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  89475662\n","TRAINING WITH SEED:  89475662\n","Epoch 0 with train loss: 2.092 train accuracy: 35.348 validation accuracy: 35.600\n","Per class train accuracy:  tensor([0.5188, 0.6667, 0.8010, 0.1965, 0.1786, 0.0580, 0.1954],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3934, 0.7222, 0.8077, 0.1962, 0.2716, 0.0351, 0.3448],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.881 train accuracy: 74.586 validation accuracy: 71.400\n","Per class train accuracy:  tensor([0.6750, 0.7889, 0.8827, 0.7067, 0.7857, 0.6377, 0.7586],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7049, 0.8889, 0.8718, 0.6709, 0.6543, 0.6140, 0.6897],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.800\n","Final per class accuracy on test set:  tensor([0.6385, 0.7692, 0.9097, 0.6520, 0.6779, 0.5825, 0.7031],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_89475662\n","Node embedding saved successfully to file: FastRP/FastRP_89475662\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  361232447\n","TRAINING WITH SEED:  361232447\n","Epoch 0 with train loss: 2.053 train accuracy: 41.474 validation accuracy: 38.600\n","Per class train accuracy:  tensor([0.2812, 0.6556, 0.6684, 0.3812, 0.3673, 0.1812, 0.4483],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.2623, 0.6667, 0.7051, 0.3354, 0.2716, 0.1930, 0.4138],\n","       device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 with train loss: 0.866 train accuracy: 74.172 validation accuracy: 72.000\n","Per class train accuracy:  tensor([0.6625, 0.7778, 0.8776, 0.7273, 0.7347, 0.6304, 0.7931],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6885, 0.8611, 0.8718, 0.6899, 0.6914, 0.6140, 0.6552],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.700\n","Final per class accuracy on test set:  tensor([0.6538, 0.7363, 0.9028, 0.6552, 0.6577, 0.5825, 0.7500],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_361232447\n","Node embedding saved successfully to file: FastRP/FastRP_361232447\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  3647665043\n","TRAINING WITH SEED:  3647665043\n","Epoch 0 with train loss: 2.092 train accuracy: 34.354 validation accuracy: 33.800\n","Per class train accuracy:  tensor([0.5562, 0.1222, 0.8214, 0.3255, 0.0612, 0.1377, 0.1379],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.5738, 0.0833, 0.7692, 0.3797, 0.0370, 0.1053, 0.0690],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.884 train accuracy: 73.924 validation accuracy: 70.000\n","Per class train accuracy:  tensor([0.6875, 0.7889, 0.8827, 0.6862, 0.7602, 0.6522, 0.7586],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7377, 0.8611, 0.8846, 0.6709, 0.5802, 0.5965, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.500\n","Final per class accuracy on test set:  tensor([0.6462, 0.7802, 0.8958, 0.6458, 0.6510, 0.6311, 0.6719],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_3647665043\n","Node embedding saved successfully to file: FastRP/FastRP_3647665043\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  1221215631\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["TRAINING WITH SEED:  1221215631\n","Epoch 0 with train loss: 2.190 train accuracy: 32.202 validation accuracy: 32.800\n","Per class train accuracy:  tensor([0.3313, 0.0667, 0.4286, 0.2962, 0.2755, 0.3696, 0.4598],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3770, 0.0278, 0.4359, 0.3544, 0.2593, 0.3684, 0.2759],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.883 train accuracy: 72.848 validation accuracy: 70.600\n","Per class train accuracy:  tensor([0.6687, 0.7778, 0.8622, 0.6862, 0.7194, 0.6449, 0.8046],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6885, 0.8889, 0.8718, 0.6835, 0.5556, 0.6842, 0.6552],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 67.700\n","Final per class accuracy on test set:  tensor([0.6308, 0.7582, 0.8889, 0.6332, 0.5839, 0.5922, 0.7500],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_1221215631\n","Node embedding saved successfully to file: FastRP/FastRP_1221215631\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2036056847\n","TRAINING WITH SEED:  2036056847\n","Epoch 0 with train loss: 2.182 train accuracy: 25.993 validation accuracy: 25.400\n","Per class train accuracy:  tensor([0.4250, 0.3222, 0.0765, 0.2405, 0.3265, 0.2754, 0.2069],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.4098, 0.3056, 0.1538, 0.2152, 0.3086, 0.2632, 0.1724],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.877 train accuracy: 73.841 validation accuracy: 69.600\n","Per class train accuracy:  tensor([0.6750, 0.7778, 0.8827, 0.6979, 0.7653, 0.6522, 0.7241],\n","       device='cuda:0')\n","Per class val accuracy:  "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["tensor([0.6393, 0.8889, 0.8718, 0.6582, 0.6420, 0.6491, 0.5517],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 70.800\n","Final per class accuracy on test set:  tensor([0.6538, 0.7692, 0.9097, 0.6646, 0.6913, 0.6214, 0.6719],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2036056847\n","Node embedding saved successfully to file: FastRP/FastRP_2036056847\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  1860537279\n","TRAINING WITH SEED:  1860537279\n","Epoch 0 with train loss: 2.147 train accuracy: 27.649 validation accuracy: 30.200\n","Per class train accuracy:  tensor([0.1437, 0.3000, 0.6276, 0.2405, 0.2602, 0.0942, 0.1724],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.0820, 0.4444, 0.7179, 0.2089, 0.2716, 0.1228, 0.4138],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.874 train accuracy: 73.179 validation accuracy: 70.000\n","Per class train accuracy:  tensor([0.7125, 0.7667, 0.8776, 0.6891, 0.7551, 0.6159, 0.7011],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6721, 0.8889, 0.8718, 0.6582, 0.6543, 0.5614, 0.6897],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.700\n","Final per class accuracy on test set:  tensor([0.6692, 0.7692, 0.9097, 0.6301, 0.6913, 0.5922, 0.6875],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_1860537279\n","Node embedding saved successfully to file: FastRP/FastRP_1860537279\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  516507873\n","TRAINING WITH SEED:  516507873\n","Epoch 0 with train loss: 1.968 train accuracy: 41.805 validation accuracy: 42.000\n","Per class train accuracy:  tensor([0.2438, 0.6556, 0.8469, 0.2405, 0.2704, 0.3696, 0.6322],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3115, 0.7778, 0.8462, 0.2975, 0.1852, 0.3509, 0.5172],\n","       device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 with train loss: 0.853 train accuracy: 75.248 validation accuracy: 71.400\n","Per class train accuracy:  tensor([0.6812, 0.7889, 0.8827, 0.7185, 0.7704, 0.6884, 0.7471],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6885, 0.9167, 0.8718, 0.6709, 0.6914, 0.5965, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 71.300\n","Final per class accuracy on test set:  tensor([0.6692, 0.7912, 0.9028, 0.6332, 0.7114, 0.6796, 0.7188],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_516507873\n","Node embedding saved successfully to file: FastRP/FastRP_516507873\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  3692371949\n","TRAINING WITH SEED:  3692371949\n","Epoch 0 with train loss: 1.948 train accuracy: 42.301 validation accuracy: 39.200\n","Per class train accuracy:  tensor([0.2438, 0.8000, 0.5408, 0.4194, 0.4388, 0.1812, 0.4598],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.1311, 0.8611, 0.5641, 0.4557, 0.3704, 0.0877, 0.2069],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.847 train accuracy: 75.911 validation accuracy: 71.400\n","Per class train accuracy:  tensor([0.7063, 0.7667, 0.8878, 0.7537, 0.7449, 0.6812, 0.7356],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6885, 0.8889, 0.8718, 0.6899, 0.6543, 0.6140, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 71.000\n","Final per class accuracy on test set:  tensor([0.6846, 0.7582, 0.9028, 0.6646, 0.6779, 0.6117, 0.7188],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_3692371949\n","Node embedding saved successfully to file: FastRP/FastRP_3692371949\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  3300171104\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["TRAINING WITH SEED:  3300171104\n","Epoch 0 with train loss: 2.064 train accuracy: 38.659 validation accuracy: 39.000\n","Per class train accuracy:  tensor([0.2188, 0.5444, 0.8214, 0.4106, 0.1735, 0.2464, 0.1609],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.2295, 0.6667, 0.7564, 0.3987, 0.1852, 0.2807, 0.1379],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.880 train accuracy: 73.841 validation accuracy: 70.600\n","Per class train accuracy:  tensor([0.6750, 0.7778, 0.8878, 0.7097, 0.7245, 0.6594, 0.7471],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6393, 0.8611, 0.8718, 0.6962, 0.6296, 0.6491, 0.5862],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 70.600\n","Final per class accuracy on test set:  tensor([0.6000, 0.7912, 0.8889, 0.6677, 0.6913, 0.6311, 0.7344],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_3300171104\n","Node embedding saved successfully to file: FastRP/FastRP_3300171104\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2794978777\n","TRAINING WITH SEED:  2794978777\n","Epoch 0 with train loss: 2.138 train accuracy: 32.616 validation accuracy: 30.200\n","Per class train accuracy:  tensor([0.1187, 0.5000, 0.7908, 0.2405, 0.1582, 0.2609, 0.2989],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.0656, 0.5556, 0.7564, 0.2468, 0.1358, 0.1930, 0.2414],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.889 train accuracy: 73.344 validation accuracy: 71.400\n","Per class train accuracy:  tensor([0.6875, 0.7667, 0.8827, 0.7155, 0.7347, 0.6087, 0.7126],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6885, 0.8889, 0.8718, 0.6709, 0.6420, 0.6842, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 68.900\n","Final per class accuracy on test set:  tensor([0.6538, 0.7473, 0.8958, 0.6364, 0.6779, 0.6117, 0.6250],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2794978777\n","Node embedding saved successfully to file: FastRP/FastRP_2794978777\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  3303475786\n","TRAINING WITH SEED:  3303475786\n","Epoch 0 with train loss: 2.130 train accuracy: 36.175 validation accuracy: 38.400\n","Per class train accuracy:  tensor([0.3187, 0.7444, 0.5969, 0.3607, 0.1939, 0.2464, 0.0805],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3279, 0.7778, 0.6282, 0.3987, 0.1852, 0.2281, 0.1379],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.872 train accuracy: 75.000 validation accuracy: 71.000\n","Per class train accuracy:  tensor([0.6938, 0.7778, 0.8929, 0.7273, 0.7857, 0.6159, 0.7241],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6393, 0.8889, 0.8590, 0.6899, 0.6914, 0.5965, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 72.300\n","Final per class accuracy on test set:  tensor([0.6538, 0.7692, 0.9028, 0.6928, 0.7047, 0.6408, 0.7188],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_3303475786\n","Node embedding saved successfully to file: FastRP/FastRP_3303475786\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2952735006\n","TRAINING WITH SEED:  2952735006\n","Epoch 0 with train loss: 2.109 train accuracy: 35.927 validation accuracy: 37.800\n","Per class train accuracy:  tensor([0.4313, 0.7333, 0.6633, 0.1584, 0.3061, 0.1014, 0.4713],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.4426, 0.8333, 0.6923, 0.2215, 0.2716, 0.1404, 0.4483],\n","       device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 with train loss: 0.894 train accuracy: 74.007 validation accuracy: 70.200\n","Per class train accuracy:  tensor([0.7125, 0.7778, 0.8827, 0.6774, 0.7908, 0.6449, 0.7126],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6721, 0.8611, 0.8718, 0.6772, 0.6420, 0.5789, 0.6552],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.700\n","Final per class accuracy on test set:  tensor([0.6462, 0.7692, 0.8958, 0.6332, 0.7047, 0.6311, 0.6562],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2952735006\n","Node embedding saved successfully to file: FastRP/FastRP_2952735006\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  572297925\n","TRAINING WITH SEED:  572297925\n","Epoch 0 with train loss: 2.025 train accuracy: 39.652 validation accuracy: 42.200\n","Per class train accuracy:  tensor([0.4875, 0.7222, 0.6020, 0.2991, 0.2500, 0.2246, 0.4138],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.5246, 0.8889, 0.5513, 0.3608, 0.2469, 0.2456, 0.4483],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.879 train accuracy: 74.586 validation accuracy: 70.600\n","Per class train accuracy:  tensor([0.7063, 0.7667, 0.8878, 0.7302, 0.7347, 0.6304, 0.7471],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6885, 0.8611, 0.8718, 0.6772, 0.5926, 0.6667, 0.6552],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 70.400\n","Final per class accuracy on test set:  tensor([0.6692, 0.7473, 0.8958, 0.6708, 0.6577, 0.5922, 0.7344],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_572297925\n","Node embedding saved successfully to file: FastRP/FastRP_572297925\n","Final results saved successfully to file: FastRP/FastRP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]}],"source":["# Use parameters from example in https://github.com/GTmac/FastRP/blob/master/fast-random-projection-blogcatalog.ipynb\n","# Except our input matrix in an adjacency matrix and since we are not tuning alpha we just set this to None\n","conf = {\n","        'projection_method': 'sparse',\n","        'input_matrix': 'adj',\n","        'weights': [0.0, 0.0, 1.0, 6.67],\n","        'normalization': True,\n","        'dim': HIDDEN_DIM,\n","        'alpha': None,\n","        'C': 0.1\n","    }\n","\n","num_nodes = node_features.shape[0]\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for seed in seeds:\n","  set_seeds(seed)\n","  # Convert adjacency matrix to scipy matrix\n","  adj_matrix = to_scipy_sparse_matrix(edge_indices)\n","  embeddings = fastrp_wrapper(adj_matrix, conf)\n","  # convert to tensor \n","  embeddings = torch.from_numpy(embeddings)\n","\n","  # Create the model\n","  model = FastRPEmbeddingWrapper(HIDDEN_DIM, num_classes)\n","  model = model.to(device)\n","\n","  # Run training loop\n","  print(\"TRAINING WITH SEED: \", str(seed))\n","  train_stats_cora = train_eval_loop_embedding_classifier(model, embeddings, train_y, train_mask, \n","                                             valid_y, valid_mask, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora)\n","  # Print out graphs if not using GPU\n","  if device == torch.device('cpu'):\n","    plot_stats(train_stats_cora, name=filename)"]},{"cell_type":"markdown","metadata":{"id":"X5HLu-NNuJ88"},"source":["# Node2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5LtiwsyJPxpt"},"outputs":[],"source":["from torch_geometric.nn import Node2Vec\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","data_name = \"Arxiv\"\n","\n","# Get masks and training labels for each split\n","if data_name == \"Cora\":\n","  num_classes = 7\n","  data = cora_data\n","  # Get the edge indices and node features for our model\n","  edge_indices = data.edge_index\n","  node_features = data.x\n","  # CHANGE: To name of model being tested\n","  filename =  \"Node2Vec_Cora\"\n","  train_mask = data.train_mask\n","  train_y = data.y[train_mask]\n","  valid_mask = data.val_mask\n","  valid_y = data.y[valid_mask]\n","  test_mask = data.test_mask\n","  test_y = data.y[test_mask]\n","elif data_name == \"Coauthor\":\n","  data = cs_data\n","  # Get the edge indices and node features for our model\n","  edge_indices = data.edge_index\n","  node_features = data.x\n","  num_classes = 15\n","  filename =  \"Node2Vec_Coauthor_CS\"\n","  train_mask = train_mask_cs\n","  train_y = data.y[train_mask]\n","  valid_mask = val_mask_cs\n","  valid_y = data.y[valid_mask]\n","  test_mask = test_mask_cs\n","  test_y = data.y[test_mask]\n","elif data_name == \"Arxiv\":\n","  data = arxiv_data\n","  edge_indices = arxiv_data.edge_index\n","  node_features = arxiv_data.x\n","  neighbour_dataset = arxiv_data\n","\n","  # Get masks and training labels for each split\n","  train_mask = train_idx\n","  train_y = arxiv_data.y[train_mask]\n","  valid_mask = valid_idx\n","  valid_y = arxiv_data.y[valid_mask]\n","  test_mask = test_idx\n","  test_y = arxiv_data.y[test_mask]\n","\n","  num_classes = 40\n","  is_cora = False\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n","seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963, 1818027900, 2135956485, 3710910636, 1517964140, 4083009686, 2455059856, 400225693, 89475662, 361232447, 3647665043, 1221215631, 2036056847, 1860537279, 516507873, 3692371949, 3300171104, 2794978777, 3303475786, 2952735006, 572297925]\n","\n","# create folder for saving all model info into if it does not exist already\n","if not os.path.exists(file_path+filename+\"/\"):\n","  os.mkdir(file_path+filename+\"/\")\n","\n","filename = filename + \"/\" + filename\n","\n","for seed in seeds:\n","  set_seeds(seed)\n","  # Create the model\n","  #model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n","  model = Node2VecWrapper(data.edge_index.to(device), embedding_size=128, walk_length=20,\n","                     context_size=10, walks_per_node=10,\n","                     num_negative_samples=1, p=1, q=1, sparse=True, out_channels=num_classes).to(device)\n","  loader = model.loader(batch_size=128, shuffle=True,\n","                      num_workers=0)\n","  optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n","\n","  def train():\n","    model.train()\n","    total_loss = 0\n","    for pos_rw, neg_rw in loader:\n","      optimizer.zero_grad()\n","      loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n","      loss.backward()\n","      optimizer.step()\n","      total_loss += loss.item()\n","    return total_loss / len(loader)\n","\n","  @torch.no_grad()\n","  def find_model_acc(model, train_z, train_y, test_z, test_y, solver: str = 'lbfgs', multi_class: str = 'auto', *args, **kwargs):\n","    pred_y = model.test(train_z, train_y, test_z, test_y, solver=solver, multi_class=multi_class, *args, **kwargs)\n","    acc = accuracy_score(test_y.detach().cpu().numpy(), pred_y)\n","    matrix = confusion_matrix(test_y.detach().cpu().numpy(), pred_y)\n","    per_class_acc = matrix.diagonal()/matrix.sum(axis=1)\n","    #print(m)\n","    #report = classification_report(test_y.detach().cpu().numpy(), pred_y)\n","    #print(report)\n","    return acc, per_class_acc\n","\n","  @torch.no_grad()\n","  def test():\n","    model.eval()\n","    \n","    pred, z = model()\n","    acc_train, per_class_train_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n","                      z[train_mask], data.y[train_mask])\n","  \n","    acc_val, per_class_val_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n","                      z[valid_mask], data.y[valid_mask])\n","\n","    acc_test, per_class_test_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n","                      z[test_mask], data.y[test_mask])\n","\n","    return z, acc_train, per_class_train_acc, acc_val, per_class_val_acc, acc_test, per_class_test_acc\n","\n","  training_stats = None\n","  for epoch in range(0, 10):\n","    loss = train()\n","    node_embeddings, acc_train, per_class_train_acc, acc_val, per_class_val_acc, acc_test, per_class_test_acc = test()\n","    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc_train: {acc_train:.4f}, Acc_val: {acc_val:.4f}, Acc_test: {acc_test:.4f}')\n","    print(f'Per class train accuracy: ', per_class_train_acc)\n","    epoch_stats = {'train_acc': acc_train, 'val_acc': acc_val, 'test_acc': acc_test, 'epoch':epoch}\n","    training_stats = update_stats(training_stats, epoch_stats)\n","  \n","  # Save training stats if on final iteration of the run\n","  save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n","  # Save final results\n","  final_results_list = [seed, acc_test, per_class_test_acc, per_class_train_acc, per_class_val_acc]\n","  save_final_results(final_results_list, filename)\n","  # Save final model weights incase we want to do further inference later\n","  torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n","\n","  plot_stats(training_stats, name=filename)"]},{"cell_type":"markdown","metadata":{"id":"NGtX2ycNQa-H"},"source":["# Similarity tests"]},{"cell_type":"markdown","metadata":{"id":"-eesbQaUQfd4"},"source":["https://github.com/SGDE2020/embedding_stability/blob/master/similarity_tests/similarity_tests.py"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["NGtX2ycNQa-H"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}