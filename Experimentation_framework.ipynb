{"cells":[{"cell_type":"markdown","metadata":{"id":"qU87TNON39IV"},"source":["# **Preliminaries:** Install and import modules"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"Mpygj8TTZ-ur","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679394810477,"user_tz":0,"elapsed":20244,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"5339df34-45a2-4e27-bfc6-8b732a423d95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mycolorpy in /usr/local/lib/python3.9/dist-packages (1.5.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mycolorpy) (1.22.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mycolorpy) (3.7.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (1.4.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (4.39.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (2.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (23.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (3.0.9)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (1.0.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (8.4.0)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mycolorpy) (5.12.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mycolorpy) (3.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mycolorpy) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: colorama in /usr/local/lib/python3.9/dist-packages (0.4.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ogb in /usr/local/lib/python3.9/dist-packages (1.3.5)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.22.4)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.4.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.15.0)\n","Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (0.2.2)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.13.1+cu116)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.26.15)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.2.2)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (4.65.0)\n","Requirement already satisfied: littleutils in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n","Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (63.4.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.9/dist-packages (2.2.0)\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.9/dist-packages (2.1.1+pt113cu116)\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.9/dist-packages (0.6.17+pt113cu116)\n","Requirement already satisfied: torch-cluster in /usr/local/lib/python3.9/dist-packages (1.6.1+pt113cu116)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (5.9.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.27.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (3.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.1.1)\n"]}],"source":["#@title [RUN] install\n","!pip install networkx\n","!pip install mycolorpy\n","!pip install colorama\n","!pip install ogb\n","\n","import torch\n","import os\n","!pip install torch-geometric torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"ZLrrWpkk6xv-","executionInfo":{"status":"ok","timestamp":1679394814090,"user_tz":0,"elapsed":2,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["#@title [RUN] Import modules\n","import numpy as np\n","import seaborn as sns\n","import math\n","import itertools\n","import scipy as sp\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch_geometric\n","from torch_geometric.datasets import Planetoid, Coauthor\n","from torch_scatter import scatter_mean, scatter_max, scatter_sum\n","from torch_geometric.utils import to_dense_adj\n","from torch.nn import Embedding\n","from torch_geometric.typing import Adj\n","from ogb.nodeproppred import PygNodePropPredDataset\n","from torch_geometric.loader import NeighborLoader\n","from torch_geometric.utils import to_scipy_sparse_matrix, degree\n","\n","#For FastRP\n","from scipy.sparse import coo_matrix, csr_matrix, csc_matrix, spdiags\n","from sklearn.preprocessing import normalize, scale, MultiLabelBinarizer\n","from sklearn import random_projection\n","\n","\n","import pdb\n","from datetime import datetime\n","\n","#for nice visualisations\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","\n","from mycolorpy import colorlist as mcp\n","import matplotlib.cm as cm\n","\n","from typing import Mapping, Tuple, Sequence, List\n","import colorama\n","\n","import scipy.linalg\n","from scipy.linalg import block_diag"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"VLrKgQEuwgtb","executionInfo":{"status":"ok","timestamp":1679394817934,"user_tz":0,"elapsed":2,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["####### PLOTS #######\n","\n","def update_stats(training_stats, epoch_stats):\n","    \"\"\" Store metrics along the training\n","    Args:\n","      epoch_stats: dict containg metrics about one epoch\n","      training_stats: dict containing lists of metrics along training\n","    Returns:\n","      updated training_stats\n","    \"\"\"\n","    if training_stats is None:\n","        training_stats = {}\n","        for key in epoch_stats.keys():\n","            training_stats[key] = []\n","    for key,val in epoch_stats.items():\n","        training_stats[key].append(val)\n","    return training_stats\n","\n","def plot_stats(training_stats, figsize=(5, 5), name=\"\"):\n","    \"\"\" Create one plot for each metric stored in training_stats\n","    \"\"\"\n","    stats_names = [key[6:] for key in training_stats.keys() if key.startswith('train_')]\n","    f, ax = plt.subplots(len(stats_names), 1, figsize=figsize)\n","    if len(stats_names)==1:\n","        ax = np.array([ax])\n","    for key, axx in zip(stats_names, ax.reshape(-1,)):\n","        axx.plot(\n","            training_stats['epoch'],\n","            training_stats[f'train_{key}'],\n","            label=f\"Training {key}\")\n","        axx.plot(\n","            training_stats['epoch'],\n","            training_stats[f'val_{key}'],\n","            label=f\"Validation {key}\")\n","        axx.set_xlabel(\"Training epoch\")\n","        axx.set_ylabel(key)\n","        axx.legend()\n","    plt.title(name)\n","\n","\n","def get_color_coded_str(i, color):\n","    return \"\\033[3{}m{}\\033[0m\".format(int(color), int(i))\n","\n","def print_color_numpy(map, list_graphs):\n","    \"\"\" print matrix map in color according to list_graphs\n","    \"\"\"\n","    list_blocks = []\n","    for i,graph in enumerate(list_graphs):\n","        block_i = (i+1)*np.ones((graph.num_nodes,graph.num_nodes))\n","        list_blocks += [block_i]\n","    block_color = block_diag(*list_blocks)\n","    \n","    map_modified = np.vectorize(get_color_coded_str)(map, block_color)\n","    print(\"\\n\".join([\" \".join([\"{}\"]*map.shape[0])]*map.shape[1]).format(*[x for y in map_modified.tolist() for x in y]))"]},{"cell_type":"markdown","metadata":{"id":"82mrcZX0A3QR"},"source":["# Cora dataset\n","\n"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"bBTnJEZWA-Iq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679394822530,"user_tz":0,"elapsed":2,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"b8a25a25-1860-4708-85e5-19224369f813"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"]},"metadata":{},"execution_count":65}],"source":["cora_dataset = Planetoid(\"/tmp/cora\", name=\"cora\", split=\"full\")\n","cora_data = cora_dataset[0]\n","cora_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UuZwDeBwJPZg"},"outputs":[],"source":["print(\"Training class sizes\")\n","print(torch.bincount(cora_dataset[0].y[cora_dataset[0].train_mask]))\n","print(\"Validation class sizes\")\n","print(torch.bincount(cora_dataset[0].y[cora_dataset[0].val_mask]))\n","print(\"Test class sizes\")\n","print(torch.bincount(cora_dataset[0].y[cora_dataset[0].test_mask]))"]},{"cell_type":"markdown","metadata":{"id":"I9AoJuMmKQAO"},"source":["# OBGN-ARVIX dataset"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"Jswepg0KKQAP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679394845289,"user_tz":0,"elapsed":16286,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"70ab8af2-45ae-40ff-e05d-7248cddbc6aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"]},{"output_type":"stream","name":"stderr","text":["Downloaded 0.08 GB: 100%|██████████| 81/81 [00:09<00:00,  8.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting dataset/arxiv.zip\n"]},{"output_type":"stream","name":"stderr","text":["Processing...\n"]},{"output_type":"stream","name":"stdout","text":["Loading necessary files...\n","This might take a while.\n","Processing graphs...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00, 1988.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Converting graphs into PyG objects...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00, 4917.12it/s]"]},{"output_type":"stream","name":"stdout","text":["Saving...\n"]},{"output_type":"stream","name":"stderr","text":["\n","Done!\n"]},{"output_type":"execute_result","data":{"text/plain":["Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343], y=[169343])"]},"metadata":{},"execution_count":66}],"source":["d_name = \"ogbn-arxiv\"\n","\n","dataset = PygNodePropPredDataset(name = d_name)\n","\n","split_idx = dataset.get_idx_split()\n","train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n","arxiv_data = dataset[0]\n","arxiv_data.y = arxiv_data.y.squeeze()\n","arxiv_data.node_year = arxiv_data.node_year.squeeze()\n","arxiv_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xj0Rb5CQKyLB"},"outputs":[],"source":["print(\"Training class sizes\")\n","print(torch.bincount(arxiv_data.y[train_idx]))\n","print(\"Validation class sizes\")\n","print(torch.bincount(arxiv_data.y[valid_idx]))\n","print(\"Test class sizes\")\n","print(torch.bincount(arxiv_data.y[test_idx]))"]},{"cell_type":"markdown","metadata":{"id":"xY5bb2rDEuqM"},"source":["#Coauthor dataset"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"g9ZXzG7SE4oO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679394856690,"user_tz":0,"elapsed":4360,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"ad7b2e79-1a52-476f-d4a3-90f71f49586b"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ms_academic_cs.npz\n","Processing...\n","Done!\n"]},{"output_type":"execute_result","data":{"text/plain":["Data(x=[18333, 6805], edge_index=[2, 163788], y=[18333])"]},"metadata":{},"execution_count":67}],"source":["cs_dataset = Coauthor(\"/tmp/coauthor\", name=\"CS\")\n","cs_data = cs_dataset[0]\n","cs_data"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"KO3BwlveIuNv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679394859724,"user_tz":0,"elapsed":575,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"fa9da774-4bfc-4dc0-8e59-5432fd6592fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["10993 3668 3672\n"]}],"source":["# Create manual split, do 60:20:20 across classes\n","num_classes_cs = 15\n","train_mask_cs_indices = []\n","val_mask_cs_indices = []\n","test_mask_cs_indices = []\n","cs_labels = cs_data.y\n","for i in range(num_classes_cs):\n","\n","  class_i = np.where(cs_labels == i)[0]\n","  np.random.seed(0)\n","  np.random.shuffle(class_i)\n","\n","  num_samples = len(class_i)\n","  train_mask_cs_indices += (class_i[:int(num_samples*0.6)]).tolist() \n","  val_mask_cs_indices += (class_i[int(num_samples*0.6):int(num_samples*0.8)]).tolist() \n","  test_mask_cs_indices += (class_i[int(num_samples*0.8):]).tolist() \n","\n","print(len(train_mask_cs_indices), len(val_mask_cs_indices), len(test_mask_cs_indices))\n","# Create the masks for training\n","# Test mask \n","train_mask_cs = torch.full((len(cs_labels),), False)\n","train_mask_cs[train_mask_cs_indices] = True\n","# Val mask\n","val_mask_cs = torch.full((len(cs_labels),), False)\n","val_mask_cs[val_mask_cs_indices] = True\n","# Train mask\n","test_mask_cs = torch.full((len(cs_labels),), False)\n","test_mask_cs[test_mask_cs_indices] = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lO6lZz9SE6kK"},"outputs":[],"source":["print(\"Training class sizes\")\n","print(torch.bincount(cs_data.y[train_mask_cs]))\n","print(\"Validation class sizes\")\n","print(torch.bincount(cs_data.y[val_mask_cs]))\n","print(\"Test class sizes\")\n","print(torch.bincount(cs_data.y[test_mask_cs]))"]},{"cell_type":"markdown","metadata":{"id":"KL8gKs07J9JP"},"source":["# Data saving / loading"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z80lU1V-Isky","outputId":"7f98ec88-40d6-45ad-de9e-c8004ca78213","executionInfo":{"status":"ok","timestamp":1679394866946,"user_tz":0,"elapsed":3423,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# use google drive for saving and loading information\n","from google.colab import drive\n","import pickle\n","import os\n","\n","drive.mount('/content/drive')\n","file_path = '/content/drive/MyDrive/L45_project/'\n","# create folder if it does not exist already\n","if not os.path.exists(file_path):\n","  os.mkdir(file_path) "]},{"cell_type":"code","execution_count":70,"metadata":{"id":"VhUfVQAZTWHu","executionInfo":{"status":"ok","timestamp":1679394870693,"user_tz":0,"elapsed":489,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["def save_training_info(training_stats: dict, node_embedding: torch.Tensor, filename: str):\n","  # write training data info to a file\n","  with open(file_path + filename + \".pkl\", 'wb') as fp:\n","    pickle.dump(training_stats, fp)\n","    print('Training stats saved successfully to file: ' + filename)\n","  # write node embedding to a file\n","  torch.save(node_embedding, file_path + filename + \"_emb.pt\")\n","  print('Node embedding saved successfully to file: ' + filename)\n","\n","\n","def load_training_info(filename: str):\n","  # load training stats dictionary \n","  with open(file_path + filename + \".pkl\", 'rb') as fp:\n","    train_stats = pickle.load(fp)\n","    print('Training stats successfully loaded from file: ' + filename)\n","  # load node embedding\n","  node_embedding = torch.load(file_path + filename + \"_emb.pt\")\n","  print('Node embedding successfully loaded from file: ' + filename)\n","  return train_stats, node_embedding\n","\n","# Final results is a list [seed, test result, [test per class accuracy], [training per class accuracy], [val per class accuracy]]\n","def save_final_results(final_results: List, filename: str):\n","  # write training data info to a file\n","  with open(file_path + filename + \".pkl\", 'ab') as fp:\n","    pickle.dump(final_results, fp)\n","    print('Final results saved successfully to file: ' + filename)\n","\n","# Returns an iterator which contains all the results from our various runs\n","def load_final_results(filename: str):\n","  with open(file_path + filename + \".pkl\", 'rb') as fp:\n","    print('Final results found in file: ' + filename)\n","    while True:\n","      try:\n","        # This notation creates a generator, which we can then iterate through\n","        yield pickle.load(fp)\n","      except EOFError:\n","        break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZECGPy9JTZrT","outputId":"ad72ff23-9243-4609-d4e7-6646392b8d11"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training stats saved successfully to file: testing\n","Node embedding saved successfully to file: testing\n","Training stats successfully loaded from file: testing\n","Node embedding successfully loaded from file: testing\n","{'c': [1, 2, 3], 'b': [4, 5, 6]} tensor([[ 1., -1.],\n","        [ 1., -1.]])\n"]}],"source":["test_dict = {'c':[1,2,3], 'b':[4,5,6]}\n","test_tensor = torch.tensor([[1., -1.], [1., -1.]])\n","save_training_info(test_dict, test_tensor, \"testing\")\n","recovered_val1, recovered_val2 = load_training_info(\"testing\")\n","print(recovered_val1, recovered_val2)"]},{"cell_type":"markdown","metadata":{"id":"yVyPiw_TBMj7"},"source":["# Model Wrappers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_yBLcOs6V7v"},"outputs":[],"source":["from torch_geometric.nn import GCN\n","\n","class GCNModelWrapper(GCN):\n","\n","  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n","    # use one less layer as our final graph layer can downsize for us\n","    # super().__init__(in_channels, hidden_channels, num_layers-1)\n","    super().__init__(in_channels, hidden_channels, num_layers)\n","    self.out_channels = out_channels\n","    self.final_layer = nn.Linear(hidden_channels, out_channels)\n","\n","  def forward(self, x: torch.Tensor, edge_index: Adj):\n","    x = super().forward(x, edge_index)\n","    output = self.final_layer(x)\n","    return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9xNcjhyBRmX"},"outputs":[],"source":["from torch_geometric.nn import GAT\n","\n","class GATModelWrapper(GAT):\n","\n","  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int, v2: bool):\n","    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n","    super().__init__(in_channels, hidden_channels, num_layers, v2=v2)\n","    self.out_channels = out_channels\n","    self.final_layer = nn.Linear(hidden_channels, out_channels)\n","\n","  def forward(self, x: torch.Tensor, edge_index: Adj):\n","    x = super().forward(x, edge_index)\n","    output = self.final_layer(x)\n","    return output, x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3upq1VfKQAQ"},"outputs":[],"source":["from torch_geometric.nn import GraphSAGE\n","\n","class GraphSAGEModelWrapper(GraphSAGE):\n","\n","  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n","    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n","    super().__init__(in_channels, hidden_channels, num_layers)\n","    self.out_channels = out_channels\n","    self.final_layer = nn.Linear(hidden_channels, out_channels)\n","\n","  def forward(self, x: torch.Tensor, edge_index: Adj):\n","    x = super().forward(x, edge_index)\n","    output = self.final_layer(x)\n","    return output, x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7htrR7C1jc3a"},"outputs":[],"source":["from torch_geometric.nn import Node2Vec\n","from torch import Tensor\n","\n","class Node2VecWrapper(Node2Vec):\n","  def __init__(self, edge_index, embedding_size, walk_length, context_size, walks_per_node, num_negative_samples, p, q, sparse, out_channels):\n","    super().__init__(edge_index, embedding_dim=embedding_size, walk_length=walk_length,\n","                     context_size=context_size, walks_per_node=walks_per_node,\n","                     num_negative_samples=num_negative_samples, p=p, q=q, sparse=sparse)\n","    self.final_layer = nn.Linear(embedding_size, out_channels)\n","  def forward(self):\n","    x = super().forward()\n","    output = F.softmax(self.final_layer(x), dim=1)\n","    return output, x\n","  def test(\n","    self,\n","    train_z: Tensor,\n","    train_y: Tensor,\n","    test_z: Tensor,\n","    test_y: Tensor,\n","    solver: str = 'lbfgs',\n","    multi_class: str = 'auto',\n","    *args,\n","    **kwargs,\n","    ) -> float:\n","    r\"\"\"Evaluates latent space quality via a logistic regression downstream\n","    task.\"\"\"\n","    from sklearn.linear_model import LogisticRegression\n","\n","    clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n","                            **kwargs).fit(train_z.detach().cpu().numpy(),\n","                                          train_y.detach().cpu().numpy())\n","    y_pred = clf.predict(test_z.detach().cpu().numpy())\n","    return y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIrBZ7ssNBzW"},"outputs":[],"source":["from torch_geometric.nn import GIN\n","\n","class GINWrapper(GIN):\n","\n","  def __init__(self, in_channels: int, hidden_channels: int, num_layers: int, out_channels: int):\n","    # Create the model to extract the node embeddings then pass these through a linear layer for classification\n","    super().__init__(in_channels, hidden_channels, num_layers)\n","    self.out_channels = out_channels\n","    self.final_layer = nn.Linear(hidden_channels, out_channels)\n","\n","  def forward(self, x: torch.Tensor, edge_index: Adj):\n","    x = super().forward(x, edge_index)\n","    output = self.final_layer(x)\n","    return output, x"]},{"cell_type":"markdown","metadata":{"id":"5mLwBJNywK-9"},"source":["# Training code\n","\n"]},{"cell_type":"code","execution_count":71,"metadata":{"cellView":"form","id":"-BLISzysQkdA","executionInfo":{"status":"ok","timestamp":1679394877613,"user_tz":0,"elapsed":415,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["# @title [RUN] Hyperparameters GNN\n","\n","NUM_EPOCHS_CORA =  10 #@param {type:\"integer\"}\n","NUM_EPOCHS_ARVIX =  110 #@param {type:\"integer\"}\n","LR         = 0.01 #@param {type:\"number\"}\n","HIDDEN_DIM = 128  #@param {type:\"integer\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFTSH-Vuv4gk"},"outputs":[],"source":["# Code taken from L45 practical notebook\n","def train_gnn(X, edge_indices, y, mask, model, optimiser, device):\n","    model.train()\n","    # Put data on device\n","    X = X.to(device)\n","    edge_indices = edge_indices.to(device)\n","    y = y.to(device)\n","    mask = mask.to(device)\n","    # Train\n","    optimiser.zero_grad()\n","    y_out, _ = model(X, edge_indices)\n","    y_hat = y_out[mask]\n","    loss = F.cross_entropy(y_hat, y)\n","    loss.backward()\n","    optimiser.step()\n","    return loss.data\n","\n","# Training loop using subgraph batching from paper 'Inductive Representation Learning on Large Graphs' https://arxiv.org/pdf/1706.02216.pdf\n","def train_gnn_subgraph(data_batch, model, optimiser, device):\n","  total_loss = 0\n","  for batch in data_batch:\n","    # Put batch in device\n","    batch = batch.to(device)\n","    # Do training loop\n","    batch_size = batch.batch_size\n","    optimiser.zero_grad()\n","    y_out, _ = model(batch.x, batch.edge_index)\n","    y_out = y_out[:batch_size]\n","    batch_y = batch.y[:batch_size]\n","    batch_y = torch.reshape(batch_y, (-1,))\n","    loss = F.cross_entropy(y_out, batch_y)\n","    loss.backward()\n","    optimiser.step()\n","    # Keep a running total of the loss\n","    total_loss += float(loss)\n","\n","  # Get the average loss across all the batches\n","  loss = total_loss / len(data_batch)\n","  return loss\n","\n","def evaluate_gnn(X, edge_indices, y, mask, model, num_classes, device):\n","    model.eval()\n","    # Put data on device\n","    X = X.to(device)\n","    edge_indices = edge_indices.to(device)\n","    y = y.to(device)\n","    mask = mask.to(device)\n","    # Evaluate\n","    with torch.no_grad():\n","      y_out, node_embeddings = model(X, edge_indices)\n","    y_hat = y_out[mask]\n","    y_hat = y_hat.data.max(1)[1]\n","    num_correct = y_hat.eq(y.data).sum()\n","    num_total = len(y)\n","    accuracy = 100.0 * (num_correct/num_total)\n","\n","    # calculate per class accuracy\n","    values, counts = torch.unique(y_hat[y_hat == y.data], return_counts=True)\n","    per_class_counts = torch.zeros(num_classes)\n","    # make sure per_class_counts is on the correct device\n","    per_class_counts = per_class_counts.to(device)\n","    # allocate the number of counts per class\n","    for i, x in enumerate(values):\n","      per_class_counts[x] = counts[i]\n","    # find total number of data points per class in the split\n","    total_per_class = torch.bincount(y.data)\n","    per_class_accuracy = torch.div(per_class_counts, total_per_class)\n","\n","    return accuracy, per_class_accuracy, node_embeddings\n","    \n","# Training loop\n","def train_eval_loop_gnn(model, edge_indices, train_x, train_y, train_mask, valid_x, valid_y, valid_mask, \n","                             test_x, test_y, test_mask, num_classes, seed, filename, device, Cora, subgraph_batches=None):\n","    optimiser = optim.Adam(model.parameters(), lr=LR)\n","    training_stats = None\n","    # Choose number of epochs\n","    NUM_EPOCHS = NUM_EPOCHS_CORA if Cora else NUM_EPOCHS_ARVIX\n","    # Training loop\n","    for epoch in range(NUM_EPOCHS):\n","        # If subgraph batching is not provided, use the full graph for training. Otherwise use subgraph batch training regime\n","        if subgraph_batches is None:\n","          train_loss = train_gnn(train_x, edge_indices, train_y, train_mask, model, optimiser, device)\n","        else:\n","          train_loss = train_gnn_subgraph(subgraph_batches, model, optimiser, device)\n","        # Calculate accuracy on full graph  \n","        train_acc, train_class_acc, _ = evaluate_gnn(train_x, edge_indices, train_y, train_mask, model, num_classes, device)\n","        valid_acc, valid_class_acc, _ = evaluate_gnn(valid_x, edge_indices, valid_y, valid_mask, model, num_classes, device)\n","        if epoch % 10 == 0 or epoch == (NUM_EPOCHS-1):\n","            print(f\"Epoch {epoch} with train loss: {train_loss:.3f} train accuracy: {train_acc:.3f} validation accuracy: {valid_acc:.3f}\")\n","            print(\"Per class train accuracy: \", train_class_acc)\n","            print(\"Per class val accuracy: \", valid_class_acc)\n","        # store the loss and the accuracy for the final plot\n","        epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n","        training_stats = update_stats(training_stats, epoch_stats)\n","\n","    # Lets look at our final test performance\n","    # Only need to get the node embeddings once, take from the training evaluation call\n","    test_acc, test_class_acc, node_embeddings = evaluate_gnn(test_x, edge_indices, test_y, test_mask, model, num_classes, device)\n","    print(f\"Our final test accuracy for the GNN is: {test_acc:.3f}\")\n","    print(\"Final per class accuracy on test set: \", test_class_acc)\n","\n","    # Save training stats if on final iteration of the run\n","    save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n","    # Save final results\n","    final_results_list = [seed, test_acc, test_class_acc, train_class_acc, valid_class_acc]\n","    save_final_results(final_results_list, filename)\n","    # Save final model weights incase we want to do further inference later\n","    torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n","    return training_stats"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"A_O5m_YIaKY-","executionInfo":{"status":"ok","timestamp":1679394881880,"user_tz":0,"elapsed":2,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["def set_seeds(seed):\n","  print(\"SETTING SEEDS TO: \", str(seed))\n","  # seed the potential sources of randomness\n","  torch.manual_seed(seed)\n","  np.random.seed(seed)\n","  random.seed(seed)"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"BDvu1pVpKQAR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679394968517,"user_tz":0,"elapsed":374,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"e1ae5075-4ae6-4948-9fb1-95f5371b3486"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using Coauthor dataset\n"]}],"source":["# CHANGE: To name of model being tested\n","filename = \"FastRP-coauthor\"\n","dataset = \"Coauthor\"\n","# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n","seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963, 1818027900, 2135956485, 3710910636, 1517964140, 4083009686, 2455059856, 400225693, 89475662, 361232447, 3647665043, 1221215631, 2036056847, 1860537279, 516507873, 3692371949, 3300171104, 2794978777, 3303475786, 2952735006, 572297925]\n","\n","# create folder for saving all model info into if it does not exist already\n","if not os.path.exists(file_path+filename+\"/\"):\n","  os.mkdir(file_path+filename+\"/\")\n","\n","if dataset == \"Cora\":\n","  print(\"Using Cora dataset\")\n","  # Get the edge indices and node features for our model. General set up variables for running with all the models\n","  edge_indices = cora_data.edge_index\n","  node_features = cora_data.x\n","  neighbour_dataset = cora_data\n","\n","  # Get masks and training labels for each split\n","  train_mask = cora_data.train_mask\n","  train_y = cora_data.y[train_mask]\n","  valid_mask = cora_data.val_mask\n","  valid_y = cora_data.y[valid_mask]\n","  test_mask = cora_data.test_mask\n","  test_y = cora_data.y[test_mask]\n","\n","  num_classes = 7\n","  is_cora=True\n","\n","elif dataset==\"Coauthor\":\n","  print(\"Using Coauthor dataset\")\n","  # Get the edge indices and node features for our model. General set up variables for running with all the models\n","  edge_indices = cs_data.edge_index\n","  node_features = cs_data.x\n","  neighbour_dataset = cs_data\n","\n","  # Get masks and training labels for each split\n","  train_mask = train_mask_cs\n","  train_y = cs_data.y[train_mask]\n","  valid_mask = val_mask_cs\n","  valid_y = cs_data.y[valid_mask]\n","  test_mask = test_mask_cs\n","  test_y = cs_data.y[test_mask]\n","\n","  num_classes = 15\n","  is_cora=True\n","\n","# Otherwise we are using arvix dataset\n","else:\n","  print(\"Using Arvix dataset\")\n","  # Get the edge indices and node features for our model\n","  edge_indices = arxiv_data.edge_index\n","  node_features = arxiv_data.x\n","  neighbour_dataset = arxiv_data\n","\n","  # Get masks and training labels for each split\n","  train_mask = train_idx\n","  train_y = arxiv_data.y[train_mask]\n","  valid_mask = valid_idx\n","  valid_y = arxiv_data.y[valid_mask]\n","  test_mask = test_idx\n","  test_y = arxiv_data.y[test_mask]\n","\n","  num_classes = 40\n","  is_cora = False\n"]},{"cell_type":"markdown","metadata":{"id":"fGgiIp_fKQAR"},"source":["# Training Loops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKAFIuc3YlIf"},"outputs":[],"source":["# Use to flush GPU memory if it gets too full\n","import gc\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rl6KVverQy7C"},"outputs":[],"source":["# General training loop for all models except GraphSAGE, using the whole graph in training instead of using subgraph batching\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for seed in seeds:\n","  set_seeds(seed)\n","  # Create the model\n","  model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes, v2=True)\n","  model = model.to(device)\n","\n","  # Run training loop\n","  print(\"TRAINING WITH SEED: \", str(seed))\n","  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n","                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora)\n","  # Print out graphs if not using GPU\n","  if device == torch.device('cpu'):\n","    plot_stats(train_stats_cora, name=filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVevwMOOKQAS"},"outputs":[],"source":["# Training loop for GraphSAGE which using subgraph batches instead of the entire graph\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for seed in seeds:\n","  set_seeds(seed)\n","  # Original paper uses neighbourhood sizes  S1 = 25 and S2 = 10 so this is what we use\n","  train_loader = NeighborLoader(neighbour_dataset, num_neighbors = [25, 10], batch_size=1024, input_nodes=train_mask)\n","\n","  # Create the model\n","  model = GraphSAGEModelWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes)\n","  model = model.to(device)\n","\n","  # Run training loop\n","  print(\"TRAINING WITH SEED: \", str(seed))\n","  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n","                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora, subgraph_batches=train_loader)\n","  # Print out graphs if not using GPU\n","  if device == torch.device('cpu'):\n","    plot_stats(train_stats_cora, name=filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLY-SWcPOjRa"},"outputs":[],"source":["# Training loop for GraphSAGE which using subgraph batches instead of the entire graph\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for seed in seeds:\n","  set_seeds(seed)\n","  # Original paper uses neighbourhood sizes  S1 = 25 and S2 = 10 so this is what we use\n","  train_loader = NeighborLoader(neighbour_dataset, num_neighbors = [25, 10], batch_size=1024, input_nodes=train_mask)\n","\n","  # Create the model\n","  model = GINWrapper(in_channels = node_features.shape[-1], hidden_channels = HIDDEN_DIM, num_layers=1, out_channels=num_classes)\n","  model = model.to(device)\n","\n","  # Run training loop\n","  print(\"TRAINING WITH SEED: \", str(seed))\n","  train_stats_cora = train_eval_loop_gnn(model, edge_indices, node_features, train_y, train_mask, \n","                                            node_features, valid_y, valid_mask, node_features, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora, subgraph_batches=train_loader)\n","  # Print out graphs if not using GPU\n","  if device == torch.device('cpu'):\n","    plot_stats(train_stats_cora, name=filename)"]},{"cell_type":"markdown","metadata":{"id":"u4GMM4D5dLoB"},"source":["# TESTING LOADING"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35RkMAgK8EHw"},"outputs":[],"source":["final_results = load_final_results(filename)\n","for r in final_results:\n","  print(r)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZ3GQQld9b97"},"outputs":[],"source":["training_stats_1, embedding = load_training_info(filename+\"_1\")\n","plot_stats(training_stats_1, name=\"Testing\")\n","print(embedding)\n","print(node_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1nR7AjndQJn"},"outputs":[],"source":["# Loading stored model weights\n","model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n","model.load_state_dict(torch.load(file_path+filename+\"/\"+\"GATV2_1_model.pt\"))\n","model.eval()"]},{"cell_type":"markdown","metadata":{"id":"GubPzc9IQyP3"},"source":["- Plot graph with average training stats\n","- Save node embeddings for each run\n","- Save training stats for each run\n","- Save test accuracy for each run\n"]},{"cell_type":"markdown","metadata":{"id":"dneCfd9SkayN"},"source":["# FastRP"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"K69XCQpzkayN","executionInfo":{"status":"ok","timestamp":1679394974640,"user_tz":0,"elapsed":396,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["class FastRPEmbeddingWrapper(nn.Module):\n","  def __init__(self, input_dim, num_classes):\n","      super().__init__()\n","      self.linear = nn.Linear(input_dim, num_classes)\n","\n","  def forward(self, x):\n","      x = self.linear(x)\n","      return x"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"c7mdOE9EkayN","executionInfo":{"status":"ok","timestamp":1679394904272,"user_tz":0,"elapsed":5,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["# Copied from https://github.com/GTmac/FastRP/blob/master/fastrp.py\n","# projection method: choose from Gaussian and Sparse\n","# input matrix: choose from adjacency and transition matrix\n","# alpha adjusts the weighting of nodes according to their degree\n","def fastrp_projection(A, seed, q=3, dim=128, projection_method='gaussian', input_matrix='adj', alpha=None):\n","    assert input_matrix == 'adj' or input_matrix == 'trans'\n","    assert projection_method == 'gaussian' or projection_method == 'sparse'\n","    \n","    N = A.shape[0]\n","    if input_matrix == 'adj':\n","        M = A\n","    else:\n","        # Change csc_matrix.sum(A) to A.sum as this caused bugs\n","        normalizer = spdiags(np.squeeze(1.0 / A.sum(axis=1) ), 0, N, N)\n","        M = normalizer @ A\n","    # Gaussian projection matrix\n","    if projection_method == 'gaussian':\n","        transformer = random_projection.GaussianRandomProjection(n_components=dim, random_state=seed)\n","    # Sparse projection matrix\n","    else:\n","        transformer = random_projection.SparseRandomProjection(n_components=dim, random_state=seed)\n","    Y = transformer.fit(M)\n","    # Random projection for A\n","    if alpha is not None:\n","      # Change csc_matrix.sum(A) to A.sum as this caused bugs\n","        Y.components_ = Y.components_ @ spdiags( \\\n","                        np.squeeze(np.power(A.sum(axis=1), alpha)), 0, N, N)\n","    cur_U = transformer.transform(M)\n","    U_list = [cur_U]\n","    \n","    for i in range(2, q + 1):\n","        cur_U = M @ cur_U\n","        U_list.append(cur_U)\n","    return U_list\n","\n","# When weights is None, concatenate instead of linearly combines the embeddings from different powers of A\n","def fastrp_merge(U_list, weights, normalization=False):\n","    dense_U_list = [_U.todense() for _U in U_list] if type(U_list[0]) == csc_matrix else U_list\n","    _U_list = [normalize(_U, norm='l2', axis=1) for _U in dense_U_list] if normalization else dense_U_list\n","\n","    if weights is None:\n","        return np.concatenate(_U_list, axis=1)\n","    U = np.zeros_like(_U_list[0])\n","    for cur_U, weight in zip(_U_list, weights):\n","        U += cur_U * weight\n","    # U = scale(U.todense())\n","    # U = normalize(U.todense(), norm='l2', axis=1)\n","    return scale(np.asarray(U.todense())) if type(U) == csr_matrix else scale(np.asarray(U))\n","\n","# A is always the adjacency matrix\n","# the choice between adj matrix and trans matrix is decided in the conf\n","def fastrp_wrapper(A, conf, seed):\n","    U_list = fastrp_projection(A,\n","                               seed,\n","                               q=len(conf['weights']),\n","                               dim=conf['dim'],\n","                               projection_method=conf['projection_method'],\n","                               input_matrix=conf['input_matrix'],\n","                               alpha=conf['alpha'],\n","    )\n","    U = fastrp_merge(U_list, conf['weights'], conf['normalization'])\n","    return U"]},{"cell_type":"code","execution_count":76,"metadata":{"id":"7wPmS2Q3kayN","executionInfo":{"status":"ok","timestamp":1679394909216,"user_tz":0,"elapsed":409,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}}},"outputs":[],"source":["# Code adpated from L45 practical notebook\n","def train_embedding_classifier(X, y, mask, model, optimiser, device):\n","    model.train()\n","    # Put data on device\n","    X = X.to(device)\n","    y = y.to(device)\n","    mask = mask.to(device)\n","    # Train\n","    optimiser.zero_grad()\n","    y_out = model(X)\n","    y_hat = y_out[mask]\n","    loss = F.cross_entropy(y_hat, y)\n","    loss.backward()\n","    optimiser.step()\n","    return loss.data\n","\n","def evaluate_embedding_classifier(X, y, mask, model, num_classes, device):\n","    model.eval()\n","    # Put data on device\n","    X = X.to(device)\n","    y = y.to(device)\n","    mask = mask.to(device)\n","    # Evaluate\n","    with torch.no_grad():\n","      y_out = model(X)\n","    y_hat = y_out[mask]\n","    y_hat = y_hat.data.max(1)[1]\n","    num_correct = y_hat.eq(y.data).sum()\n","    num_total = len(y)\n","    accuracy = 100.0 * (num_correct/num_total)\n","\n","    # calculate per class accuracy\n","    values, counts = torch.unique(y_hat[y_hat == y.data], return_counts=True)\n","    per_class_counts = torch.zeros(num_classes)\n","    # make sure per_class_counts is on the correct device\n","    per_class_counts = per_class_counts.to(device)\n","    # allocate the number of counts per class\n","    for i, x in enumerate(values):\n","      per_class_counts[x] = counts[i]\n","    # find total number of data points per class in the split\n","    total_per_class = torch.bincount(y.data)\n","    per_class_accuracy = torch.div(per_class_counts, total_per_class)\n","\n","    return accuracy, per_class_accuracy\n","    \n","# Training loop\n","def train_eval_loop_embedding_classifier(model, embeddings, train_y, train_mask, \n","                                         valid_y, valid_mask, test_y, test_mask, num_classes, seed, filename, device, Cora):\n","    optimiser = optim.Adam(model.parameters(), lr=LR)\n","    training_stats = None\n","    # Choose number of epochs\n","    NUM_EPOCHS = NUM_EPOCHS_CORA if Cora else NUM_EPOCHS_ARVIX\n","    # Training loop\n","    for epoch in range(NUM_EPOCHS):\n","        train_loss = train_embedding_classifier(embeddings, train_y, train_mask, model, optimiser, device)\n","        # Calculate accuracy on full graph  \n","        train_acc, train_class_acc = evaluate_embedding_classifier(embeddings, train_y, train_mask, model, num_classes, device)\n","        valid_acc, valid_class_acc = evaluate_embedding_classifier(embeddings, valid_y, valid_mask, model, num_classes, device)\n","        if epoch % 10 == 0 or epoch == (NUM_EPOCHS-1):\n","            print(f\"Epoch {epoch} with train loss: {train_loss:.3f} train accuracy: {train_acc:.3f} validation accuracy: {valid_acc:.3f}\")\n","            print(\"Per class train accuracy: \", train_class_acc)\n","            print(\"Per class val accuracy: \", valid_class_acc)\n","        # store the loss and the accuracy for the final plot\n","        epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n","        training_stats = update_stats(training_stats, epoch_stats)\n","\n","    # Lets look at our final test performance\n","    # Only need to get the node embeddings once, take from the training evaluation call\n","    test_acc, test_class_acc = evaluate_embedding_classifier(embeddings, test_y, test_mask, model, num_classes, device)\n","    print(f\"Our final test accuracy for the GNN is: {test_acc:.3f}\")\n","    print(\"Final per class accuracy on test set: \", test_class_acc)\n","\n","    # Save training stats if on final iteration of the run, the node embeddings are actually passed in for training, where  \n","    node_embeddings = embeddings\n","    save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n","    # Save final results\n","    final_results_list = [seed, test_acc, test_class_acc, train_class_acc, valid_class_acc]\n","    save_final_results(final_results_list, filename)\n","    # Save final model weights incase we want to do further inference later\n","    torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n","    return training_stats"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"GOvJm2pnkayN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679394919569,"user_tz":0,"elapsed":4326,"user":{"displayName":"Emily Morris","userId":"08202831375881547702"}},"outputId":"0ca74084-cb19-4957-b250-b50e6a12544c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using adjacency matrix\n","SETTING SEEDS TO:  4193977854\n","TRAINING WITH SEED:  4193977854\n","Epoch 0 with train loss: 1.941 train accuracy: 40.811 validation accuracy: 38.600\n","Per class train accuracy:  tensor([0.3250, 0.7111, 0.7755, 0.2933, 0.1990, 0.3333, 0.4598],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.2787, 0.8333, 0.7179, 0.2658, 0.2963, 0.3333, 0.1724],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.859 train accuracy: 74.338 validation accuracy: 70.800\n","Per class train accuracy:  tensor([0.6500, 0.7556, 0.8980, 0.7126, 0.7296, 0.6739, 0.8161],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6721, 0.8056, 0.8590, 0.7278, 0.6667, 0.5789, 0.5172],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.800\n","Final per class accuracy on test set:  tensor([0.6462, 0.7033, 0.8750, 0.6771, 0.6309, 0.6990, 0.6562],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_4193977854\n","Node embedding saved successfully to file: FastRP/FastRP_4193977854\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  1863727779\n","TRAINING WITH SEED:  1863727779\n","Epoch 0 with train loss: 2.013 train accuracy: 35.265 validation accuracy: 33.600\n","Per class train accuracy:  tensor([0.1625, 0.2889, 0.8520, 0.1994, 0.3827, 0.2971, 0.2644],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.1803, 0.4167, 0.8462, 0.1519, 0.3210, 0.2632, 0.3793],\n","       device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 with train loss: 0.872 train accuracy: 74.917 validation accuracy: 69.200\n","Per class train accuracy:  tensor([0.6375, 0.7667, 0.9031, 0.7243, 0.7704, 0.6594, 0.7816],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6885, 0.8611, 0.8846, 0.6582, 0.6296, 0.5614, 0.5862],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 67.600\n","Final per class accuracy on test set:  tensor([0.5923, 0.7033, 0.8681, 0.6238, 0.6510, 0.6505, 0.7344],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_1863727779\n","Node embedding saved successfully to file: FastRP/FastRP_1863727779\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  170173784\n","TRAINING WITH SEED:  170173784\n","Epoch 0 with train loss: 2.056 train accuracy: 41.887 validation accuracy: 41.000\n","Per class train accuracy:  tensor([0.2812, 0.7333, 0.7347, 0.2463, 0.4847, 0.2391, 0.4483],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3934, 0.8333, 0.7179, 0.2785, 0.3827, 0.1754, 0.3448],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.869 train accuracy: 74.586 validation accuracy: 70.000\n","Per class train accuracy:  tensor([0.7125, 0.7889, 0.8980, 0.7537, 0.7143, 0.6087, 0.6782],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6885, 0.8611, 0.8590, 0.7089, 0.6543, 0.5614, 0.4483],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.100\n","Final per class accuracy on test set:  tensor([0.6923, 0.7582, 0.8750, 0.6740, 0.6242, 0.5825, 0.5938],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_170173784\n","Node embedding saved successfully to file: FastRP/FastRP_170173784\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2342954646\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["TRAINING WITH SEED:  2342954646\n","Epoch 0 with train loss: 1.996 train accuracy: 41.142 validation accuracy: 43.600\n","Per class train accuracy:  tensor([0.1312, 0.6556, 0.8010, 0.3343, 0.3061, 0.4058, 0.3448],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.0984, 0.7778, 0.7564, 0.3354, 0.4074, 0.5439, 0.2759],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.892 train accuracy: 74.007 validation accuracy: 72.800\n","Per class train accuracy:  tensor([0.6875, 0.7333, 0.8776, 0.6862, 0.7704, 0.7101, 0.7241],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7049, 0.8056, 0.8590, 0.7405, 0.6420, 0.7193, 0.5172],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 70.700\n","Final per class accuracy on test set:  tensor([0.6692, 0.7253, 0.8819, 0.6520, 0.6980, 0.7087, 0.6562],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2342954646\n","Node embedding saved successfully to file: FastRP/FastRP_2342954646\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  116846604\n","TRAINING WITH SEED:  116846604\n","Epoch 0 with train loss: 2.319 train accuracy: 17.053 validation accuracy: 18.400\n","Per class train accuracy:  tensor([0.3812, 0.5889, 0.0867, 0.0733, 0.0867, 0.1232, 0.1839],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3115, 0.6667, 0.1538, 0.0696, 0.1481, 0.1228, 0.2414],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.897 train accuracy: 73.096 validation accuracy: 71.600\n","Per class train accuracy:  tensor([0.7000, 0.7444, 0.8827, 0.6804, 0.7194, 0.6522, 0.7816],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7213, 0.7778, 0.8462, 0.7089, 0.6420, 0.6491, 0.6552],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.000\n","Final per class accuracy on test set:  tensor([0.7000, 0.7033, 0.8681, 0.6426, 0.6174, 0.6699, 0.6875],\n","       device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training stats saved successfully to file: FastRP/FastRP_116846604\n","Node embedding saved successfully to file: FastRP/FastRP_116846604\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2105922959\n","TRAINING WITH SEED:  2105922959\n","Epoch 0 with train loss: 2.162 train accuracy: 33.775 validation accuracy: 31.600\n","Per class train accuracy:  tensor([0.1937, 0.7556, 0.7143, 0.2434, 0.1582, 0.1667, 0.3678],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.2295, 0.7778, 0.6923, 0.2025, 0.0864, 0.2632, 0.2759],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.911 train accuracy: 73.096 validation accuracy: 70.400\n","Per class train accuracy:  tensor([0.6750, 0.7333, 0.9031, 0.6921, 0.7806, 0.6087, 0.6782],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6885, 0.7778, 0.8590, 0.6899, 0.6914, 0.5965, 0.5517],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 70.400\n","Final per class accuracy on test set:  tensor([0.6077, 0.7473, 0.8889, 0.6395, 0.7852, 0.6311, 0.6719],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2105922959\n","Node embedding saved successfully to file: FastRP/FastRP_2105922959\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2739899259\n","TRAINING WITH SEED:  2739899259\n","Epoch 0 with train loss: 2.125 train accuracy: 35.679 validation accuracy: 34.400\n","Per class train accuracy:  tensor([0.1750, 0.6111, 0.6786, 0.2757, 0.2908, 0.1957, 0.4253],\n","       device='cuda:0')\n","Per class val accuracy:  "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["tensor([0.1311, 0.6667, 0.6795, 0.3354, 0.1358, 0.1579, 0.4828],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.869 train accuracy: 75.414 validation accuracy: 71.400\n","Per class train accuracy:  tensor([0.7000, 0.7556, 0.8827, 0.7243, 0.7551, 0.6739, 0.8046],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6721, 0.7778, 0.8590, 0.7278, 0.6790, 0.5614, 0.6552],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.700\n","Final per class accuracy on test set:  tensor([0.6615, 0.7143, 0.8889, 0.6426, 0.6510, 0.6602, 0.7500],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2739899259\n","Node embedding saved successfully to file: FastRP/FastRP_2739899259\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  1024258131\n","TRAINING WITH SEED:  1024258131\n","Epoch 0 with train loss: 1.976 train accuracy: 41.060 validation accuracy: 42.800\n","Per class train accuracy:  tensor([0.4563, 0.7111, 0.7806, 0.4018, 0.1939, 0.1014, 0.1954],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.5246, 0.7222, 0.7821, 0.4114, 0.2469, 0.0702, 0.2069],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.923 train accuracy: 72.765 validation accuracy: 70.400\n","Per class train accuracy:  tensor([0.6500, 0.8111, 0.8827, 0.7243, 0.7041, 0.5580, 0.7701],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7049, 0.8611, 0.8590, 0.7215, 0.5926, 0.5439, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 65.800\n","Final per class accuracy on test set:  tensor([0.6231, 0.7253, 0.8750, 0.6270, 0.5570, 0.5728, 0.6719],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_1024258131\n","Node embedding saved successfully to file: FastRP/FastRP_1024258131\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  806299656\n","TRAINING WITH SEED:  806299656\n","Epoch 0 with train loss: 2.022 train accuracy: 39.818 validation accuracy: 38.400\n","Per class train accuracy:  tensor([0.2500, 0.7222, 0.7857, 0.2551, 0.3265, 0.2391, 0.4368],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.2951, 0.8056, 0.7179, 0.2848, 0.2716, 0.1579, 0.4483],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.918 train accuracy: 72.103 validation accuracy: 67.600\n","Per class train accuracy:  tensor([0.5688, 0.7333, 0.8878, 0.6950, 0.6837, 0.7246, 0.7931],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6066, 0.8333, 0.8462, 0.6519, 0.6420, 0.5614, 0.6207],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 67.600\n","Final per class accuracy on test set:  tensor([0.5615, 0.7033, 0.8681, 0.6395, 0.6376, 0.6893, 0.6875],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_806299656\n","Node embedding saved successfully to file: FastRP/FastRP_806299656\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  880019963\n","TRAINING WITH SEED:  880019963\n","Epoch 0 with train loss: 2.089 train accuracy: 35.596 validation accuracy: 31.800\n","Per class train accuracy:  tensor([0.1937, 0.2889, 0.8520, 0.2170, 0.2551, 0.3913, 0.3218],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.1311, 0.3056, 0.8333, 0.2152, 0.1852, 0.4035, 0.1034],\n","       device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 with train loss: 0.896 train accuracy: 72.682 validation accuracy: 65.400\n","Per class train accuracy:  tensor([0.6438, 0.7333, 0.8929, 0.6510, 0.7908, 0.6594, 0.7586],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6557, 0.8056, 0.8590, 0.5759, 0.6790, 0.5263, 0.5172],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 67.400\n","Final per class accuracy on test set:  tensor([0.6385, 0.7363, 0.8819, 0.5423, 0.7383, 0.6893, 0.6719],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_880019963\n","Node embedding saved successfully to file: FastRP/FastRP_880019963\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  1818027900\n","TRAINING WITH SEED:  1818027900\n","Epoch 0 with train loss: 2.196 train accuracy: 31.788 validation accuracy: 30.800\n","Per class train accuracy:  tensor([0.2188, 0.6222, 0.7500, 0.2434, 0.1122, 0.2101, 0.1379],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.1967, 0.7500, 0.7179, 0.2215, 0.1481, 0.1754, 0.0690],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.918 train accuracy: 73.675 validation accuracy: 70.600\n","Per class train accuracy:  tensor([0.6000, 0.7889, 0.9031, 0.6745, 0.7449, 0.7246, 0.8046],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6721, 0.8333, 0.8846, 0.6329, 0.6667, 0.6842, 0.6897],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 67.500\n","Final per class accuracy on test set:  tensor([0.6385, 0.7692, 0.8889, 0.5893, 0.5973, 0.6990, 0.7031],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_1818027900\n","Node embedding saved successfully to file: FastRP/FastRP_1818027900\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2135956485\n","TRAINING WITH SEED:  2135956485\n","Epoch 0 with train loss: 2.015 train accuracy: 37.500 validation accuracy: 37.800\n","Per class train accuracy:  tensor([0.2688, 0.0333, 0.7398, 0.4018, 0.3163, 0.2319, 0.3563],\n","       device='cuda:0')\n","Per class val accuracy:  "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["tensor([0.3279, 0.0278, 0.6923, 0.4557, 0.1975, 0.3158, 0.2759],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.872 train accuracy: 73.427 validation accuracy: 71.000\n","Per class train accuracy:  tensor([0.6812, 0.7556, 0.8878, 0.6950, 0.7398, 0.6957, 0.6667],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6721, 0.8889, 0.8462, 0.6772, 0.6914, 0.6842, 0.4828],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 70.200\n","Final per class accuracy on test set:  tensor([0.6538, 0.7253, 0.8819, 0.6803, 0.6443, 0.6796, 0.6406],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2135956485\n","Node embedding saved successfully to file: FastRP/FastRP_2135956485\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  3710910636\n","TRAINING WITH SEED:  3710910636\n","Epoch 0 with train loss: 2.061 train accuracy: 38.907 validation accuracy: 41.600\n","Per class train accuracy:  tensor([0.0500, 0.5889, 0.8061, 0.4194, 0.3010, 0.0435, 0.4943],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.0656, 0.7222, 0.8077, 0.4241, 0.2963, 0.1579, 0.5172],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.920 train accuracy: 71.937 validation accuracy: 69.600\n","Per class train accuracy:  tensor([0.5625, 0.8222, 0.8929, 0.6540, 0.7755, 0.6449, 0.7586],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.5246, 0.8889, 0.8590, 0.6709, 0.7037, 0.6491, 0.5862],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.000\n","Final per class accuracy on test set:  tensor([0.5846, 0.7582, 0.8750, 0.6395, 0.6711, 0.6893, 0.6875],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_3710910636\n","Node embedding saved successfully to file: FastRP/FastRP_3710910636\n","Final results saved successfully to file: FastRP/FastRP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["SETTING SEEDS TO:  1517964140\n","TRAINING WITH SEED:  1517964140\n","Epoch 0 with train loss: 1.880 train accuracy: 44.702 validation accuracy: 42.800\n","Per class train accuracy:  tensor([0.4125, 0.7000, 0.8010, 0.2845, 0.4235, 0.1957, 0.5402],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3934, 0.8611, 0.7821, 0.3291, 0.3210, 0.1228, 0.4483],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.847 train accuracy: 75.166 validation accuracy: 72.000\n","Per class train accuracy:  tensor([0.6313, 0.8222, 0.8776, 0.7155, 0.7653, 0.7536, 0.7241],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6230, 0.8611, 0.8590, 0.7405, 0.6543, 0.6667, 0.5517],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 71.000\n","Final per class accuracy on test set:  tensor([0.6538, 0.7253, 0.8750, 0.6646, 0.7383, 0.6893, 0.6250],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_1517964140\n","Node embedding saved successfully to file: FastRP/FastRP_1517964140\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  4083009686\n","TRAINING WITH SEED:  4083009686\n","Epoch 0 with train loss: 2.241 train accuracy: 32.036 validation accuracy: 34.800\n","Per class train accuracy:  tensor([0.3000, 0.5556, 0.5714, 0.2669, 0.1480, 0.2971, 0.1839],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3115, 0.6667, 0.5641, 0.3228, 0.2099, 0.2982, 0.0690],\n","       device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 with train loss: 0.943 train accuracy: 70.695 validation accuracy: 67.200\n","Per class train accuracy:  tensor([0.5875, 0.7556, 0.8776, 0.7097, 0.7041, 0.5362, 0.7586],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.5574, 0.8889, 0.8590, 0.7089, 0.5802, 0.4737, 0.5862],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 66.500\n","Final per class accuracy on test set:  tensor([0.5692, 0.7363, 0.8889, 0.6520, 0.5638, 0.5340, 0.7656],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_4083009686\n","Node embedding saved successfully to file: FastRP/FastRP_4083009686\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2455059856\n","TRAINING WITH SEED:  2455059856\n","Epoch 0 with train loss: 2.015 train accuracy: 38.079 validation accuracy: 35.600\n","Per class train accuracy:  tensor([0.2562, 0.2778, 0.8520, 0.2903, 0.1633, 0.3841, 0.4943],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.2951, 0.2500, 0.8462, 0.2722, 0.1358, 0.3333, 0.4138],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.846 train accuracy: 74.917 validation accuracy: 72.000\n","Per class train accuracy:  tensor([0.6938, 0.7556, 0.8724, 0.7507, 0.7245, 0.7174, 0.6667],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6557, 0.7778, 0.8590, 0.7278, 0.6790, 0.6842, 0.5517],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 72.800\n","Final per class accuracy on test set:  tensor([0.6615, 0.7363, 0.8750, 0.6834, 0.7383, 0.7767, 0.6406],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2455059856\n","Node embedding saved successfully to file: FastRP/FastRP_2455059856\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  400225693\n","TRAINING WITH SEED:  400225693\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0 with train loss: 2.110 train accuracy: 32.781 validation accuracy: 31.200\n","Per class train accuracy:  tensor([0.2188, 0.7556, 0.4184, 0.2610, 0.3367, 0.2101, 0.3103],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.2131, 0.8333, 0.4103, 0.2722, 0.3086, 0.1404, 0.1724],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.897 train accuracy: 73.013 validation accuracy: 69.000\n","Per class train accuracy:  tensor([0.5938, 0.7667, 0.8827, 0.6979, 0.7347, 0.7174, 0.7356],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.5902, 0.8889, 0.8846, 0.6835, 0.6296, 0.6316, 0.4483],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 68.400\n","Final per class accuracy on test set:  tensor([0.6154, 0.7582, 0.8750, 0.5893, 0.6980, 0.7282, 0.6562],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_400225693\n","Node embedding saved successfully to file: FastRP/FastRP_400225693\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  89475662\n","TRAINING WITH SEED:  89475662\n","Epoch 0 with train loss: 2.113 train accuracy: 35.513 validation accuracy: 39.600\n","Per class train accuracy:  tensor([0.1437, 0.6000, 0.7704, 0.3021, 0.1429, 0.2464, 0.4138],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.2295, 0.7500, 0.7308, 0.2975, 0.2963, 0.2281, 0.5517],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.934 train accuracy: 71.689 validation accuracy: 67.000\n","Per class train accuracy:  tensor([0.6687, 0.7556, 0.9031, 0.6950, 0.6837, 0.5942, 0.7011],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6557, 0.8056, 0.8846, 0.7025, 0.5432, 0.5614, 0.3448],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 67.900\n","Final per class accuracy on test set:  tensor([0.6923, 0.7363, 0.8958, 0.6552, 0.5369, 0.6214, 0.6250],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_89475662\n","Node embedding saved successfully to file: FastRP/FastRP_89475662\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  361232447\n","TRAINING WITH SEED:  361232447\n","Epoch 0 with train loss: 2.131 train accuracy: 33.940 validation accuracy: 36.000\n","Per class train accuracy:  tensor([0.2500, 0.5778, 0.5255, 0.2405, 0.3367, 0.1739, 0.4943],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.2623, 0.6389, 0.5513, 0.3228, 0.3210, 0.1754, 0.3793],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.923 train accuracy: 71.275 validation accuracy: 68.600\n","Per class train accuracy:  tensor([0.5938, 0.7333, 0.8929, 0.7009, 0.7143, 0.6304, 0.6782],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6066, 0.8333, 0.8718, 0.6772, 0.6173, 0.5965, 0.5862],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 67.000\n","Final per class accuracy on test set:  tensor([0.6231, 0.7253, 0.8819, 0.6364, 0.6309, 0.5825, 0.6094],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_361232447\n","Node embedding saved successfully to file: FastRP/FastRP_361232447\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  3647665043\n","TRAINING WITH SEED:  3647665043\n","Epoch 0 with train loss: 2.196 train accuracy: 28.477 validation accuracy: 27.600\n","Per class train accuracy:  tensor([0.4313, 0.6333, 0.2908, 0.0968, 0.2245, 0.4203, 0.2989],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3607, 0.7778, 0.2564, 0.1076, 0.2222, 0.4561, 0.2414],\n","       device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 with train loss: 0.907 train accuracy: 74.089 validation accuracy: 69.400\n","Per class train accuracy:  tensor([0.6625, 0.7556, 0.8929, 0.6774, 0.7704, 0.6957, 0.7816],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7213, 0.8056, 0.8590, 0.6266, 0.6173, 0.6667, 0.6897],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 67.100\n","Final per class accuracy on test set:  tensor([0.6154, 0.7033, 0.8889, 0.6113, 0.6040, 0.6796, 0.6875],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_3647665043\n","Node embedding saved successfully to file: FastRP/FastRP_3647665043\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  1221215631\n","TRAINING WITH SEED:  1221215631\n","Epoch 0 with train loss: 2.094 train accuracy: 34.603 validation accuracy: 37.400\n","Per class train accuracy:  tensor([0.1750, 0.6556, 0.6429, 0.3343, 0.1735, 0.2029, 0.3333],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.1639, 0.7222, 0.6154, 0.3861, 0.2469, 0.1930, 0.3793],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.915 train accuracy: 72.765 validation accuracy: 69.800\n","Per class train accuracy:  tensor([0.5938, 0.7444, 0.8827, 0.7302, 0.7143, 0.6812, 0.7011],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.5902, 0.8333, 0.8590, 0.7342, 0.5802, 0.6316, 0.5862],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.300\n","Final per class accuracy on test set:  tensor([0.5923, 0.7363, 0.8750, 0.6677, 0.6510, 0.7184, 0.6094],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_1221215631\n","Node embedding saved successfully to file: FastRP/FastRP_1221215631\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2036056847\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["TRAINING WITH SEED:  2036056847\n","Epoch 0 with train loss: 2.069 train accuracy: 38.411 validation accuracy: 34.800\n","Per class train accuracy:  tensor([0.2438, 0.7000, 0.7755, 0.2845, 0.2194, 0.2029, 0.4828],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.2623, 0.7500, 0.7308, 0.2722, 0.1111, 0.2105, 0.3448],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.922 train accuracy: 73.344 validation accuracy: 71.800\n","Per class train accuracy:  tensor([0.6875, 0.8000, 0.8724, 0.6833, 0.7347, 0.6594, 0.7471],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7213, 0.8056, 0.8718, 0.7025, 0.6667, 0.6316, 0.5862],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.900\n","Final per class accuracy on test set:  tensor([0.6077, 0.7582, 0.8819, 0.6646, 0.7047, 0.6214, 0.6719],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2036056847\n","Node embedding saved successfully to file: FastRP/FastRP_2036056847\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  1860537279\n","TRAINING WITH SEED:  1860537279\n","Epoch 0 with train loss: 2.169 train accuracy: 31.457 validation accuracy: 33.000\n","Per class train accuracy:  tensor([0.1375, 0.3667, 0.7806, 0.2493, 0.1531, 0.2971, 0.1839],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.0820, 0.5278, 0.7821, 0.2722, 0.1481, 0.3684, 0.1379],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.921 train accuracy: 74.752 validation accuracy: 70.600\n","Per class train accuracy:  tensor([0.6625, 0.7556, 0.9184, 0.7419, 0.7449, 0.6449, 0.7011],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.5738, 0.8056, 0.8974, 0.7215, 0.6914, 0.5965, 0.5172],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 70.300\n","Final per class accuracy on test set:  tensor([0.6462, 0.7363, 0.8819, 0.6771, 0.6980, 0.6505, 0.5938],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_1860537279\n","Node embedding saved successfully to file: FastRP/FastRP_1860537279\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  516507873\n","TRAINING WITH SEED:  516507873\n","Epoch 0 with train loss: 2.120 train accuracy: 39.487 validation accuracy: 41.200\n","Per class train accuracy:  tensor([0.1000, 0.6667, 0.7092, 0.3724, 0.3316, 0.2246, 0.4483],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.1148, 0.6389, 0.6795, 0.4494, 0.3333, 0.3509, 0.1724],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.890 train accuracy: 74.255 validation accuracy: 71.400\n","Per class train accuracy:  tensor([0.6750, 0.7778, 0.8724, 0.6833, 0.7806, 0.6957, 0.7586],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7049, 0.8056, 0.8590, 0.7215, 0.6667, 0.5439, 0.6552],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 70.200\n","Final per class accuracy on test set:  tensor([0.6308, 0.7253, 0.8681, 0.6708, 0.6913, 0.6408, 0.7188],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_516507873\n","Node embedding saved successfully to file: FastRP/FastRP_516507873\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  3692371949\n","TRAINING WITH SEED:  3692371949\n","Epoch 0 with train loss: 2.164 train accuracy: 32.947 validation accuracy: 29.000\n","Per class train accuracy:  tensor([0.2875, 0.6444, 0.7908, 0.0938, 0.1990, 0.1377, 0.5632],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.2787, 0.8056, 0.7949, 0.0506, 0.1975, 0.0526, 0.3448],\n","       device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 with train loss: 0.920 train accuracy: 73.179 validation accuracy: 69.400\n","Per class train accuracy:  tensor([0.6750, 0.7444, 0.8827, 0.7185, 0.7653, 0.6014, 0.6667],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7049, 0.8611, 0.8590, 0.7025, 0.6420, 0.5263, 0.4483],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 70.300\n","Final per class accuracy on test set:  tensor([0.6385, 0.7253, 0.8819, 0.6708, 0.7114, 0.6117, 0.6875],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_3692371949\n","Node embedding saved successfully to file: FastRP/FastRP_3692371949\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  3300171104\n","TRAINING WITH SEED:  3300171104\n","Epoch 0 with train loss: 2.149 train accuracy: 29.636 validation accuracy: 31.000\n","Per class train accuracy:  tensor([0.2812, 0.1444, 0.7194, 0.1701, 0.2347, 0.2536, 0.2299],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3115, 0.1667, 0.7051, 0.2152, 0.2222, 0.3158, 0.1724],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.882 train accuracy: 73.924 validation accuracy: 70.800\n","Per class train accuracy:  tensor([0.6875, 0.7556, 0.8929, 0.6862, 0.7551, 0.6304, 0.8161],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7213, 0.8889, 0.8718, 0.6329, 0.6914, 0.6491, 0.5862],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.600\n","Final per class accuracy on test set:  tensor([0.6923, 0.7363, 0.8889, 0.6270, 0.6913, 0.5728, 0.7656],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_3300171104\n","Node embedding saved successfully to file: FastRP/FastRP_3300171104\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2794978777\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["TRAINING WITH SEED:  2794978777\n","Epoch 0 with train loss: 2.037 train accuracy: 35.679 validation accuracy: 38.000\n","Per class train accuracy:  tensor([0.3375, 0.1333, 0.7908, 0.2815, 0.1837, 0.3043, 0.4138],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3115, 0.1667, 0.7949, 0.3987, 0.1358, 0.3684, 0.2759],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.864 train accuracy: 74.503 validation accuracy: 72.600\n","Per class train accuracy:  tensor([0.6125, 0.7667, 0.8776, 0.7214, 0.7500, 0.7029, 0.8161],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6721, 0.8611, 0.8718, 0.7532, 0.5432, 0.7018, 0.6897],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.600\n","Final per class accuracy on test set:  tensor([0.5846, 0.7363, 0.8889, 0.6395, 0.6779, 0.7087, 0.7344],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2794978777\n","Node embedding saved successfully to file: FastRP/FastRP_2794978777\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  3303475786\n","TRAINING WITH SEED:  3303475786\n","Epoch 0 with train loss: 2.004 train accuracy: 37.997 validation accuracy: 38.600\n","Per class train accuracy:  tensor([0.1125, 0.6000, 0.8571, 0.3724, 0.1939, 0.1377, 0.4023],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.0656, 0.7222, 0.8846, 0.4114, 0.1358, 0.1754, 0.2759],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.891 train accuracy: 73.593 validation accuracy: 70.800\n","Per class train accuracy:  tensor([0.7250, 0.7444, 0.8878, 0.6921, 0.7092, 0.6449, 0.7816],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.7049, 0.7778, 0.8718, 0.6962, 0.6173, 0.6842, 0.5517],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.300\n","Final per class accuracy on test set:  tensor([0.6692, 0.7363, 0.9028, 0.6552, 0.6242, 0.6699, 0.5938],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_3303475786\n","Node embedding saved successfully to file: FastRP/FastRP_3303475786\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  2952735006\n","TRAINING WITH SEED:  2952735006\n","Epoch 0 with train loss: 2.116 train accuracy: 33.692 validation accuracy: 31.000\n","Per class train accuracy:  tensor([0.3625, 0.5222, 0.8469, 0.2434, 0.1633, 0.1377, 0.0230],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.3443, 0.6667, 0.7949, 0.1456, 0.1852, 0.1754, 0.0000],\n","       device='cuda:0')\n","Epoch 9 with train loss: 0.874 train accuracy: 72.434 validation accuracy: 70.800\n","Per class train accuracy:  tensor([0.6187, 0.7778, 0.8929, 0.7155, 0.6837, 0.6594, 0.7126],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6557, 0.8611, 0.8718, 0.6582, 0.6420, 0.7018, 0.6552],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 68.700\n","Final per class accuracy on test set:  tensor([0.6385, 0.7582, 0.8889, 0.6270, 0.6443, 0.6505, 0.6875],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_2952735006\n","Node embedding saved successfully to file: FastRP/FastRP_2952735006\n","Final results saved successfully to file: FastRP/FastRP\n","SETTING SEEDS TO:  572297925\n","TRAINING WITH SEED:  572297925\n","Epoch 0 with train loss: 1.922 train accuracy: 40.646 validation accuracy: 44.200\n","Per class train accuracy:  tensor([0.3875, 0.3000, 0.8724, 0.3167, 0.2296, 0.3116, 0.4023],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.4754, 0.5278, 0.8590, 0.3861, 0.2469, 0.2982, 0.2759],\n","       device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 with train loss: 0.877 train accuracy: 74.089 validation accuracy: 70.000\n","Per class train accuracy:  tensor([0.6500, 0.7556, 0.9031, 0.7038, 0.7245, 0.7464, 0.7011],\n","       device='cuda:0')\n","Per class val accuracy:  tensor([0.6393, 0.8889, 0.8590, 0.7278, 0.5926, 0.6140, 0.4828],\n","       device='cuda:0')\n","Our final test accuracy for the GNN is: 69.900\n","Final per class accuracy on test set:  tensor([0.6308, 0.7473, 0.8819, 0.6583, 0.6309, 0.7476, 0.6406],\n","       device='cuda:0')\n","Training stats saved successfully to file: FastRP/FastRP_572297925\n","Node embedding saved successfully to file: FastRP/FastRP_572297925\n","Final results saved successfully to file: FastRP/FastRP\n"]}],"source":["# Use parameters from example in https://github.com/GTmac/FastRP/blob/master/fast-random-projection-blogcatalog.ipynb\n","# Except our input matrix in an adjacency matrix and since we are not tuning alpha we just set this to None\n","input_matrix = 'adj'\n","alpha = -0.67\n","conf = {\n","        'projection_method': 'sparse',\n","        'input_matrix': input_matrix,\n","        'weights': [0.0, 0.0, 1.0, 6.67],\n","        'normalization': True,\n","        'dim': HIDDEN_DIM,\n","        'alpha': alpha,\n","        'C': 0.1\n","    }\n","\n","num_nodes = node_features.shape[0]\n","\n","# Convert adjacency matrix to scipy matrix\n","adj_matrix = to_scipy_sparse_matrix(edge_indices)\n","# Whether we are using the adjacency or transition matrix\n","if input_matrix == 'trans':\n","  # Create the degree matrix for the graph\n","  degrees = degree(edge_indices[0])\n","  diagonal_degree = sp.sparse.spdiags(degrees, 0, degrees.size()[0], degrees.size()[0])\n","  # Create the transition matrix for the graph = D-1(A)\n","  transition_matrix = scipy.sparse.linalg.inv(diagonal_degree).multiply(adj_matrix)\n","  print(\"Using transition matrix\")\n","else:\n","  transition_matrix = adj_matrix\n","  print(\"Using adjacency matrix\")\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for seed in seeds:\n","  set_seeds(seed)\n","  \n","  embeddings = fastrp_wrapper(transition_matrix, conf, seed)\n","  # convert to tensor \n","  embeddings = torch.from_numpy(embeddings)\n","\n","  # Create the model\n","  model = FastRPEmbeddingWrapper(HIDDEN_DIM, num_classes)\n","  model = model.to(device)\n","\n","  # Run training loop\n","  print(\"TRAINING WITH SEED: \", str(seed))\n","  train_stats_cora = train_eval_loop_embedding_classifier(model, embeddings, train_y, train_mask, \n","                                             valid_y, valid_mask, test_y, test_mask, num_classes, seed, filename+\"/\"+filename, device, is_cora)\n","  # Print out graphs if not using GPU\n","  if device == torch.device('cpu'):\n","    plot_stats(train_stats_cora, name=filename)"]},{"cell_type":"markdown","metadata":{"id":"X5HLu-NNuJ88"},"source":["# Node2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5LtiwsyJPxpt"},"outputs":[],"source":["from torch_geometric.nn import Node2Vec\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","data_name = \"Arxiv\"\n","\n","# Get masks and training labels for each split\n","if data_name == \"Cora\":\n","  num_classes = 7\n","  data = cora_data\n","  # Get the edge indices and node features for our model\n","  edge_indices = data.edge_index\n","  node_features = data.x\n","  # CHANGE: To name of model being tested\n","  filename =  \"Node2Vec_Cora\"\n","  train_mask = data.train_mask\n","  train_y = data.y[train_mask]\n","  valid_mask = data.val_mask\n","  valid_y = data.y[valid_mask]\n","  test_mask = data.test_mask\n","  test_y = data.y[test_mask]\n","elif data_name == \"Coauthor\":\n","  data = cs_data\n","  # Get the edge indices and node features for our model\n","  edge_indices = data.edge_index\n","  node_features = data.x\n","  num_classes = 15\n","  filename =  \"Node2Vec_Coauthor_CS\"\n","  train_mask = train_mask_cs\n","  train_y = data.y[train_mask]\n","  valid_mask = val_mask_cs\n","  valid_y = data.y[valid_mask]\n","  test_mask = test_mask_cs\n","  test_y = data.y[test_mask]\n","elif data_name == \"Arxiv\":\n","  data = arxiv_data\n","  edge_indices = arxiv_data.edge_index\n","  node_features = arxiv_data.x\n","  neighbour_dataset = arxiv_data\n","\n","  # Get masks and training labels for each split\n","  train_mask = train_idx\n","  train_y = arxiv_data.y[train_mask]\n","  valid_mask = valid_idx\n","  valid_y = arxiv_data.y[valid_mask]\n","  test_mask = test_idx\n","  test_y = arxiv_data.y[test_mask]\n","\n","  num_classes = 40\n","  is_cora = False\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# use 30 seeds which have been randomly generated using seed_list = [np.random.randint(4294967296 - 1) for i in range(30)]\n","seeds = [4193977854, 1863727779, 170173784, 2342954646, 116846604, 2105922959, 2739899259, 1024258131, 806299656, 880019963, 1818027900, 2135956485, 3710910636, 1517964140, 4083009686, 2455059856, 400225693, 89475662, 361232447, 3647665043, 1221215631, 2036056847, 1860537279, 516507873, 3692371949, 3300171104, 2794978777, 3303475786, 2952735006, 572297925]\n","\n","# create folder for saving all model info into if it does not exist already\n","if not os.path.exists(file_path+filename+\"/\"):\n","  os.mkdir(file_path+filename+\"/\")\n","\n","filename = filename + \"/\" + filename\n","\n","for seed in seeds:\n","  set_seeds(seed)\n","  # Create the model\n","  #model = GATModelWrapper(in_channels = node_features.shape[-1], hidden_channels = node_features.shape[-1], num_layers=1, out_channels=num_classes, v2=True)\n","  model = Node2VecWrapper(data.edge_index.to(device), embedding_size=128, walk_length=20,\n","                     context_size=10, walks_per_node=10,\n","                     num_negative_samples=1, p=1, q=1, sparse=True, out_channels=num_classes).to(device)\n","  loader = model.loader(batch_size=128, shuffle=True,\n","                      num_workers=0)\n","  optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n","\n","  def train():\n","    model.train()\n","    total_loss = 0\n","    for pos_rw, neg_rw in loader:\n","      optimizer.zero_grad()\n","      loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n","      loss.backward()\n","      optimizer.step()\n","      total_loss += loss.item()\n","    return total_loss / len(loader)\n","\n","  @torch.no_grad()\n","  def find_model_acc(model, train_z, train_y, test_z, test_y, solver: str = 'lbfgs', multi_class: str = 'auto', *args, **kwargs):\n","    pred_y = model.test(train_z, train_y, test_z, test_y, solver=solver, multi_class=multi_class, *args, **kwargs)\n","    acc = accuracy_score(test_y.detach().cpu().numpy(), pred_y)\n","    matrix = confusion_matrix(test_y.detach().cpu().numpy(), pred_y)\n","    per_class_acc = matrix.diagonal()/matrix.sum(axis=1)\n","    #print(m)\n","    #report = classification_report(test_y.detach().cpu().numpy(), pred_y)\n","    #print(report)\n","    return acc, per_class_acc\n","\n","  @torch.no_grad()\n","  def test():\n","    model.eval()\n","    \n","    pred, z = model()\n","    acc_train, per_class_train_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n","                      z[train_mask], data.y[train_mask])\n","  \n","    acc_val, per_class_val_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n","                      z[valid_mask], data.y[valid_mask])\n","\n","    acc_test, per_class_test_acc = find_model_acc(model, z[train_mask], data.y[train_mask],\n","                      z[test_mask], data.y[test_mask])\n","\n","    return z, acc_train, per_class_train_acc, acc_val, per_class_val_acc, acc_test, per_class_test_acc\n","\n","  training_stats = None\n","  for epoch in range(0, 10):\n","    loss = train()\n","    node_embeddings, acc_train, per_class_train_acc, acc_val, per_class_val_acc, acc_test, per_class_test_acc = test()\n","    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc_train: {acc_train:.4f}, Acc_val: {acc_val:.4f}, Acc_test: {acc_test:.4f}')\n","    print(f'Per class train accuracy: ', per_class_train_acc)\n","    epoch_stats = {'train_acc': acc_train, 'val_acc': acc_val, 'test_acc': acc_test, 'epoch':epoch}\n","    training_stats = update_stats(training_stats, epoch_stats)\n","  \n","  # Save training stats if on final iteration of the run\n","  save_training_info(training_stats, node_embeddings, filename+\"_\"+str(seed))\n","  # Save final results\n","  final_results_list = [seed, acc_test, per_class_test_acc, per_class_train_acc, per_class_val_acc]\n","  save_final_results(final_results_list, filename)\n","  # Save final model weights incase we want to do further inference later\n","  torch.save(model.state_dict(), file_path+filename+\"_\" + str(seed) + \"_model.pt\")\n","\n","  plot_stats(training_stats, name=filename)"]},{"cell_type":"markdown","metadata":{"id":"NGtX2ycNQa-H"},"source":["# Similarity tests"]},{"cell_type":"markdown","metadata":{"id":"-eesbQaUQfd4"},"source":["https://github.com/SGDE2020/embedding_stability/blob/master/similarity_tests/similarity_tests.py"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["NGtX2ycNQa-H"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}